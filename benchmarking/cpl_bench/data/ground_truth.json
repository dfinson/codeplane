[
    {
        "issue": "4",
        "title": "Cache model inference",
        "query_level": "Q1",
        "task": "I need to implement inference result caching in Evee's evaluation pipeline. The cache should intercept model inference calls in the ModelEvaluator (`_infer_record` and `_infer_record_async`), store InferenceOutput results keyed by input record hash, and skip re-inference on cache hits. I need to add cache configuration fields to Config/ModelVariantConfig in the config models, update the evaluation loop, add cache hit/miss logging, and write tests for the caching behavior.",
        "gt_files": [
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/config/models.py",
            "src/evee/core/base_model.py",
            "src/evee/core/models/inference_output.py",
            "src/evee/logging/local_metrics_logger.py",
            "tests/evee/evaluation/test_model_evaluator_evaluation.py",
            "tests/evee/evaluation/test_model_evaluator_init.py",
            "tests/evee/conftest.py",
            "docs/user-guide/configuration.md",
            "docs/user-guide/models.md",
            "example/experiment/config.yaml",
            "pyproject.toml",
        ],
        "gt_categories": [
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "Core evaluator with `_infer_record`, `_infer_record_async` — cache check/store injected here",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "`ModelVariantConfig` — needs `cache_enabled`, `cache_dir` fields",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`ModelWrapper`, `@model` decorator — cache config read per-model; interface unchanged",
            },
            {
                "path": "src/evee/core/models/inference_output.py",
                "category": "C",
                "relevance": "`InferenceOutput` dataclass — cached result structure; unchanged",
            },
            {
                "path": "src/evee/logging/local_metrics_logger.py",
                "category": "C",
                "relevance": "Logging infra — existing logger used for cache hits/misses; no changes",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_evaluation.py",
                "category": "C",
                "relevance": "Tests needing cache hit/miss scenarios",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_init.py",
                "category": "C",
                "relevance": "Evaluator init — cache config initialization tests",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Test fixtures needing cache config fields",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference — needs cache configuration docs",
            },
            {
                "path": "docs/user-guide/models.md",
                "category": "S",
                "relevance": "Model documentation — caching behavior",
            },
            {
                "path": "example/experiment/config.yaml",
                "category": "S",
                "relevance": "Example config — cache configuration example",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "Potential new dependency for cache storage",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "4",
        "title": "Cache model inference",
        "query_level": "Q2",
        "task": "Add caching support for deterministic model inference results in Evee. When a model's results are deterministic, re-running evaluation should reuse cached inference outputs instead of calling the model again. This involves changes to the evaluation pipeline, configuration schema, and model infrastructure. Need to know where inference happens and how config is structured.",
        "gt_files": [
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/config/models.py",
            "src/evee/core/base_model.py",
            "src/evee/core/models/inference_output.py",
            "src/evee/logging/local_metrics_logger.py",
            "tests/evee/evaluation/test_model_evaluator_evaluation.py",
            "tests/evee/evaluation/test_model_evaluator_init.py",
            "tests/evee/conftest.py",
            "docs/user-guide/configuration.md",
            "docs/user-guide/models.md",
            "example/experiment/config.yaml",
            "pyproject.toml",
        ],
        "gt_categories": [
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "Core evaluator with `_infer_record`, `_infer_record_async` — cache check/store injected here",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "`ModelVariantConfig` — needs `cache_enabled`, `cache_dir` fields",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`ModelWrapper`, `@model` decorator — cache config read per-model; interface unchanged",
            },
            {
                "path": "src/evee/core/models/inference_output.py",
                "category": "C",
                "relevance": "`InferenceOutput` dataclass — cached result structure; unchanged",
            },
            {
                "path": "src/evee/logging/local_metrics_logger.py",
                "category": "C",
                "relevance": "Logging infra — existing logger used for cache hits/misses; no changes",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_evaluation.py",
                "category": "C",
                "relevance": "Tests needing cache hit/miss scenarios",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_init.py",
                "category": "C",
                "relevance": "Evaluator init — cache config initialization tests",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Test fixtures needing cache config fields",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference — needs cache configuration docs",
            },
            {
                "path": "docs/user-guide/models.md",
                "category": "S",
                "relevance": "Model documentation — caching behavior",
            },
            {
                "path": "example/experiment/config.yaml",
                "category": "S",
                "relevance": "Example config — cache configuration example",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "Potential new dependency for cache storage",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "4",
        "title": "Cache model inference",
        "query_level": "Q3",
        "task": "How can I add result caching to Evee so that re-running experiments with the same models doesn't repeat inference? I want to save time and costs during iterative development. Where should the caching logic live in the codebase?",
        "gt_files": [
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/config/models.py",
            "src/evee/core/base_model.py",
            "src/evee/core/models/inference_output.py",
            "src/evee/logging/local_metrics_logger.py",
            "tests/evee/evaluation/test_model_evaluator_evaluation.py",
            "tests/evee/evaluation/test_model_evaluator_init.py",
            "tests/evee/conftest.py",
            "docs/user-guide/configuration.md",
            "docs/user-guide/models.md",
            "example/experiment/config.yaml",
            "pyproject.toml",
        ],
        "gt_categories": [
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "Core evaluator with `_infer_record`, `_infer_record_async` — cache check/store injected here",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "`ModelVariantConfig` — needs `cache_enabled`, `cache_dir` fields",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`ModelWrapper`, `@model` decorator — cache config read per-model; interface unchanged",
            },
            {
                "path": "src/evee/core/models/inference_output.py",
                "category": "C",
                "relevance": "`InferenceOutput` dataclass — cached result structure; unchanged",
            },
            {
                "path": "src/evee/logging/local_metrics_logger.py",
                "category": "C",
                "relevance": "Logging infra — existing logger used for cache hits/misses; no changes",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_evaluation.py",
                "category": "C",
                "relevance": "Tests needing cache hit/miss scenarios",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_init.py",
                "category": "C",
                "relevance": "Evaluator init — cache config initialization tests",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Test fixtures needing cache config fields",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference — needs cache configuration docs",
            },
            {
                "path": "docs/user-guide/models.md",
                "category": "S",
                "relevance": "Model documentation — caching behavior",
            },
            {
                "path": "example/experiment/config.yaml",
                "category": "S",
                "relevance": "Example config — cache configuration example",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "Potential new dependency for cache storage",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "38",
        "title": "Evee MCP server",
        "query_level": "Q1",
        "task": "I need to build an MCP server for the Evee evaluation framework using FastMCP. The server should expose tools for running experiments (`run_experiment`), validating configs (`validate_config`), discovering components (`list_components`), and viewing results (`view_results`). It also needs static resources for config schemas, model/metric patterns, and Azure evaluator metadata. The server uses an execution runner for subprocess-based evaluation and an environment resolver for venv discovery. I need the existing tool implementations, resource definitions, test fixtures, and documentation structure.",
        "gt_files": [
            "src/evee/mcp/server.py",
            "src/evee/mcp/__init__.py",
            "src/evee/mcp/constants.py",
            "src/evee/mcp/README.md",
            "src/evee/mcp/tools/__init__.py",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/validation.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/resources/__init__.py",
            "src/evee/mcp/resources/base.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/connections.py",
            "src/evee/mcp/resources/model_patterns.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "src/evee/mcp/resources/evaluators.py",
            "src/evee/mcp/resources/patterns.py",
            "src/evee/mcp/resources/app_viewer.py",
            "src/evee/execution/runner.py",
            "src/evee/execution/environment.py",
            "src/evee/ui/results-viewer/src/app.tsx",
            "tests/mcp/conftest.py",
            "tests/mcp/test_e2e.py",
            "tests/mcp/test_tools.py",
            "tests/mcp/test_resources.py",
            "tests/mcp/__init__.py",
            "pyproject.toml",
            "docs/user-guide/mcp-server.md",
            "docs/design/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/server.py",
                "category": "E",
                "relevance": "Main FastMCP server with tool and resource registrations",
            },
            {
                "path": "src/evee/mcp/__init__.py",
                "category": "E",
                "relevance": "MCP package public API",
            },
            {
                "path": "src/evee/mcp/constants.py",
                "category": "E",
                "relevance": "SERVER_NAME, MimeTypes, ResourceURIs, ToolNames",
            },
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP server documentation",
            },
            {
                "path": "src/evee/mcp/tools/__init__.py",
                "category": "E",
                "relevance": "Tools package/registry",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "E",
                "relevance": "BaseTool, ToolResult base classes",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "E",
                "relevance": "RunExperimentTool implementation",
            },
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "E",
                "relevance": "ValidateConfigTool implementation",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "E",
                "relevance": "ListComponentsTool implementation",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "E",
                "relevance": "ViewResultsTool implementation",
            },
            {
                "path": "src/evee/mcp/resources/__init__.py",
                "category": "E",
                "relevance": "Resources package/registry",
            },
            {
                "path": "src/evee/mcp/resources/base.py",
                "category": "E",
                "relevance": "BaseResource, ResourceMetadata",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "ConfigSchemaResource",
            },
            {
                "path": "src/evee/mcp/resources/connections.py",
                "category": "E",
                "relevance": "Connection patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/model_patterns.py",
                "category": "E",
                "relevance": "Model implementation patterns",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "E",
                "relevance": "Metric patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/evaluators.py",
                "category": "E",
                "relevance": "Azure evaluators resource",
            },
            {
                "path": "src/evee/mcp/resources/patterns.py",
                "category": "E",
                "relevance": "Decorator patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/app_viewer.py",
                "category": "E",
                "relevance": "Results viewer app resource",
            },
            {
                "path": "src/evee/execution/runner.py",
                "category": "E",
                "relevance": "ExecutionRunner used by MCP tools",
            },
            {
                "path": "src/evee/execution/environment.py",
                "category": "E",
                "relevance": "EnvironmentResolver for venv discovery",
            },
            {
                "path": "src/evee/ui/results-viewer/src/app.tsx",
                "category": "E",
                "relevance": "Results viewer HTML app",
            },
            {"path": "tests/mcp/conftest.py", "category": "C", "relevance": "MCP test fixtures"},
            {"path": "tests/mcp/test_e2e.py", "category": "C", "relevance": "End-to-end MCP tests"},
            {"path": "tests/mcp/test_tools.py", "category": "C", "relevance": "Tool unit tests"},
            {
                "path": "tests/mcp/test_resources.py",
                "category": "C",
                "relevance": "Resource unit tests",
            },
            {
                "path": "tests/mcp/__init__.py",
                "category": "C",
                "relevance": "MCP test package init",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "MCP dependency, CLI entry point",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP server user documentation",
            },
            {
                "path": "docs/design/mcp-server.md",
                "category": "S",
                "relevance": "MCP server design document",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "38",
        "title": "Evee MCP server",
        "query_level": "Q2",
        "task": "I'm working on an MCP server that exposes Evee's evaluation capabilities to AI coding assistants in IDEs. It needs tools for experiment management, configuration validation, component discovery, and result viewing, plus documentation resources. I need to understand the current MCP server architecture, tool/resource patterns, and test structure.",
        "gt_files": [
            "src/evee/mcp/server.py",
            "src/evee/mcp/__init__.py",
            "src/evee/mcp/constants.py",
            "src/evee/mcp/README.md",
            "src/evee/mcp/tools/__init__.py",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/validation.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/resources/__init__.py",
            "src/evee/mcp/resources/base.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/connections.py",
            "src/evee/mcp/resources/model_patterns.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "src/evee/mcp/resources/evaluators.py",
            "src/evee/mcp/resources/patterns.py",
            "src/evee/mcp/resources/app_viewer.py",
            "src/evee/execution/runner.py",
            "src/evee/execution/environment.py",
            "src/evee/ui/results-viewer/src/app.tsx",
            "tests/mcp/conftest.py",
            "tests/mcp/test_e2e.py",
            "tests/mcp/test_tools.py",
            "tests/mcp/test_resources.py",
            "tests/mcp/__init__.py",
            "pyproject.toml",
            "docs/user-guide/mcp-server.md",
            "docs/design/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/server.py",
                "category": "E",
                "relevance": "Main FastMCP server with tool and resource registrations",
            },
            {
                "path": "src/evee/mcp/__init__.py",
                "category": "E",
                "relevance": "MCP package public API",
            },
            {
                "path": "src/evee/mcp/constants.py",
                "category": "E",
                "relevance": "SERVER_NAME, MimeTypes, ResourceURIs, ToolNames",
            },
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP server documentation",
            },
            {
                "path": "src/evee/mcp/tools/__init__.py",
                "category": "E",
                "relevance": "Tools package/registry",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "E",
                "relevance": "BaseTool, ToolResult base classes",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "E",
                "relevance": "RunExperimentTool implementation",
            },
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "E",
                "relevance": "ValidateConfigTool implementation",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "E",
                "relevance": "ListComponentsTool implementation",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "E",
                "relevance": "ViewResultsTool implementation",
            },
            {
                "path": "src/evee/mcp/resources/__init__.py",
                "category": "E",
                "relevance": "Resources package/registry",
            },
            {
                "path": "src/evee/mcp/resources/base.py",
                "category": "E",
                "relevance": "BaseResource, ResourceMetadata",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "ConfigSchemaResource",
            },
            {
                "path": "src/evee/mcp/resources/connections.py",
                "category": "E",
                "relevance": "Connection patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/model_patterns.py",
                "category": "E",
                "relevance": "Model implementation patterns",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "E",
                "relevance": "Metric patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/evaluators.py",
                "category": "E",
                "relevance": "Azure evaluators resource",
            },
            {
                "path": "src/evee/mcp/resources/patterns.py",
                "category": "E",
                "relevance": "Decorator patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/app_viewer.py",
                "category": "E",
                "relevance": "Results viewer app resource",
            },
            {
                "path": "src/evee/execution/runner.py",
                "category": "E",
                "relevance": "ExecutionRunner used by MCP tools",
            },
            {
                "path": "src/evee/execution/environment.py",
                "category": "E",
                "relevance": "EnvironmentResolver for venv discovery",
            },
            {
                "path": "src/evee/ui/results-viewer/src/app.tsx",
                "category": "E",
                "relevance": "Results viewer HTML app",
            },
            {"path": "tests/mcp/conftest.py", "category": "C", "relevance": "MCP test fixtures"},
            {"path": "tests/mcp/test_e2e.py", "category": "C", "relevance": "End-to-end MCP tests"},
            {"path": "tests/mcp/test_tools.py", "category": "C", "relevance": "Tool unit tests"},
            {
                "path": "tests/mcp/test_resources.py",
                "category": "C",
                "relevance": "Resource unit tests",
            },
            {
                "path": "tests/mcp/__init__.py",
                "category": "C",
                "relevance": "MCP test package init",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "MCP dependency, CLI entry point",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP server user documentation",
            },
            {
                "path": "docs/design/mcp-server.md",
                "category": "S",
                "relevance": "MCP server design document",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "38",
        "title": "Evee MCP server",
        "query_level": "Q3",
        "task": "I want to add an MCP server to Evee so that AI assistants can interact with the evaluation framework. Where is the MCP code and how are tools and resources structured? What patterns should I follow?",
        "gt_files": [
            "src/evee/mcp/server.py",
            "src/evee/mcp/__init__.py",
            "src/evee/mcp/constants.py",
            "src/evee/mcp/README.md",
            "src/evee/mcp/tools/__init__.py",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/validation.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/resources/__init__.py",
            "src/evee/mcp/resources/base.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/connections.py",
            "src/evee/mcp/resources/model_patterns.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "src/evee/mcp/resources/evaluators.py",
            "src/evee/mcp/resources/patterns.py",
            "src/evee/mcp/resources/app_viewer.py",
            "src/evee/execution/runner.py",
            "src/evee/execution/environment.py",
            "src/evee/ui/results-viewer/src/app.tsx",
            "tests/mcp/conftest.py",
            "tests/mcp/test_e2e.py",
            "tests/mcp/test_tools.py",
            "tests/mcp/test_resources.py",
            "tests/mcp/__init__.py",
            "pyproject.toml",
            "docs/user-guide/mcp-server.md",
            "docs/design/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/server.py",
                "category": "E",
                "relevance": "Main FastMCP server with tool and resource registrations",
            },
            {
                "path": "src/evee/mcp/__init__.py",
                "category": "E",
                "relevance": "MCP package public API",
            },
            {
                "path": "src/evee/mcp/constants.py",
                "category": "E",
                "relevance": "SERVER_NAME, MimeTypes, ResourceURIs, ToolNames",
            },
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP server documentation",
            },
            {
                "path": "src/evee/mcp/tools/__init__.py",
                "category": "E",
                "relevance": "Tools package/registry",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "E",
                "relevance": "BaseTool, ToolResult base classes",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "E",
                "relevance": "RunExperimentTool implementation",
            },
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "E",
                "relevance": "ValidateConfigTool implementation",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "E",
                "relevance": "ListComponentsTool implementation",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "E",
                "relevance": "ViewResultsTool implementation",
            },
            {
                "path": "src/evee/mcp/resources/__init__.py",
                "category": "E",
                "relevance": "Resources package/registry",
            },
            {
                "path": "src/evee/mcp/resources/base.py",
                "category": "E",
                "relevance": "BaseResource, ResourceMetadata",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "ConfigSchemaResource",
            },
            {
                "path": "src/evee/mcp/resources/connections.py",
                "category": "E",
                "relevance": "Connection patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/model_patterns.py",
                "category": "E",
                "relevance": "Model implementation patterns",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "E",
                "relevance": "Metric patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/evaluators.py",
                "category": "E",
                "relevance": "Azure evaluators resource",
            },
            {
                "path": "src/evee/mcp/resources/patterns.py",
                "category": "E",
                "relevance": "Decorator patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/app_viewer.py",
                "category": "E",
                "relevance": "Results viewer app resource",
            },
            {
                "path": "src/evee/execution/runner.py",
                "category": "E",
                "relevance": "ExecutionRunner used by MCP tools",
            },
            {
                "path": "src/evee/execution/environment.py",
                "category": "E",
                "relevance": "EnvironmentResolver for venv discovery",
            },
            {
                "path": "src/evee/ui/results-viewer/src/app.tsx",
                "category": "E",
                "relevance": "Results viewer HTML app",
            },
            {"path": "tests/mcp/conftest.py", "category": "C", "relevance": "MCP test fixtures"},
            {"path": "tests/mcp/test_e2e.py", "category": "C", "relevance": "End-to-end MCP tests"},
            {"path": "tests/mcp/test_tools.py", "category": "C", "relevance": "Tool unit tests"},
            {
                "path": "tests/mcp/test_resources.py",
                "category": "C",
                "relevance": "Resource unit tests",
            },
            {
                "path": "tests/mcp/__init__.py",
                "category": "C",
                "relevance": "MCP test package init",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "MCP dependency, CLI entry point",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP server user documentation",
            },
            {
                "path": "docs/design/mcp-server.md",
                "category": "S",
                "relevance": "MCP server design document",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "57",
        "title": "Add tests for all supported python versions in CI",
        "query_level": "Q1",
        "task": "I need to expand Evee's CI pipeline to test on all Python LTS versions (3.11, 3.12, 3.13). The CI workflow in `.github/workflows/ci.yml` currently has a `matrix.python-version` that needs expanding. I also need to update `requires-python` in the root `pyproject.toml` and all package/example/sample `pyproject.toml` files to support 3.11+, and add CI status badges to the README. Need to verify the integration test workflow and devcontainer configuration as well.",
        "gt_files": [
            ".github/workflows/ci.yml",
            "pyproject.toml",
            "packages/evee-mlflow/pyproject.toml",
            "packages/evee-azureml/pyproject.toml",
            "README.md",
            ".github/workflows/integration-tests.yml",
            "example/core/pyproject.toml",
            "example/azureml/pyproject.toml",
            "example/mlflow/pyproject.toml",
            "samples/coding-sample/pyproject.toml",
            "samples/agent-sample/pyproject.toml",
            ".devcontainer/devcontainer.json",
            "Makefile",
        ],
        "gt_categories": [
            {
                "path": ".github/workflows/ci.yml",
                "category": "E",
                "relevance": "CI workflow — expand `matrix.python-version` to [3.11, 3.12, 3.13]",
            },
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "`requires-python` constraint and ruff target-version",
            },
            {
                "path": "packages/evee-mlflow/pyproject.toml",
                "category": "E",
                "relevance": "MLflow backend `requires-python`",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "E",
                "relevance": "AzureML backend `requires-python`",
            },
            {
                "path": "README.md",
                "category": "E",
                "relevance": "Needs CI status badges per Python version and version text",
            },
            {
                "path": ".github/workflows/integration-tests.yml",
                "category": "C",
                "relevance": "Integration test workflow — may need multi-version matrix",
            },
            {
                "path": "example/core/pyproject.toml",
                "category": "C",
                "relevance": "Example project `requires-python` — follows main, updated separately",
            },
            {
                "path": "example/azureml/pyproject.toml",
                "category": "C",
                "relevance": "Example project `requires-python` — follows main",
            },
            {
                "path": "example/mlflow/pyproject.toml",
                "category": "C",
                "relevance": "Example project `requires-python` — follows main",
            },
            {
                "path": "samples/coding-sample/pyproject.toml",
                "category": "C",
                "relevance": "Sample `requires-python` — follows main",
            },
            {
                "path": "samples/agent-sample/pyproject.toml",
                "category": "C",
                "relevance": "Sample `requires-python` — follows main",
            },
            {
                "path": ".devcontainer/devcontainer.json",
                "category": "C",
                "relevance": "Dev container Python version — secondary concern",
            },
            {
                "path": "Makefile",
                "category": "C",
                "relevance": "Setup/test targets — no changes needed",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "57",
        "title": "Add tests for all supported python versions in CI",
        "query_level": "Q2",
        "task": "Expand CI tests to run on Python 3.11, 3.12, and 3.13. This means updating the CI matrix, changing the minimum Python version across all `pyproject.toml` files in the monorepo, and adding CI badges to the README. Need to find all Python version constraints and CI configs.",
        "gt_files": [
            ".github/workflows/ci.yml",
            "pyproject.toml",
            "packages/evee-mlflow/pyproject.toml",
            "packages/evee-azureml/pyproject.toml",
            "README.md",
            ".github/workflows/integration-tests.yml",
            "example/core/pyproject.toml",
            "example/azureml/pyproject.toml",
            "example/mlflow/pyproject.toml",
            "samples/coding-sample/pyproject.toml",
            "samples/agent-sample/pyproject.toml",
            ".devcontainer/devcontainer.json",
            "Makefile",
        ],
        "gt_categories": [
            {
                "path": ".github/workflows/ci.yml",
                "category": "E",
                "relevance": "CI workflow — expand `matrix.python-version` to [3.11, 3.12, 3.13]",
            },
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "`requires-python` constraint and ruff target-version",
            },
            {
                "path": "packages/evee-mlflow/pyproject.toml",
                "category": "E",
                "relevance": "MLflow backend `requires-python`",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "E",
                "relevance": "AzureML backend `requires-python`",
            },
            {
                "path": "README.md",
                "category": "E",
                "relevance": "Needs CI status badges per Python version and version text",
            },
            {
                "path": ".github/workflows/integration-tests.yml",
                "category": "C",
                "relevance": "Integration test workflow — may need multi-version matrix",
            },
            {
                "path": "example/core/pyproject.toml",
                "category": "C",
                "relevance": "Example project `requires-python` — follows main, updated separately",
            },
            {
                "path": "example/azureml/pyproject.toml",
                "category": "C",
                "relevance": "Example project `requires-python` — follows main",
            },
            {
                "path": "example/mlflow/pyproject.toml",
                "category": "C",
                "relevance": "Example project `requires-python` — follows main",
            },
            {
                "path": "samples/coding-sample/pyproject.toml",
                "category": "C",
                "relevance": "Sample `requires-python` — follows main",
            },
            {
                "path": "samples/agent-sample/pyproject.toml",
                "category": "C",
                "relevance": "Sample `requires-python` — follows main",
            },
            {
                "path": ".devcontainer/devcontainer.json",
                "category": "C",
                "relevance": "Dev container Python version — secondary concern",
            },
            {
                "path": "Makefile",
                "category": "C",
                "relevance": "Setup/test targets — no changes needed",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "57",
        "title": "Add tests for all supported python versions in CI",
        "query_level": "Q3",
        "task": "How do I add support for testing Evee on all Python LTS versions and show badges in the README? Where are the CI workflows and Python version constraints defined?",
        "gt_files": [
            ".github/workflows/ci.yml",
            "pyproject.toml",
            "packages/evee-mlflow/pyproject.toml",
            "packages/evee-azureml/pyproject.toml",
            "README.md",
            ".github/workflows/integration-tests.yml",
            "example/core/pyproject.toml",
            "example/azureml/pyproject.toml",
            "example/mlflow/pyproject.toml",
            "samples/coding-sample/pyproject.toml",
            "samples/agent-sample/pyproject.toml",
            ".devcontainer/devcontainer.json",
            "Makefile",
        ],
        "gt_categories": [
            {
                "path": ".github/workflows/ci.yml",
                "category": "E",
                "relevance": "CI workflow — expand `matrix.python-version` to [3.11, 3.12, 3.13]",
            },
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "`requires-python` constraint and ruff target-version",
            },
            {
                "path": "packages/evee-mlflow/pyproject.toml",
                "category": "E",
                "relevance": "MLflow backend `requires-python`",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "E",
                "relevance": "AzureML backend `requires-python`",
            },
            {
                "path": "README.md",
                "category": "E",
                "relevance": "Needs CI status badges per Python version and version text",
            },
            {
                "path": ".github/workflows/integration-tests.yml",
                "category": "C",
                "relevance": "Integration test workflow — may need multi-version matrix",
            },
            {
                "path": "example/core/pyproject.toml",
                "category": "C",
                "relevance": "Example project `requires-python` — follows main, updated separately",
            },
            {
                "path": "example/azureml/pyproject.toml",
                "category": "C",
                "relevance": "Example project `requires-python` — follows main",
            },
            {
                "path": "example/mlflow/pyproject.toml",
                "category": "C",
                "relevance": "Example project `requires-python` — follows main",
            },
            {
                "path": "samples/coding-sample/pyproject.toml",
                "category": "C",
                "relevance": "Sample `requires-python` — follows main",
            },
            {
                "path": "samples/agent-sample/pyproject.toml",
                "category": "C",
                "relevance": "Sample `requires-python` — follows main",
            },
            {
                "path": ".devcontainer/devcontainer.json",
                "category": "C",
                "relevance": "Dev container Python version — secondary concern",
            },
            {
                "path": "Makefile",
                "category": "C",
                "relevance": "Setup/test targets — no changes needed",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "63",
        "title": "Load test benchmarks",
        "query_level": "Q1",
        "task": "I need to implement load test benchmarks for Evee's evaluation pipeline. The tests should create mock models (both sync and async using `@model` decorator) of varying complexity, generate mock datasets of different sizes (100, 1000, 10000+ records), and measure evaluation throughput. I need to understand the `ModelEvaluator` evaluation loop (sync vs async paths), dataset loading via `DatasetFactory`, metrics aggregation, and progress tracking. Tests should use `NoOpTrackingBackend` as baseline and possibly add a `benchmark` pytest marker.",
        "gt_files": [
            "pyproject.toml",
            "Makefile",
            ".github/workflows/ci.yml",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/core/base_model.py",
            "src/evee/core/base_dataset.py",
            "src/evee/datasets/jsonl_dataset.py",
            "src/evee/datasets/dataset_factory.py",
            "src/evee/config/models.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "src/evee/evaluation/progress_tracker.py",
            "src/evee/logging/local_metrics_logger.py",
            "src/evee/core/models/evaluation_output.py",
            "src/evee/core/models/inference_output.py",
            "src/evee/tracking/backends/no_op_fallback_backend.py",
            "tests/evee/conftest.py",
            "tests/evee/evaluation/test_model_evaluator_evaluation.py",
            "tests/evee/core/test_base_model.py",
        ],
        "gt_categories": [
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Pytest config, `benchmark` marker",
            },
            {"path": "Makefile", "category": "E", "relevance": "`test-benchmark` target"},
            {
                "path": ".github/workflows/ci.yml",
                "category": "E",
                "relevance": "CI pipeline for benchmark reporting",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "C",
                "relevance": "Core evaluator: `evaluate()`, sync/async paths — code being benchmarked",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`@model` decorator, `_is_async` detection — mock model creation patterns",
            },
            {
                "path": "src/evee/core/base_dataset.py",
                "category": "C",
                "relevance": "`BaseDataset` — mock datasets of various sizes",
            },
            {
                "path": "src/evee/datasets/jsonl_dataset.py",
                "category": "C",
                "relevance": "JSONL dataset loader for large data",
            },
            {
                "path": "src/evee/datasets/dataset_factory.py",
                "category": "C",
                "relevance": "DatasetFactory for dataset creation",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "Config models for benchmark test configuration",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Results aggregation performance",
            },
            {
                "path": "src/evee/evaluation/progress_tracker.py",
                "category": "C",
                "relevance": "Progress tracking under load",
            },
            {
                "path": "src/evee/logging/local_metrics_logger.py",
                "category": "C",
                "relevance": "Logging performance",
            },
            {
                "path": "src/evee/core/models/evaluation_output.py",
                "category": "C",
                "relevance": "EvaluationOutput data model",
            },
            {
                "path": "src/evee/core/models/inference_output.py",
                "category": "C",
                "relevance": "InferenceOutput data model",
            },
            {
                "path": "src/evee/tracking/backends/no_op_fallback_backend.py",
                "category": "C",
                "relevance": "NoOp backend for baseline benchmarks",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Shared test fixtures",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_evaluation.py",
                "category": "C",
                "relevance": "Existing eval tests (async/sync patterns)",
            },
            {
                "path": "tests/evee/core/test_base_model.py",
                "category": "C",
                "relevance": "Async model tests — benchmark reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "63",
        "title": "Load test benchmarks",
        "query_level": "Q2",
        "task": "I want to benchmark Evee's evaluation pipeline performance with different model types and dataset sizes. I need to create mock models and datasets, run them through the evaluation loop, and measure throughput and resource usage. Where does the evaluation happen and how do models and datasets work?",
        "gt_files": [
            "pyproject.toml",
            "Makefile",
            ".github/workflows/ci.yml",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/core/base_model.py",
            "src/evee/core/base_dataset.py",
            "src/evee/datasets/jsonl_dataset.py",
            "src/evee/datasets/dataset_factory.py",
            "src/evee/config/models.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "src/evee/evaluation/progress_tracker.py",
            "src/evee/logging/local_metrics_logger.py",
            "src/evee/core/models/evaluation_output.py",
            "src/evee/core/models/inference_output.py",
            "src/evee/tracking/backends/no_op_fallback_backend.py",
            "tests/evee/conftest.py",
            "tests/evee/evaluation/test_model_evaluator_evaluation.py",
            "tests/evee/core/test_base_model.py",
        ],
        "gt_categories": [
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Pytest config, `benchmark` marker",
            },
            {"path": "Makefile", "category": "E", "relevance": "`test-benchmark` target"},
            {
                "path": ".github/workflows/ci.yml",
                "category": "E",
                "relevance": "CI pipeline for benchmark reporting",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "C",
                "relevance": "Core evaluator: `evaluate()`, sync/async paths — code being benchmarked",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`@model` decorator, `_is_async` detection — mock model creation patterns",
            },
            {
                "path": "src/evee/core/base_dataset.py",
                "category": "C",
                "relevance": "`BaseDataset` — mock datasets of various sizes",
            },
            {
                "path": "src/evee/datasets/jsonl_dataset.py",
                "category": "C",
                "relevance": "JSONL dataset loader for large data",
            },
            {
                "path": "src/evee/datasets/dataset_factory.py",
                "category": "C",
                "relevance": "DatasetFactory for dataset creation",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "Config models for benchmark test configuration",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Results aggregation performance",
            },
            {
                "path": "src/evee/evaluation/progress_tracker.py",
                "category": "C",
                "relevance": "Progress tracking under load",
            },
            {
                "path": "src/evee/logging/local_metrics_logger.py",
                "category": "C",
                "relevance": "Logging performance",
            },
            {
                "path": "src/evee/core/models/evaluation_output.py",
                "category": "C",
                "relevance": "EvaluationOutput data model",
            },
            {
                "path": "src/evee/core/models/inference_output.py",
                "category": "C",
                "relevance": "InferenceOutput data model",
            },
            {
                "path": "src/evee/tracking/backends/no_op_fallback_backend.py",
                "category": "C",
                "relevance": "NoOp backend for baseline benchmarks",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Shared test fixtures",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_evaluation.py",
                "category": "C",
                "relevance": "Existing eval tests (async/sync patterns)",
            },
            {
                "path": "tests/evee/core/test_base_model.py",
                "category": "C",
                "relevance": "Async model tests — benchmark reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "63",
        "title": "Load test benchmarks",
        "query_level": "Q3",
        "task": "How can I load test Evee to find its performance limits? I want to test with different sizes of data and types of models. Where is the evaluation logic?",
        "gt_files": [
            "pyproject.toml",
            "Makefile",
            ".github/workflows/ci.yml",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/core/base_model.py",
            "src/evee/core/base_dataset.py",
            "src/evee/datasets/jsonl_dataset.py",
            "src/evee/datasets/dataset_factory.py",
            "src/evee/config/models.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "src/evee/evaluation/progress_tracker.py",
            "src/evee/logging/local_metrics_logger.py",
            "src/evee/core/models/evaluation_output.py",
            "src/evee/core/models/inference_output.py",
            "src/evee/tracking/backends/no_op_fallback_backend.py",
            "tests/evee/conftest.py",
            "tests/evee/evaluation/test_model_evaluator_evaluation.py",
            "tests/evee/core/test_base_model.py",
        ],
        "gt_categories": [
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Pytest config, `benchmark` marker",
            },
            {"path": "Makefile", "category": "E", "relevance": "`test-benchmark` target"},
            {
                "path": ".github/workflows/ci.yml",
                "category": "E",
                "relevance": "CI pipeline for benchmark reporting",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "C",
                "relevance": "Core evaluator: `evaluate()`, sync/async paths — code being benchmarked",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`@model` decorator, `_is_async` detection — mock model creation patterns",
            },
            {
                "path": "src/evee/core/base_dataset.py",
                "category": "C",
                "relevance": "`BaseDataset` — mock datasets of various sizes",
            },
            {
                "path": "src/evee/datasets/jsonl_dataset.py",
                "category": "C",
                "relevance": "JSONL dataset loader for large data",
            },
            {
                "path": "src/evee/datasets/dataset_factory.py",
                "category": "C",
                "relevance": "DatasetFactory for dataset creation",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "Config models for benchmark test configuration",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Results aggregation performance",
            },
            {
                "path": "src/evee/evaluation/progress_tracker.py",
                "category": "C",
                "relevance": "Progress tracking under load",
            },
            {
                "path": "src/evee/logging/local_metrics_logger.py",
                "category": "C",
                "relevance": "Logging performance",
            },
            {
                "path": "src/evee/core/models/evaluation_output.py",
                "category": "C",
                "relevance": "EvaluationOutput data model",
            },
            {
                "path": "src/evee/core/models/inference_output.py",
                "category": "C",
                "relevance": "InferenceOutput data model",
            },
            {
                "path": "src/evee/tracking/backends/no_op_fallback_backend.py",
                "category": "C",
                "relevance": "NoOp backend for baseline benchmarks",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Shared test fixtures",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_evaluation.py",
                "category": "C",
                "relevance": "Existing eval tests (async/sync patterns)",
            },
            {
                "path": "tests/evee/core/test_base_model.py",
                "category": "C",
                "relevance": "Async model tests — benchmark reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "72",
        "title": "Azure AI Foundry Integration",
        "query_level": "Q1",
        "task": "I need to integrate Azure AI Foundry as both a tracking and compute backend for Evee. The tracking backend should send metrics to Foundry dashboards via OpenTelemetry or native SDK. The compute backend should execute evaluations on Foundry infrastructure. I should follow the `packages/evee-azureml/` pattern — examining the AzureML tracking backend, compute backend, auth, config models, and entry points. I also need the Terraform AI Foundry module for infrastructure provisioning and the CLI commands for tracking/compute backend configuration.",
        "gt_files": [
            "src/evee/cli/commands/tracking.py",
            "src/evee/cli/commands/compute.py",
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/constants.py",
            "src/evee/cli/azure_evaluators.json",
            "src/evee/core/telemetry.py",
            "Makefile",
            "src/evee/tracking/backend.py",
            "src/evee/tracking/factory.py",
            "src/evee/tracking/events.py",
            "src/evee/tracking/__init__.py",
            "src/evee/compute/backend.py",
            "src/evee/config/models.py",
            "packages/evee-azureml/src/evee_azureml/tracking.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "packages/evee-azureml/src/evee_azureml/auth.py",
            "packages/evee-azureml/pyproject.toml",
            "src/evee/execution/experiment_runner.py",
            "infra/terraform/modules/ai-foundry/main.tf",
            "infra/terraform/modules/ai-foundry/variables.tf",
            "infra/terraform/modules/ai-foundry/outputs.tf",
            "infra/terraform/generate-env.sh",
            "docs/backends/overview.md",
            "docs/backends/custom-backends.md",
            "docs/advanced/infrastructure.md",
            "docs/user-guide/configuration.md",
            "pyproject.toml",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/tracking.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to `VALID_TRACKING_BACKENDS`',
            },
            {
                "path": "src/evee/cli/commands/compute.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to `VALID_COMPUTE_BACKENDS`, `BACKEND_PACKAGES`',
            },
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "Foundry evaluator metric scaffolding",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "E",
                "relevance": "`TEMPLATE_TYPE_FOUNDRY` constant",
            },
            {
                "path": "src/evee/cli/azure_evaluators.json",
                "category": "E",
                "relevance": "Azure AI Foundry evaluator metadata",
            },
            {
                "path": "src/evee/core/telemetry.py",
                "category": "E",
                "relevance": "Azure partner telemetry headers for Foundry",
            },
            {
                "path": "Makefile",
                "category": "E",
                "relevance": "`setup-foundry`, `test-foundry` targets",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend protocol — interface only, unchanged",
            },
            {
                "path": "src/evee/tracking/factory.py",
                "category": "C",
                "relevance": "Factory uses `entry_points()` dynamically — no changes",
            },
            {
                "path": "src/evee/tracking/events.py",
                "category": "C",
                "relevance": "Tracking event definitions — existing events sufficient",
            },
            {
                "path": "src/evee/tracking/__init__.py",
                "category": "C",
                "relevance": "Tracking public API — unchanged",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — interface only, unchanged",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": '`ComputeBackendConfig`, `TrackingBackendConfig` — uses `extra="allow"`, no changes',
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/tracking.py",
                "category": "C",
                "relevance": "AzureML tracking — reference pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "AzureML compute — reference pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "AzureML config models — config pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/auth.py",
                "category": "C",
                "relevance": "Azure identity auth — auth pattern",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points pattern for backends",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Uses `entry_points()` to discover backends — no changes",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/main.tf",
                "category": "S",
                "relevance": "Foundry Hub and Project resources",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/variables.tf",
                "category": "S",
                "relevance": "Foundry module variables",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/outputs.tf",
                "category": "S",
                "relevance": "Foundry endpoints",
            },
            {
                "path": "infra/terraform/generate-env.sh",
                "category": "S",
                "relevance": "AZURE_AI_FOUNDRY_PROJECT_ENDPOINT generation",
            },
            {"path": "docs/backends/overview.md", "category": "S", "relevance": "Backend overview"},
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Custom backend implementation guide",
            },
            {
                "path": "docs/advanced/infrastructure.md",
                "category": "S",
                "relevance": "Terraform infrastructure docs",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
            {"path": "pyproject.toml", "category": "S", "relevance": "Core entry points"},
        ],
        "difficulty": "complex",
    },
    {
        "issue": "72",
        "title": "Azure AI Foundry Integration",
        "query_level": "Q2",
        "task": "I want to add Azure AI Foundry as a backend for Evee, similar to the existing Azure ML backend. I need to understand the compute and tracking backend plugin architecture, the AzureML reference implementation, Terraform modules for Foundry, and the CLI commands for backend management.",
        "gt_files": [
            "src/evee/cli/commands/tracking.py",
            "src/evee/cli/commands/compute.py",
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/constants.py",
            "src/evee/cli/azure_evaluators.json",
            "src/evee/core/telemetry.py",
            "Makefile",
            "src/evee/tracking/backend.py",
            "src/evee/tracking/factory.py",
            "src/evee/tracking/events.py",
            "src/evee/tracking/__init__.py",
            "src/evee/compute/backend.py",
            "src/evee/config/models.py",
            "packages/evee-azureml/src/evee_azureml/tracking.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "packages/evee-azureml/src/evee_azureml/auth.py",
            "packages/evee-azureml/pyproject.toml",
            "src/evee/execution/experiment_runner.py",
            "infra/terraform/modules/ai-foundry/main.tf",
            "infra/terraform/modules/ai-foundry/variables.tf",
            "infra/terraform/modules/ai-foundry/outputs.tf",
            "infra/terraform/generate-env.sh",
            "docs/backends/overview.md",
            "docs/backends/custom-backends.md",
            "docs/advanced/infrastructure.md",
            "docs/user-guide/configuration.md",
            "pyproject.toml",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/tracking.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to `VALID_TRACKING_BACKENDS`',
            },
            {
                "path": "src/evee/cli/commands/compute.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to `VALID_COMPUTE_BACKENDS`, `BACKEND_PACKAGES`',
            },
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "Foundry evaluator metric scaffolding",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "E",
                "relevance": "`TEMPLATE_TYPE_FOUNDRY` constant",
            },
            {
                "path": "src/evee/cli/azure_evaluators.json",
                "category": "E",
                "relevance": "Azure AI Foundry evaluator metadata",
            },
            {
                "path": "src/evee/core/telemetry.py",
                "category": "E",
                "relevance": "Azure partner telemetry headers for Foundry",
            },
            {
                "path": "Makefile",
                "category": "E",
                "relevance": "`setup-foundry`, `test-foundry` targets",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend protocol — interface only, unchanged",
            },
            {
                "path": "src/evee/tracking/factory.py",
                "category": "C",
                "relevance": "Factory uses `entry_points()` dynamically — no changes",
            },
            {
                "path": "src/evee/tracking/events.py",
                "category": "C",
                "relevance": "Tracking event definitions — existing events sufficient",
            },
            {
                "path": "src/evee/tracking/__init__.py",
                "category": "C",
                "relevance": "Tracking public API — unchanged",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — interface only, unchanged",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": '`ComputeBackendConfig`, `TrackingBackendConfig` — uses `extra="allow"`, no changes',
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/tracking.py",
                "category": "C",
                "relevance": "AzureML tracking — reference pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "AzureML compute — reference pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "AzureML config models — config pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/auth.py",
                "category": "C",
                "relevance": "Azure identity auth — auth pattern",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points pattern for backends",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Uses `entry_points()` to discover backends — no changes",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/main.tf",
                "category": "S",
                "relevance": "Foundry Hub and Project resources",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/variables.tf",
                "category": "S",
                "relevance": "Foundry module variables",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/outputs.tf",
                "category": "S",
                "relevance": "Foundry endpoints",
            },
            {
                "path": "infra/terraform/generate-env.sh",
                "category": "S",
                "relevance": "AZURE_AI_FOUNDRY_PROJECT_ENDPOINT generation",
            },
            {"path": "docs/backends/overview.md", "category": "S", "relevance": "Backend overview"},
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Custom backend implementation guide",
            },
            {
                "path": "docs/advanced/infrastructure.md",
                "category": "S",
                "relevance": "Terraform infrastructure docs",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
            {"path": "pyproject.toml", "category": "S", "relevance": "Core entry points"},
        ],
        "difficulty": "complex",
    },
    {
        "issue": "72",
        "title": "Azure AI Foundry Integration",
        "query_level": "Q3",
        "task": "How do I add a new backend to Evee for Azure AI Foundry? I need both tracking and compute support. What's the pattern for backend plugins and where is the existing Azure ML code?",
        "gt_files": [
            "src/evee/cli/commands/tracking.py",
            "src/evee/cli/commands/compute.py",
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/constants.py",
            "src/evee/cli/azure_evaluators.json",
            "src/evee/core/telemetry.py",
            "Makefile",
            "src/evee/tracking/backend.py",
            "src/evee/tracking/factory.py",
            "src/evee/tracking/events.py",
            "src/evee/tracking/__init__.py",
            "src/evee/compute/backend.py",
            "src/evee/config/models.py",
            "packages/evee-azureml/src/evee_azureml/tracking.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "packages/evee-azureml/src/evee_azureml/auth.py",
            "packages/evee-azureml/pyproject.toml",
            "src/evee/execution/experiment_runner.py",
            "infra/terraform/modules/ai-foundry/main.tf",
            "infra/terraform/modules/ai-foundry/variables.tf",
            "infra/terraform/modules/ai-foundry/outputs.tf",
            "infra/terraform/generate-env.sh",
            "docs/backends/overview.md",
            "docs/backends/custom-backends.md",
            "docs/advanced/infrastructure.md",
            "docs/user-guide/configuration.md",
            "pyproject.toml",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/tracking.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to `VALID_TRACKING_BACKENDS`',
            },
            {
                "path": "src/evee/cli/commands/compute.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to `VALID_COMPUTE_BACKENDS`, `BACKEND_PACKAGES`',
            },
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "Foundry evaluator metric scaffolding",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "E",
                "relevance": "`TEMPLATE_TYPE_FOUNDRY` constant",
            },
            {
                "path": "src/evee/cli/azure_evaluators.json",
                "category": "E",
                "relevance": "Azure AI Foundry evaluator metadata",
            },
            {
                "path": "src/evee/core/telemetry.py",
                "category": "E",
                "relevance": "Azure partner telemetry headers for Foundry",
            },
            {
                "path": "Makefile",
                "category": "E",
                "relevance": "`setup-foundry`, `test-foundry` targets",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend protocol — interface only, unchanged",
            },
            {
                "path": "src/evee/tracking/factory.py",
                "category": "C",
                "relevance": "Factory uses `entry_points()` dynamically — no changes",
            },
            {
                "path": "src/evee/tracking/events.py",
                "category": "C",
                "relevance": "Tracking event definitions — existing events sufficient",
            },
            {
                "path": "src/evee/tracking/__init__.py",
                "category": "C",
                "relevance": "Tracking public API — unchanged",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — interface only, unchanged",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": '`ComputeBackendConfig`, `TrackingBackendConfig` — uses `extra="allow"`, no changes',
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/tracking.py",
                "category": "C",
                "relevance": "AzureML tracking — reference pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "AzureML compute — reference pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "AzureML config models — config pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/auth.py",
                "category": "C",
                "relevance": "Azure identity auth — auth pattern",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points pattern for backends",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Uses `entry_points()` to discover backends — no changes",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/main.tf",
                "category": "S",
                "relevance": "Foundry Hub and Project resources",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/variables.tf",
                "category": "S",
                "relevance": "Foundry module variables",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/outputs.tf",
                "category": "S",
                "relevance": "Foundry endpoints",
            },
            {
                "path": "infra/terraform/generate-env.sh",
                "category": "S",
                "relevance": "AZURE_AI_FOUNDRY_PROJECT_ENDPOINT generation",
            },
            {"path": "docs/backends/overview.md", "category": "S", "relevance": "Backend overview"},
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Custom backend implementation guide",
            },
            {
                "path": "docs/advanced/infrastructure.md",
                "category": "S",
                "relevance": "Terraform infrastructure docs",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
            {"path": "pyproject.toml", "category": "S", "relevance": "Core entry points"},
        ],
        "difficulty": "complex",
    },
    {
        "issue": "108",
        "title": "Implement Integration Tests with Mocked Services",
        "query_level": "Q1",
        "task": "I need to implement integration tests that run Evee's full evaluation pipeline end-to-end with mocked services. The tests should load a real config, create mock `@model` and `@metric` decorated classes with deterministic responses, use `NoOpTrackingBackend`, run through `ExperimentRunner` and `ModelEvaluator`, and validate output artifacts. No external network calls. I need the existing integration test patterns in `tests/evee/integration/`, the evaluation pipeline code, config models, dataset loading, and CI workflow configuration to add these to PR gating.",
        "gt_files": [
            "pyproject.toml",
            "Makefile",
            ".github/workflows/ci.yml",
            ".github/workflows/integration-tests.yml",
            "tests/evee/integration/helpers.py",
            "tests/evee/integration/test_example_evaluate_locally_core.py",
            "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
            "tests/evee/integration/test_model_cleanup.py",
            "tests/evee/conftest.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/core/base_model.py",
            "src/evee/core/base_metric.py",
            "src/evee/core/base_dataset.py",
            "src/evee/datasets/jsonl_dataset.py",
            "src/evee/tracking/backends/no_op_fallback_backend.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "example/experiment/config.yaml",
            "example/experiment/data/sample_dataset.jsonl",
        ],
        "gt_categories": [
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Pytest config, `mocked-integration` marker",
            },
            {"path": "Makefile", "category": "E", "relevance": "`test-mocked-integration` target"},
            {
                "path": ".github/workflows/ci.yml",
                "category": "E",
                "relevance": "CI config — add mocked integration to PR gating",
            },
            {
                "path": ".github/workflows/integration-tests.yml",
                "category": "E",
                "relevance": "Integration test workflow — add mocked tier",
            },
            {
                "path": "tests/evee/integration/helpers.py",
                "category": "C",
                "relevance": "Integration test helpers: `run_evee_evaluation()`, `EvaluationResult`",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_core.py",
                "category": "C",
                "relevance": "Existing local integration test pattern",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
                "category": "C",
                "relevance": "Existing MLflow integration test",
            },
            {
                "path": "tests/evee/integration/test_model_cleanup.py",
                "category": "C",
                "relevance": "Existing mocked integration test pattern",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Shared fixtures: `mock_config_dict`, `mock_config_yaml`, `evaluator_with_setup`",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "C",
                "relevance": "Core evaluator pipeline — code exercised, not modified",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "Config models for loading real config — not modified",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "ExperimentRunner — top-level execution flow, not modified",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`@model` decorator for mocked model registration — not modified",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`@metric` decorator for mocked metrics — not modified",
            },
            {
                "path": "src/evee/core/base_dataset.py",
                "category": "C",
                "relevance": "`@dataset` decorator for mock dataset — not modified",
            },
            {
                "path": "src/evee/datasets/jsonl_dataset.py",
                "category": "C",
                "relevance": "JSONL dataset loader — not modified",
            },
            {
                "path": "src/evee/tracking/backends/no_op_fallback_backend.py",
                "category": "C",
                "relevance": "NoOp backend for no-network tests — not modified",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Output validation reference — not modified",
            },
            {
                "path": "example/experiment/config.yaml",
                "category": "S",
                "relevance": "Reference config",
            },
            {
                "path": "example/experiment/data/sample_dataset.jsonl",
                "category": "S",
                "relevance": "Reference dataset",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "108",
        "title": "Implement Integration Tests with Mocked Services",
        "query_level": "Q2",
        "task": "Add mocked integration tests for Evee that test the full evaluation flow without external services. I need to understand the existing integration test structure, how the evaluation pipeline works end-to-end, how to create mock models/metrics, and how to wire them into the config and execution flow.",
        "gt_files": [
            "pyproject.toml",
            "Makefile",
            ".github/workflows/ci.yml",
            ".github/workflows/integration-tests.yml",
            "tests/evee/integration/helpers.py",
            "tests/evee/integration/test_example_evaluate_locally_core.py",
            "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
            "tests/evee/integration/test_model_cleanup.py",
            "tests/evee/conftest.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/core/base_model.py",
            "src/evee/core/base_metric.py",
            "src/evee/core/base_dataset.py",
            "src/evee/datasets/jsonl_dataset.py",
            "src/evee/tracking/backends/no_op_fallback_backend.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "example/experiment/config.yaml",
            "example/experiment/data/sample_dataset.jsonl",
        ],
        "gt_categories": [
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Pytest config, `mocked-integration` marker",
            },
            {"path": "Makefile", "category": "E", "relevance": "`test-mocked-integration` target"},
            {
                "path": ".github/workflows/ci.yml",
                "category": "E",
                "relevance": "CI config — add mocked integration to PR gating",
            },
            {
                "path": ".github/workflows/integration-tests.yml",
                "category": "E",
                "relevance": "Integration test workflow — add mocked tier",
            },
            {
                "path": "tests/evee/integration/helpers.py",
                "category": "C",
                "relevance": "Integration test helpers: `run_evee_evaluation()`, `EvaluationResult`",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_core.py",
                "category": "C",
                "relevance": "Existing local integration test pattern",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
                "category": "C",
                "relevance": "Existing MLflow integration test",
            },
            {
                "path": "tests/evee/integration/test_model_cleanup.py",
                "category": "C",
                "relevance": "Existing mocked integration test pattern",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Shared fixtures: `mock_config_dict`, `mock_config_yaml`, `evaluator_with_setup`",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "C",
                "relevance": "Core evaluator pipeline — code exercised, not modified",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "Config models for loading real config — not modified",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "ExperimentRunner — top-level execution flow, not modified",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`@model` decorator for mocked model registration — not modified",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`@metric` decorator for mocked metrics — not modified",
            },
            {
                "path": "src/evee/core/base_dataset.py",
                "category": "C",
                "relevance": "`@dataset` decorator for mock dataset — not modified",
            },
            {
                "path": "src/evee/datasets/jsonl_dataset.py",
                "category": "C",
                "relevance": "JSONL dataset loader — not modified",
            },
            {
                "path": "src/evee/tracking/backends/no_op_fallback_backend.py",
                "category": "C",
                "relevance": "NoOp backend for no-network tests — not modified",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Output validation reference — not modified",
            },
            {
                "path": "example/experiment/config.yaml",
                "category": "S",
                "relevance": "Reference config",
            },
            {
                "path": "example/experiment/data/sample_dataset.jsonl",
                "category": "S",
                "relevance": "Reference dataset",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "108",
        "title": "Implement Integration Tests with Mocked Services",
        "query_level": "Q3",
        "task": "I want to add end-to-end tests for Evee that don't call any external APIs. They should test the full evaluation flow with mocked models. Where are the existing integration tests and how does the evaluation pipeline work?",
        "gt_files": [
            "pyproject.toml",
            "Makefile",
            ".github/workflows/ci.yml",
            ".github/workflows/integration-tests.yml",
            "tests/evee/integration/helpers.py",
            "tests/evee/integration/test_example_evaluate_locally_core.py",
            "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
            "tests/evee/integration/test_model_cleanup.py",
            "tests/evee/conftest.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/core/base_model.py",
            "src/evee/core/base_metric.py",
            "src/evee/core/base_dataset.py",
            "src/evee/datasets/jsonl_dataset.py",
            "src/evee/tracking/backends/no_op_fallback_backend.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "example/experiment/config.yaml",
            "example/experiment/data/sample_dataset.jsonl",
        ],
        "gt_categories": [
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Pytest config, `mocked-integration` marker",
            },
            {"path": "Makefile", "category": "E", "relevance": "`test-mocked-integration` target"},
            {
                "path": ".github/workflows/ci.yml",
                "category": "E",
                "relevance": "CI config — add mocked integration to PR gating",
            },
            {
                "path": ".github/workflows/integration-tests.yml",
                "category": "E",
                "relevance": "Integration test workflow — add mocked tier",
            },
            {
                "path": "tests/evee/integration/helpers.py",
                "category": "C",
                "relevance": "Integration test helpers: `run_evee_evaluation()`, `EvaluationResult`",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_core.py",
                "category": "C",
                "relevance": "Existing local integration test pattern",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
                "category": "C",
                "relevance": "Existing MLflow integration test",
            },
            {
                "path": "tests/evee/integration/test_model_cleanup.py",
                "category": "C",
                "relevance": "Existing mocked integration test pattern",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Shared fixtures: `mock_config_dict`, `mock_config_yaml`, `evaluator_with_setup`",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "C",
                "relevance": "Core evaluator pipeline — code exercised, not modified",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "Config models for loading real config — not modified",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "ExperimentRunner — top-level execution flow, not modified",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`@model` decorator for mocked model registration — not modified",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`@metric` decorator for mocked metrics — not modified",
            },
            {
                "path": "src/evee/core/base_dataset.py",
                "category": "C",
                "relevance": "`@dataset` decorator for mock dataset — not modified",
            },
            {
                "path": "src/evee/datasets/jsonl_dataset.py",
                "category": "C",
                "relevance": "JSONL dataset loader — not modified",
            },
            {
                "path": "src/evee/tracking/backends/no_op_fallback_backend.py",
                "category": "C",
                "relevance": "NoOp backend for no-network tests — not modified",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Output validation reference — not modified",
            },
            {
                "path": "example/experiment/config.yaml",
                "category": "S",
                "relevance": "Reference config",
            },
            {
                "path": "example/experiment/data/sample_dataset.jsonl",
                "category": "S",
                "relevance": "Reference dataset",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "172",
        "title": "MCP Server - Documentation, Testing & Integration",
        "query_level": "Q1",
        "task": "I need to create comprehensive documentation and end-to-end testing for Evee's MCP server. Documentation should cover installation, MCP client configuration (VS Code, Cursor, Claude Desktop), user guide with example workflows, security documentation, and API reference for all 5 tools and resources. Testing should include IDE client testing, project configuration testing (venv, conda), backend testing (local, Azure ML, MLflow), security testing (path traversal, injection), and performance testing. I need the full MCP server implementation, all tools and resources, existing tests/fixtures, current docs, the execution infrastructure, and the results viewer UI.",
        "gt_files": [
            "tests/test_mcp_server.py",
            "tests/mcp/__init__.py",
            "tests/mcp/conftest.py",
            "tests/mcp/test_e2e.py",
            "tests/mcp/test_resources.py",
            "tests/mcp/test_tools.py",
            "docs/user-guide/mcp-server.md",
            "docs/design/mcp-server.md",
            "docs/user-guide/cli.md",
            "docs/user-guide/configuration.md",
            "docs/troubleshooting.md",
            "src/evee/mcp/README.md",
            "mkdocs.yml",
            "src/evee/mcp/server.py",
            "src/evee/mcp/__init__.py",
            "src/evee/mcp/constants.py",
            "src/evee/mcp/tools/__init__.py",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/validation.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/resources/__init__.py",
            "src/evee/mcp/resources/base.py",
            "src/evee/mcp/resources/evaluators.py",
            "src/evee/mcp/resources/patterns.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/connections.py",
            "src/evee/mcp/resources/model_patterns.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "src/evee/mcp/resources/app_viewer.py",
            "src/evee/execution/runner.py",
            "src/evee/execution/environment.py",
            "src/evee/execution/experiment_runner.py",
            "pyproject.toml",
        ],
        "gt_categories": [
            {
                "path": "tests/test_mcp_server.py",
                "category": "E",
                "relevance": "Basic MCP server tests — enhance coverage",
            },
            {"path": "tests/mcp/__init__.py", "category": "E", "relevance": "MCP test package"},
            {
                "path": "tests/mcp/conftest.py",
                "category": "E",
                "relevance": "MCP test fixtures — enhance/add fixtures",
            },
            {
                "path": "tests/mcp/test_e2e.py",
                "category": "E",
                "relevance": "E2E tests — add IDE client, security, performance tests",
            },
            {
                "path": "tests/mcp/test_resources.py",
                "category": "E",
                "relevance": "Resource tests — enhance coverage",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "E",
                "relevance": "Tool tests — enhance coverage",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "E",
                "relevance": "MCP user docs — installation guide, client config, workflows",
            },
            {
                "path": "docs/design/mcp-server.md",
                "category": "E",
                "relevance": "MCP design document — security docs, API reference",
            },
            {
                "path": "docs/user-guide/cli.md",
                "category": "E",
                "relevance": "CLI reference (view-results MCP)",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "E",
                "relevance": "Config reference — MCP config examples",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "E",
                "relevance": "MCP troubleshooting entries",
            },
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP README — update/enhance",
            },
            {"path": "mkdocs.yml", "category": "E", "relevance": "Docs navigation — add MCP pages"},
            {
                "path": "src/evee/mcp/server.py",
                "category": "C",
                "relevance": "Main MCP server — read for understanding",
            },
            {
                "path": "src/evee/mcp/__init__.py",
                "category": "C",
                "relevance": "MCP package exports",
            },
            {"path": "src/evee/mcp/constants.py", "category": "C", "relevance": "Constants"},
            {
                "path": "src/evee/mcp/tools/__init__.py",
                "category": "C",
                "relevance": "Tools registry",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "C",
                "relevance": "BaseTool classes",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "RunExperimentTool",
            },
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "C",
                "relevance": "ValidateConfigTool",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "C",
                "relevance": "ListComponentsTool",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "ViewResultsTool",
            },
            {
                "path": "src/evee/mcp/resources/__init__.py",
                "category": "C",
                "relevance": "Resources registry",
            },
            {
                "path": "src/evee/mcp/resources/base.py",
                "category": "C",
                "relevance": "BaseResource",
            },
            {
                "path": "src/evee/mcp/resources/evaluators.py",
                "category": "C",
                "relevance": "Evaluators resource",
            },
            {
                "path": "src/evee/mcp/resources/patterns.py",
                "category": "C",
                "relevance": "Patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "C",
                "relevance": "Config schema resource",
            },
            {
                "path": "src/evee/mcp/resources/connections.py",
                "category": "C",
                "relevance": "Connections resource",
            },
            {
                "path": "src/evee/mcp/resources/model_patterns.py",
                "category": "C",
                "relevance": "Model patterns",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "C",
                "relevance": "Metric patterns",
            },
            {
                "path": "src/evee/mcp/resources/app_viewer.py",
                "category": "C",
                "relevance": "Results viewer",
            },
            {
                "path": "src/evee/execution/runner.py",
                "category": "C",
                "relevance": "ExecutionRunner used by tools",
            },
            {
                "path": "src/evee/execution/environment.py",
                "category": "C",
                "relevance": "EnvironmentResolver",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "ExperimentRunner used by run_experiment",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "MCP dependency, CLI entry point",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "172",
        "title": "MCP Server - Documentation, Testing & Integration",
        "query_level": "Q2",
        "task": "I'm creating documentation and tests for Evee's MCP server. I need to understand all the MCP tools, resources, how they work, existing test patterns, current documentation, and the execution infrastructure. I also need to test security aspects like path traversal prevention and performance with large datasets.",
        "gt_files": [
            "tests/test_mcp_server.py",
            "tests/mcp/__init__.py",
            "tests/mcp/conftest.py",
            "tests/mcp/test_e2e.py",
            "tests/mcp/test_resources.py",
            "tests/mcp/test_tools.py",
            "docs/user-guide/mcp-server.md",
            "docs/design/mcp-server.md",
            "docs/user-guide/cli.md",
            "docs/user-guide/configuration.md",
            "docs/troubleshooting.md",
            "src/evee/mcp/README.md",
            "mkdocs.yml",
            "src/evee/mcp/server.py",
            "src/evee/mcp/__init__.py",
            "src/evee/mcp/constants.py",
            "src/evee/mcp/tools/__init__.py",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/validation.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/resources/__init__.py",
            "src/evee/mcp/resources/base.py",
            "src/evee/mcp/resources/evaluators.py",
            "src/evee/mcp/resources/patterns.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/connections.py",
            "src/evee/mcp/resources/model_patterns.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "src/evee/mcp/resources/app_viewer.py",
            "src/evee/execution/runner.py",
            "src/evee/execution/environment.py",
            "src/evee/execution/experiment_runner.py",
            "pyproject.toml",
        ],
        "gt_categories": [
            {
                "path": "tests/test_mcp_server.py",
                "category": "E",
                "relevance": "Basic MCP server tests — enhance coverage",
            },
            {"path": "tests/mcp/__init__.py", "category": "E", "relevance": "MCP test package"},
            {
                "path": "tests/mcp/conftest.py",
                "category": "E",
                "relevance": "MCP test fixtures — enhance/add fixtures",
            },
            {
                "path": "tests/mcp/test_e2e.py",
                "category": "E",
                "relevance": "E2E tests — add IDE client, security, performance tests",
            },
            {
                "path": "tests/mcp/test_resources.py",
                "category": "E",
                "relevance": "Resource tests — enhance coverage",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "E",
                "relevance": "Tool tests — enhance coverage",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "E",
                "relevance": "MCP user docs — installation guide, client config, workflows",
            },
            {
                "path": "docs/design/mcp-server.md",
                "category": "E",
                "relevance": "MCP design document — security docs, API reference",
            },
            {
                "path": "docs/user-guide/cli.md",
                "category": "E",
                "relevance": "CLI reference (view-results MCP)",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "E",
                "relevance": "Config reference — MCP config examples",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "E",
                "relevance": "MCP troubleshooting entries",
            },
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP README — update/enhance",
            },
            {"path": "mkdocs.yml", "category": "E", "relevance": "Docs navigation — add MCP pages"},
            {
                "path": "src/evee/mcp/server.py",
                "category": "C",
                "relevance": "Main MCP server — read for understanding",
            },
            {
                "path": "src/evee/mcp/__init__.py",
                "category": "C",
                "relevance": "MCP package exports",
            },
            {"path": "src/evee/mcp/constants.py", "category": "C", "relevance": "Constants"},
            {
                "path": "src/evee/mcp/tools/__init__.py",
                "category": "C",
                "relevance": "Tools registry",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "C",
                "relevance": "BaseTool classes",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "RunExperimentTool",
            },
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "C",
                "relevance": "ValidateConfigTool",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "C",
                "relevance": "ListComponentsTool",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "ViewResultsTool",
            },
            {
                "path": "src/evee/mcp/resources/__init__.py",
                "category": "C",
                "relevance": "Resources registry",
            },
            {
                "path": "src/evee/mcp/resources/base.py",
                "category": "C",
                "relevance": "BaseResource",
            },
            {
                "path": "src/evee/mcp/resources/evaluators.py",
                "category": "C",
                "relevance": "Evaluators resource",
            },
            {
                "path": "src/evee/mcp/resources/patterns.py",
                "category": "C",
                "relevance": "Patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "C",
                "relevance": "Config schema resource",
            },
            {
                "path": "src/evee/mcp/resources/connections.py",
                "category": "C",
                "relevance": "Connections resource",
            },
            {
                "path": "src/evee/mcp/resources/model_patterns.py",
                "category": "C",
                "relevance": "Model patterns",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "C",
                "relevance": "Metric patterns",
            },
            {
                "path": "src/evee/mcp/resources/app_viewer.py",
                "category": "C",
                "relevance": "Results viewer",
            },
            {
                "path": "src/evee/execution/runner.py",
                "category": "C",
                "relevance": "ExecutionRunner used by tools",
            },
            {
                "path": "src/evee/execution/environment.py",
                "category": "C",
                "relevance": "EnvironmentResolver",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "ExperimentRunner used by run_experiment",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "MCP dependency, CLI entry point",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "172",
        "title": "MCP Server - Documentation, Testing & Integration",
        "query_level": "Q3",
        "task": "I need to document and test the Evee MCP server thoroughly. Where is all the MCP code, what tools does it expose, and what existing docs and tests are there? What security considerations should I address?",
        "gt_files": [
            "tests/test_mcp_server.py",
            "tests/mcp/__init__.py",
            "tests/mcp/conftest.py",
            "tests/mcp/test_e2e.py",
            "tests/mcp/test_resources.py",
            "tests/mcp/test_tools.py",
            "docs/user-guide/mcp-server.md",
            "docs/design/mcp-server.md",
            "docs/user-guide/cli.md",
            "docs/user-guide/configuration.md",
            "docs/troubleshooting.md",
            "src/evee/mcp/README.md",
            "mkdocs.yml",
            "src/evee/mcp/server.py",
            "src/evee/mcp/__init__.py",
            "src/evee/mcp/constants.py",
            "src/evee/mcp/tools/__init__.py",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/validation.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/resources/__init__.py",
            "src/evee/mcp/resources/base.py",
            "src/evee/mcp/resources/evaluators.py",
            "src/evee/mcp/resources/patterns.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/connections.py",
            "src/evee/mcp/resources/model_patterns.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "src/evee/mcp/resources/app_viewer.py",
            "src/evee/execution/runner.py",
            "src/evee/execution/environment.py",
            "src/evee/execution/experiment_runner.py",
            "pyproject.toml",
        ],
        "gt_categories": [
            {
                "path": "tests/test_mcp_server.py",
                "category": "E",
                "relevance": "Basic MCP server tests — enhance coverage",
            },
            {"path": "tests/mcp/__init__.py", "category": "E", "relevance": "MCP test package"},
            {
                "path": "tests/mcp/conftest.py",
                "category": "E",
                "relevance": "MCP test fixtures — enhance/add fixtures",
            },
            {
                "path": "tests/mcp/test_e2e.py",
                "category": "E",
                "relevance": "E2E tests — add IDE client, security, performance tests",
            },
            {
                "path": "tests/mcp/test_resources.py",
                "category": "E",
                "relevance": "Resource tests — enhance coverage",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "E",
                "relevance": "Tool tests — enhance coverage",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "E",
                "relevance": "MCP user docs — installation guide, client config, workflows",
            },
            {
                "path": "docs/design/mcp-server.md",
                "category": "E",
                "relevance": "MCP design document — security docs, API reference",
            },
            {
                "path": "docs/user-guide/cli.md",
                "category": "E",
                "relevance": "CLI reference (view-results MCP)",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "E",
                "relevance": "Config reference — MCP config examples",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "E",
                "relevance": "MCP troubleshooting entries",
            },
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP README — update/enhance",
            },
            {"path": "mkdocs.yml", "category": "E", "relevance": "Docs navigation — add MCP pages"},
            {
                "path": "src/evee/mcp/server.py",
                "category": "C",
                "relevance": "Main MCP server — read for understanding",
            },
            {
                "path": "src/evee/mcp/__init__.py",
                "category": "C",
                "relevance": "MCP package exports",
            },
            {"path": "src/evee/mcp/constants.py", "category": "C", "relevance": "Constants"},
            {
                "path": "src/evee/mcp/tools/__init__.py",
                "category": "C",
                "relevance": "Tools registry",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "C",
                "relevance": "BaseTool classes",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "RunExperimentTool",
            },
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "C",
                "relevance": "ValidateConfigTool",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "C",
                "relevance": "ListComponentsTool",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "ViewResultsTool",
            },
            {
                "path": "src/evee/mcp/resources/__init__.py",
                "category": "C",
                "relevance": "Resources registry",
            },
            {
                "path": "src/evee/mcp/resources/base.py",
                "category": "C",
                "relevance": "BaseResource",
            },
            {
                "path": "src/evee/mcp/resources/evaluators.py",
                "category": "C",
                "relevance": "Evaluators resource",
            },
            {
                "path": "src/evee/mcp/resources/patterns.py",
                "category": "C",
                "relevance": "Patterns resource",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "C",
                "relevance": "Config schema resource",
            },
            {
                "path": "src/evee/mcp/resources/connections.py",
                "category": "C",
                "relevance": "Connections resource",
            },
            {
                "path": "src/evee/mcp/resources/model_patterns.py",
                "category": "C",
                "relevance": "Model patterns",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "C",
                "relevance": "Metric patterns",
            },
            {
                "path": "src/evee/mcp/resources/app_viewer.py",
                "category": "C",
                "relevance": "Results viewer",
            },
            {
                "path": "src/evee/execution/runner.py",
                "category": "C",
                "relevance": "ExecutionRunner used by tools",
            },
            {
                "path": "src/evee/execution/environment.py",
                "category": "C",
                "relevance": "EnvironmentResolver",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "ExperimentRunner used by run_experiment",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "MCP dependency, CLI entry point",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "191",
        "title": "Azure AI Foundry: Tracking and Compute Backend Support (Phase 1)",
        "query_level": "Q1",
        "task": "I need to implement Azure AI Foundry as both a tracking and compute backend for Evee, following the `packages/evee-azureml/` pattern exactly. The tracking backend should implement the `TrackingBackend` protocol and send metrics to Foundry dashboards. The compute backend should implement `ComputeBackend` ABC and submit evaluations to Foundry infrastructure. I need to register via entry points, add CLI commands for tracking/compute backend selection, add a `make run_foundry` target in the example Makefile, configure Terraform infrastructure, and document region limitations for LLM-based evaluators.",
        "gt_files": [
            "src/evee/cli/commands/tracking.py",
            "src/evee/cli/commands/compute.py",
            "src/evee/cli/commands/new.py",
            "Makefile",
            "example/Makefile",
            "packages/evee-azureml/src/evee_azureml/tracking.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "packages/evee-azureml/src/evee_azureml/auth.py",
            "packages/evee-azureml/pyproject.toml",
            "packages/evee-azureml/tests/test_tracking.py",
            "packages/evee-azureml/tests/test_azureml_backend_pkg.py",
            "src/evee/tracking/backend.py",
            "src/evee/tracking/factory.py",
            "src/evee/tracking/events.py",
            "src/evee/compute/backend.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "tests/evee/tracking/test_tracking_factory.py",
            "pyproject.toml",
            "example/experiment/config.azureml.yaml",
            "infra/terraform/modules/ai-foundry/main.tf",
            "infra/terraform/modules/ai-foundry/variables.tf",
            "infra/terraform/modules/ai-foundry/outputs.tf",
            "infra/terraform/main.tf",
            "docs/backends/overview.md",
            "docs/backends/custom-backends.md",
            "docs/user-guide/configuration.md",
            "docs/user-guide/cli.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/tracking.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to backend lists',
            },
            {
                "path": "src/evee/cli/commands/compute.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to backend lists',
            },
            {
                "path": "src/evee/cli/commands/new.py",
                "category": "E",
                "relevance": "Add Foundry overlay/template option",
            },
            {
                "path": "Makefile",
                "category": "E",
                "relevance": "`setup-foundry`, `test-foundry` targets",
            },
            {"path": "example/Makefile", "category": "E", "relevance": "`run_foundry` target"},
            {
                "path": "packages/evee-azureml/src/evee_azureml/tracking.py",
                "category": "C",
                "relevance": "Reference tracking backend",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "Reference compute backend",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "Reference config models",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/auth.py",
                "category": "C",
                "relevance": "Azure identity auth pattern",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points pattern",
            },
            {
                "path": "packages/evee-azureml/tests/test_tracking.py",
                "category": "C",
                "relevance": "Tracking test patterns",
            },
            {
                "path": "packages/evee-azureml/tests/test_azureml_backend_pkg.py",
                "category": "C",
                "relevance": "Compute test patterns",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend protocol — unchanged",
            },
            {
                "path": "src/evee/tracking/factory.py",
                "category": "C",
                "relevance": "Backend factory — uses entry_points, unchanged",
            },
            {
                "path": "src/evee/tracking/events.py",
                "category": "C",
                "relevance": "Event types — sufficient for Foundry",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — unchanged",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": 'Config models — uses `extra="allow"`, unchanged',
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Backend loading via entry points — unchanged",
            },
            {
                "path": "tests/evee/tracking/test_tracking_factory.py",
                "category": "C",
                "relevance": "Factory tests pattern",
            },
            {"path": "pyproject.toml", "category": "S", "relevance": "Core entry points reference"},
            {
                "path": "example/experiment/config.azureml.yaml",
                "category": "S",
                "relevance": "Reference AzureML config pattern",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/main.tf",
                "category": "S",
                "relevance": "Foundry infrastructure",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/variables.tf",
                "category": "S",
                "relevance": "Foundry variables",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/outputs.tf",
                "category": "S",
                "relevance": "Foundry endpoints",
            },
            {"path": "infra/terraform/main.tf", "category": "S", "relevance": "Module invocation"},
            {"path": "docs/backends/overview.md", "category": "S", "relevance": "Backend overview"},
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Backend implementation guide",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
            {"path": "docs/user-guide/cli.md", "category": "S", "relevance": "CLI reference"},
        ],
        "difficulty": "complex",
    },
    {
        "issue": "191",
        "title": "Azure AI Foundry: Tracking and Compute Backend Support (Phase 1)",
        "query_level": "Q2",
        "task": "Add Azure AI Foundry as a tracking and compute backend for Evee. I need to follow the existing AzureML backend package pattern, understand the tracking/compute backend plugin architecture, update CLI commands, add Terraform infrastructure, and create documentation. What are the reference implementations and where do entry points get registered?",
        "gt_files": [
            "src/evee/cli/commands/tracking.py",
            "src/evee/cli/commands/compute.py",
            "src/evee/cli/commands/new.py",
            "Makefile",
            "example/Makefile",
            "packages/evee-azureml/src/evee_azureml/tracking.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "packages/evee-azureml/src/evee_azureml/auth.py",
            "packages/evee-azureml/pyproject.toml",
            "packages/evee-azureml/tests/test_tracking.py",
            "packages/evee-azureml/tests/test_azureml_backend_pkg.py",
            "src/evee/tracking/backend.py",
            "src/evee/tracking/factory.py",
            "src/evee/tracking/events.py",
            "src/evee/compute/backend.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "tests/evee/tracking/test_tracking_factory.py",
            "pyproject.toml",
            "example/experiment/config.azureml.yaml",
            "infra/terraform/modules/ai-foundry/main.tf",
            "infra/terraform/modules/ai-foundry/variables.tf",
            "infra/terraform/modules/ai-foundry/outputs.tf",
            "infra/terraform/main.tf",
            "docs/backends/overview.md",
            "docs/backends/custom-backends.md",
            "docs/user-guide/configuration.md",
            "docs/user-guide/cli.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/tracking.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to backend lists',
            },
            {
                "path": "src/evee/cli/commands/compute.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to backend lists',
            },
            {
                "path": "src/evee/cli/commands/new.py",
                "category": "E",
                "relevance": "Add Foundry overlay/template option",
            },
            {
                "path": "Makefile",
                "category": "E",
                "relevance": "`setup-foundry`, `test-foundry` targets",
            },
            {"path": "example/Makefile", "category": "E", "relevance": "`run_foundry` target"},
            {
                "path": "packages/evee-azureml/src/evee_azureml/tracking.py",
                "category": "C",
                "relevance": "Reference tracking backend",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "Reference compute backend",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "Reference config models",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/auth.py",
                "category": "C",
                "relevance": "Azure identity auth pattern",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points pattern",
            },
            {
                "path": "packages/evee-azureml/tests/test_tracking.py",
                "category": "C",
                "relevance": "Tracking test patterns",
            },
            {
                "path": "packages/evee-azureml/tests/test_azureml_backend_pkg.py",
                "category": "C",
                "relevance": "Compute test patterns",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend protocol — unchanged",
            },
            {
                "path": "src/evee/tracking/factory.py",
                "category": "C",
                "relevance": "Backend factory — uses entry_points, unchanged",
            },
            {
                "path": "src/evee/tracking/events.py",
                "category": "C",
                "relevance": "Event types — sufficient for Foundry",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — unchanged",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": 'Config models — uses `extra="allow"`, unchanged',
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Backend loading via entry points — unchanged",
            },
            {
                "path": "tests/evee/tracking/test_tracking_factory.py",
                "category": "C",
                "relevance": "Factory tests pattern",
            },
            {"path": "pyproject.toml", "category": "S", "relevance": "Core entry points reference"},
            {
                "path": "example/experiment/config.azureml.yaml",
                "category": "S",
                "relevance": "Reference AzureML config pattern",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/main.tf",
                "category": "S",
                "relevance": "Foundry infrastructure",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/variables.tf",
                "category": "S",
                "relevance": "Foundry variables",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/outputs.tf",
                "category": "S",
                "relevance": "Foundry endpoints",
            },
            {"path": "infra/terraform/main.tf", "category": "S", "relevance": "Module invocation"},
            {"path": "docs/backends/overview.md", "category": "S", "relevance": "Backend overview"},
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Backend implementation guide",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
            {"path": "docs/user-guide/cli.md", "category": "S", "relevance": "CLI reference"},
        ],
        "difficulty": "complex",
    },
    {
        "issue": "191",
        "title": "Azure AI Foundry: Tracking and Compute Backend Support (Phase 1)",
        "query_level": "Q3",
        "task": "I want to make Evee work with Azure AI Foundry for running evaluations and tracking metrics. There's already an Azure ML backend — I need to do something similar for Foundry. Where should I start and what's the backend plugin structure?",
        "gt_files": [
            "src/evee/cli/commands/tracking.py",
            "src/evee/cli/commands/compute.py",
            "src/evee/cli/commands/new.py",
            "Makefile",
            "example/Makefile",
            "packages/evee-azureml/src/evee_azureml/tracking.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "packages/evee-azureml/src/evee_azureml/auth.py",
            "packages/evee-azureml/pyproject.toml",
            "packages/evee-azureml/tests/test_tracking.py",
            "packages/evee-azureml/tests/test_azureml_backend_pkg.py",
            "src/evee/tracking/backend.py",
            "src/evee/tracking/factory.py",
            "src/evee/tracking/events.py",
            "src/evee/compute/backend.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "tests/evee/tracking/test_tracking_factory.py",
            "pyproject.toml",
            "example/experiment/config.azureml.yaml",
            "infra/terraform/modules/ai-foundry/main.tf",
            "infra/terraform/modules/ai-foundry/variables.tf",
            "infra/terraform/modules/ai-foundry/outputs.tf",
            "infra/terraform/main.tf",
            "docs/backends/overview.md",
            "docs/backends/custom-backends.md",
            "docs/user-guide/configuration.md",
            "docs/user-guide/cli.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/tracking.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to backend lists',
            },
            {
                "path": "src/evee/cli/commands/compute.py",
                "category": "E",
                "relevance": 'Add `"foundry"` to backend lists',
            },
            {
                "path": "src/evee/cli/commands/new.py",
                "category": "E",
                "relevance": "Add Foundry overlay/template option",
            },
            {
                "path": "Makefile",
                "category": "E",
                "relevance": "`setup-foundry`, `test-foundry` targets",
            },
            {"path": "example/Makefile", "category": "E", "relevance": "`run_foundry` target"},
            {
                "path": "packages/evee-azureml/src/evee_azureml/tracking.py",
                "category": "C",
                "relevance": "Reference tracking backend",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "Reference compute backend",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "Reference config models",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/auth.py",
                "category": "C",
                "relevance": "Azure identity auth pattern",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points pattern",
            },
            {
                "path": "packages/evee-azureml/tests/test_tracking.py",
                "category": "C",
                "relevance": "Tracking test patterns",
            },
            {
                "path": "packages/evee-azureml/tests/test_azureml_backend_pkg.py",
                "category": "C",
                "relevance": "Compute test patterns",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend protocol — unchanged",
            },
            {
                "path": "src/evee/tracking/factory.py",
                "category": "C",
                "relevance": "Backend factory — uses entry_points, unchanged",
            },
            {
                "path": "src/evee/tracking/events.py",
                "category": "C",
                "relevance": "Event types — sufficient for Foundry",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — unchanged",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": 'Config models — uses `extra="allow"`, unchanged',
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Backend loading via entry points — unchanged",
            },
            {
                "path": "tests/evee/tracking/test_tracking_factory.py",
                "category": "C",
                "relevance": "Factory tests pattern",
            },
            {"path": "pyproject.toml", "category": "S", "relevance": "Core entry points reference"},
            {
                "path": "example/experiment/config.azureml.yaml",
                "category": "S",
                "relevance": "Reference AzureML config pattern",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/main.tf",
                "category": "S",
                "relevance": "Foundry infrastructure",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/variables.tf",
                "category": "S",
                "relevance": "Foundry variables",
            },
            {
                "path": "infra/terraform/modules/ai-foundry/outputs.tf",
                "category": "S",
                "relevance": "Foundry endpoints",
            },
            {"path": "infra/terraform/main.tf", "category": "S", "relevance": "Module invocation"},
            {"path": "docs/backends/overview.md", "category": "S", "relevance": "Backend overview"},
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Backend implementation guide",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
            {"path": "docs/user-guide/cli.md", "category": "S", "relevance": "CLI reference"},
        ],
        "difficulty": "complex",
    },
    {
        "issue": "192",
        "title": "Azure AI Foundry: SDK Integration Exploration (Phase 2)",
        "query_level": "Q1",
        "task": "I need to explore deeper SDK-level integration between Evee and Azure AI Foundry. This is Phase 2, which assumes Phase 1 (basic tracking/compute backend) is complete. I need to investigate whether Evee evaluators can be called directly from the Foundry SDK, whether Foundry-native configuration and deployment of Evee experiments is feasible, and whether shared model/dataset registries make sense. I need the existing AzureML backend package as reference, the compute/tracking backend protocols, config models, Terraform Foundry modules, and architecture documentation.",
        "gt_files": [
            "packages/evee-azureml/src/evee_azureml/tracking.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "packages/evee-azureml/src/evee_azureml/auth.py",
            "packages/evee-azureml/src/evee_azureml/utils.py",
            "packages/evee-azureml/pyproject.toml",
            "src/evee/tracking/backend.py",
            "src/evee/compute/backend.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "docs/backends/custom-backends.md",
            "docs/design/architecture.md",
        ],
        "gt_categories": [
            {
                "path": "packages/evee-azureml/src/evee_azureml/tracking.py",
                "category": "C",
                "relevance": "Reference tracking backend — studying integration patterns",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "Reference compute backend",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "Config models pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/auth.py",
                "category": "C",
                "relevance": "Auth pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/utils.py",
                "category": "C",
                "relevance": "Shared utilities",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points, azure-ai-projects dependency",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend protocol — studying extensibility",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — studying extensibility",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "Config models — studying flexibility",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Backend loading — studying plugin architecture",
            },
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Backend implementation guide",
            },
            {
                "path": "docs/design/architecture.md",
                "category": "S",
                "relevance": "Architecture reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "192",
        "title": "Azure AI Foundry: SDK Integration Exploration (Phase 2)",
        "query_level": "Q2",
        "task": "Explore making Evee work more deeply within Azure AI Foundry's SDK workflows. I need to understand Evee's backend plugin architecture, the existing AzureML reference implementation, and the Foundry infrastructure setup to evaluate SDK integration feasibility.",
        "gt_files": [
            "packages/evee-azureml/src/evee_azureml/tracking.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "packages/evee-azureml/src/evee_azureml/auth.py",
            "packages/evee-azureml/src/evee_azureml/utils.py",
            "packages/evee-azureml/pyproject.toml",
            "src/evee/tracking/backend.py",
            "src/evee/compute/backend.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "docs/backends/custom-backends.md",
            "docs/design/architecture.md",
        ],
        "gt_categories": [
            {
                "path": "packages/evee-azureml/src/evee_azureml/tracking.py",
                "category": "C",
                "relevance": "Reference tracking backend — studying integration patterns",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "Reference compute backend",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "Config models pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/auth.py",
                "category": "C",
                "relevance": "Auth pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/utils.py",
                "category": "C",
                "relevance": "Shared utilities",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points, azure-ai-projects dependency",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend protocol — studying extensibility",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — studying extensibility",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "Config models — studying flexibility",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Backend loading — studying plugin architecture",
            },
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Backend implementation guide",
            },
            {
                "path": "docs/design/architecture.md",
                "category": "S",
                "relevance": "Architecture reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "192",
        "title": "Azure AI Foundry: SDK Integration Exploration (Phase 2)",
        "query_level": "Q3",
        "task": "Can Evee be embedded more deeply into Azure AI Foundry beyond just backends? I need to explore SDK-level integration possibilities. Where is the relevant backend code and infrastructure configuration?",
        "gt_files": [
            "packages/evee-azureml/src/evee_azureml/tracking.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "packages/evee-azureml/src/evee_azureml/auth.py",
            "packages/evee-azureml/src/evee_azureml/utils.py",
            "packages/evee-azureml/pyproject.toml",
            "src/evee/tracking/backend.py",
            "src/evee/compute/backend.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "docs/backends/custom-backends.md",
            "docs/design/architecture.md",
        ],
        "gt_categories": [
            {
                "path": "packages/evee-azureml/src/evee_azureml/tracking.py",
                "category": "C",
                "relevance": "Reference tracking backend — studying integration patterns",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "Reference compute backend",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "Config models pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/auth.py",
                "category": "C",
                "relevance": "Auth pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/utils.py",
                "category": "C",
                "relevance": "Shared utilities",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points, azure-ai-projects dependency",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend protocol — studying extensibility",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — studying extensibility",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "Config models — studying flexibility",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Backend loading — studying plugin architecture",
            },
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Backend implementation guide",
            },
            {
                "path": "docs/design/architecture.md",
                "category": "S",
                "relevance": "Architecture reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "193",
        "title": "feat(cli): Support configurable dependency sources in `evee new` scaffolding",
        "query_level": "Q1",
        "task": "I need to add an interactive source selection flow to the `evee new` CLI command. Currently `new.py` hardcodes Git-based dependencies via `_GIT_BASE`. I need to add interactive prompts for choosing between Git, pre-built wheels, and local source, with flags (`--from-git`, `--wheels`, `--from-repo`, `--from-source`) for automation. The pyproject.toml templates in `src/evee/cli/templates/overlays/` use placeholders like `{evee_core_dependency}` and `{evee_core_source}` that need different rendering per source type. Tests in `test_new_command.py` cover version pinning and from-source scenarios.",
        "gt_files": [
            "src/evee/cli/commands/new.py",
            "src/evee/cli/utils/new_project_operations.py",
            "src/evee/cli/templates/overlays/core/pyproject.toml",
            "src/evee/cli/templates/overlays/mlflow/pyproject.toml",
            "src/evee/cli/templates/overlays/azureml/pyproject.toml",
            "tests/evee/cli/test_new_command.py",
            "tests/evee/cli/test_template.py",
            "src/evee/cli/main.py",
            "tests/evee/cli/test_e2e_project_workflow.py",
            "tests/evee/integration/test_e2e_new_project_workflow.py",
            "tests/evee/integration/test_wheels_provisioner.py",
            "docs/user-guide/cli.md",
            "docs/getting-started/installation.md",
            "docs/getting-started/quickstart.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/new.py",
                "category": "E",
                "relevance": "Main command: `new_project()`, `_resolve_evee_version()`, `_backend_dep_from_source/git()` — add `--from-git`, `--wheels`, `--from-source` flags and interactive source selection",
            },
            {
                "path": "src/evee/cli/utils/new_project_operations.py",
                "category": "E",
                "relevance": "`copy_and_render_template()` — may need source-type-aware rendering",
            },
            {
                "path": "src/evee/cli/templates/overlays/core/pyproject.toml",
                "category": "E",
                "relevance": "Template with dependency/source placeholders — update placeholders for new sources",
            },
            {
                "path": "src/evee/cli/templates/overlays/mlflow/pyproject.toml",
                "category": "E",
                "relevance": "MLflow template with placeholders",
            },
            {
                "path": "src/evee/cli/templates/overlays/azureml/pyproject.toml",
                "category": "E",
                "relevance": "AzureML template with placeholders",
            },
            {
                "path": "tests/evee/cli/test_new_command.py",
                "category": "E",
                "relevance": "Tests for new project command — add source selection tests",
            },
            {
                "path": "tests/evee/cli/test_template.py",
                "category": "E",
                "relevance": "Template rendering tests — add source variant tests",
            },
            {
                "path": "src/evee/cli/main.py",
                "category": "C",
                "relevance": "CLI entry point — no changes needed for new flags on `new` subcommand",
            },
            {
                "path": "tests/evee/cli/test_e2e_project_workflow.py",
                "category": "C",
                "relevance": "E2E workflow tests — reference patterns",
            },
            {
                "path": "tests/evee/integration/test_e2e_new_project_workflow.py",
                "category": "C",
                "relevance": "Integration test for new project",
            },
            {
                "path": "tests/evee/integration/test_wheels_provisioner.py",
                "category": "C",
                "relevance": "Wheel provisioning tests",
            },
            {
                "path": "docs/user-guide/cli.md",
                "category": "S",
                "relevance": "CLI reference with `evee new` section",
            },
            {
                "path": "docs/getting-started/installation.md",
                "category": "S",
                "relevance": "Installation docs",
            },
            {
                "path": "docs/getting-started/quickstart.md",
                "category": "S",
                "relevance": "Quickstart docs",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "193",
        "title": "feat(cli): Support configurable dependency sources in `evee new` scaffolding",
        "query_level": "Q2",
        "task": "I want to add dependency source selection to `evee new` so users can choose between Git, wheels, or local source for installing Evee packages. I need to understand the current scaffolding flow in the CLI, the template overlay system, and how pyproject.toml placeholders are rendered.",
        "gt_files": [
            "src/evee/cli/commands/new.py",
            "src/evee/cli/utils/new_project_operations.py",
            "src/evee/cli/templates/overlays/core/pyproject.toml",
            "src/evee/cli/templates/overlays/mlflow/pyproject.toml",
            "src/evee/cli/templates/overlays/azureml/pyproject.toml",
            "tests/evee/cli/test_new_command.py",
            "tests/evee/cli/test_template.py",
            "src/evee/cli/main.py",
            "tests/evee/cli/test_e2e_project_workflow.py",
            "tests/evee/integration/test_e2e_new_project_workflow.py",
            "tests/evee/integration/test_wheels_provisioner.py",
            "docs/user-guide/cli.md",
            "docs/getting-started/installation.md",
            "docs/getting-started/quickstart.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/new.py",
                "category": "E",
                "relevance": "Main command: `new_project()`, `_resolve_evee_version()`, `_backend_dep_from_source/git()` — add `--from-git`, `--wheels`, `--from-source` flags and interactive source selection",
            },
            {
                "path": "src/evee/cli/utils/new_project_operations.py",
                "category": "E",
                "relevance": "`copy_and_render_template()` — may need source-type-aware rendering",
            },
            {
                "path": "src/evee/cli/templates/overlays/core/pyproject.toml",
                "category": "E",
                "relevance": "Template with dependency/source placeholders — update placeholders for new sources",
            },
            {
                "path": "src/evee/cli/templates/overlays/mlflow/pyproject.toml",
                "category": "E",
                "relevance": "MLflow template with placeholders",
            },
            {
                "path": "src/evee/cli/templates/overlays/azureml/pyproject.toml",
                "category": "E",
                "relevance": "AzureML template with placeholders",
            },
            {
                "path": "tests/evee/cli/test_new_command.py",
                "category": "E",
                "relevance": "Tests for new project command — add source selection tests",
            },
            {
                "path": "tests/evee/cli/test_template.py",
                "category": "E",
                "relevance": "Template rendering tests — add source variant tests",
            },
            {
                "path": "src/evee/cli/main.py",
                "category": "C",
                "relevance": "CLI entry point — no changes needed for new flags on `new` subcommand",
            },
            {
                "path": "tests/evee/cli/test_e2e_project_workflow.py",
                "category": "C",
                "relevance": "E2E workflow tests — reference patterns",
            },
            {
                "path": "tests/evee/integration/test_e2e_new_project_workflow.py",
                "category": "C",
                "relevance": "Integration test for new project",
            },
            {
                "path": "tests/evee/integration/test_wheels_provisioner.py",
                "category": "C",
                "relevance": "Wheel provisioning tests",
            },
            {
                "path": "docs/user-guide/cli.md",
                "category": "S",
                "relevance": "CLI reference with `evee new` section",
            },
            {
                "path": "docs/getting-started/installation.md",
                "category": "S",
                "relevance": "Installation docs",
            },
            {
                "path": "docs/getting-started/quickstart.md",
                "category": "S",
                "relevance": "Quickstart docs",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "193",
        "title": "feat(cli): Support configurable dependency sources in `evee new` scaffolding",
        "query_level": "Q3",
        "task": "How does `evee new` work for creating new projects? I need to add support for different ways of installing Evee packages — not just from Git. Where is the scaffolding code and template system?",
        "gt_files": [
            "src/evee/cli/commands/new.py",
            "src/evee/cli/utils/new_project_operations.py",
            "src/evee/cli/templates/overlays/core/pyproject.toml",
            "src/evee/cli/templates/overlays/mlflow/pyproject.toml",
            "src/evee/cli/templates/overlays/azureml/pyproject.toml",
            "tests/evee/cli/test_new_command.py",
            "tests/evee/cli/test_template.py",
            "src/evee/cli/main.py",
            "tests/evee/cli/test_e2e_project_workflow.py",
            "tests/evee/integration/test_e2e_new_project_workflow.py",
            "tests/evee/integration/test_wheels_provisioner.py",
            "docs/user-guide/cli.md",
            "docs/getting-started/installation.md",
            "docs/getting-started/quickstart.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/new.py",
                "category": "E",
                "relevance": "Main command: `new_project()`, `_resolve_evee_version()`, `_backend_dep_from_source/git()` — add `--from-git`, `--wheels`, `--from-source` flags and interactive source selection",
            },
            {
                "path": "src/evee/cli/utils/new_project_operations.py",
                "category": "E",
                "relevance": "`copy_and_render_template()` — may need source-type-aware rendering",
            },
            {
                "path": "src/evee/cli/templates/overlays/core/pyproject.toml",
                "category": "E",
                "relevance": "Template with dependency/source placeholders — update placeholders for new sources",
            },
            {
                "path": "src/evee/cli/templates/overlays/mlflow/pyproject.toml",
                "category": "E",
                "relevance": "MLflow template with placeholders",
            },
            {
                "path": "src/evee/cli/templates/overlays/azureml/pyproject.toml",
                "category": "E",
                "relevance": "AzureML template with placeholders",
            },
            {
                "path": "tests/evee/cli/test_new_command.py",
                "category": "E",
                "relevance": "Tests for new project command — add source selection tests",
            },
            {
                "path": "tests/evee/cli/test_template.py",
                "category": "E",
                "relevance": "Template rendering tests — add source variant tests",
            },
            {
                "path": "src/evee/cli/main.py",
                "category": "C",
                "relevance": "CLI entry point — no changes needed for new flags on `new` subcommand",
            },
            {
                "path": "tests/evee/cli/test_e2e_project_workflow.py",
                "category": "C",
                "relevance": "E2E workflow tests — reference patterns",
            },
            {
                "path": "tests/evee/integration/test_e2e_new_project_workflow.py",
                "category": "C",
                "relevance": "Integration test for new project",
            },
            {
                "path": "tests/evee/integration/test_wheels_provisioner.py",
                "category": "C",
                "relevance": "Wheel provisioning tests",
            },
            {
                "path": "docs/user-guide/cli.md",
                "category": "S",
                "relevance": "CLI reference with `evee new` section",
            },
            {
                "path": "docs/getting-started/installation.md",
                "category": "S",
                "relevance": "Installation docs",
            },
            {
                "path": "docs/getting-started/quickstart.md",
                "category": "S",
                "relevance": "Quickstart docs",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "201",
        "title": "Move example project into end-to-end testing",
        "query_level": "Q1",
        "task": "I need to move the `example/` project directory into the `tests/` folder so it becomes a pure testing fixture instead of serving double duty as a user-facing sample. All integration tests in `tests/evee/integration/` reference `example/` via `get_example_dir()` in `helpers.py`. The MCP conftest also references it via `example_project` fixture. I need to update all path references, CI workflows, debug configs, pyproject.toml coverage settings, and documentation that mentions `example/`. The `samples/agent-sample/` should remain as the user-facing reference project.",
        "gt_files": [
            "tests/evee/integration/helpers.py",
            "tests/mcp/conftest.py",
            ".github/workflows/integration-tests.yml",
            ".vscode/launch.json",
            "pyproject.toml",
            "cspell.config.yaml",
            "tests/evee/integration/test_example_evaluate_locally_core.py",
            "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
            "tests/evee/integration/test_example_evaluate_submission_remote_azureml.py",
            "tests/evee/integration/test_cli.py",
            "tests/evee/integration/test_e2e_new_project_workflow.py",
            "tests/evee/integration/test_model_cleanup.py",
            "CONTRIBUTING.md",
            "docs/development/contributing.md",
        ],
        "gt_categories": [
            {
                "path": "tests/evee/integration/helpers.py",
                "category": "E",
                "relevance": "`get_example_dir()` — path update from `example/` to `tests/fixtures/example/`",
            },
            {
                "path": "tests/mcp/conftest.py",
                "category": "E",
                "relevance": "`example_project` fixture — path update",
            },
            {
                "path": ".github/workflows/integration-tests.yml",
                "category": "E",
                "relevance": "Integration test workflow — path references",
            },
            {
                "path": ".vscode/launch.json",
                "category": "E",
                "relevance": "Debug configs referencing `example/`",
            },
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Coverage omit, testpaths — path references",
            },
            {
                "path": "cspell.config.yaml",
                "category": "E",
                "relevance": "Spell check paths referencing `example/`",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_core.py",
                "category": "C",
                "relevance": "Core integration test — import path may change",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
                "category": "C",
                "relevance": "MLflow integration test",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_submission_remote_azureml.py",
                "category": "C",
                "relevance": "AzureML integration test",
            },
            {
                "path": "tests/evee/integration/test_cli.py",
                "category": "C",
                "relevance": "CLI integration test",
            },
            {
                "path": "tests/evee/integration/test_e2e_new_project_workflow.py",
                "category": "C",
                "relevance": "E2E workflow test",
            },
            {
                "path": "tests/evee/integration/test_model_cleanup.py",
                "category": "C",
                "relevance": "Model cleanup test",
            },
            {
                "path": "CONTRIBUTING.md",
                "category": "S",
                "relevance": "References to `example/` directory",
            },
            {
                "path": "docs/development/contributing.md",
                "category": "S",
                "relevance": "Project structure showing `example/`",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "201",
        "title": "Move example project into end-to-end testing",
        "query_level": "Q2",
        "task": "Move the `example/` directory into `tests/` for use as an E2E testing fixture. I need to find all references to `example/` across the codebase — in tests, CI workflows, documentation, and configuration — and update them. The agent sample should remain as the user-facing sample project.",
        "gt_files": [
            "tests/evee/integration/helpers.py",
            "tests/mcp/conftest.py",
            ".github/workflows/integration-tests.yml",
            ".vscode/launch.json",
            "pyproject.toml",
            "cspell.config.yaml",
            "tests/evee/integration/test_example_evaluate_locally_core.py",
            "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
            "tests/evee/integration/test_example_evaluate_submission_remote_azureml.py",
            "tests/evee/integration/test_cli.py",
            "tests/evee/integration/test_e2e_new_project_workflow.py",
            "tests/evee/integration/test_model_cleanup.py",
            "CONTRIBUTING.md",
            "docs/development/contributing.md",
        ],
        "gt_categories": [
            {
                "path": "tests/evee/integration/helpers.py",
                "category": "E",
                "relevance": "`get_example_dir()` — path update from `example/` to `tests/fixtures/example/`",
            },
            {
                "path": "tests/mcp/conftest.py",
                "category": "E",
                "relevance": "`example_project` fixture — path update",
            },
            {
                "path": ".github/workflows/integration-tests.yml",
                "category": "E",
                "relevance": "Integration test workflow — path references",
            },
            {
                "path": ".vscode/launch.json",
                "category": "E",
                "relevance": "Debug configs referencing `example/`",
            },
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Coverage omit, testpaths — path references",
            },
            {
                "path": "cspell.config.yaml",
                "category": "E",
                "relevance": "Spell check paths referencing `example/`",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_core.py",
                "category": "C",
                "relevance": "Core integration test — import path may change",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
                "category": "C",
                "relevance": "MLflow integration test",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_submission_remote_azureml.py",
                "category": "C",
                "relevance": "AzureML integration test",
            },
            {
                "path": "tests/evee/integration/test_cli.py",
                "category": "C",
                "relevance": "CLI integration test",
            },
            {
                "path": "tests/evee/integration/test_e2e_new_project_workflow.py",
                "category": "C",
                "relevance": "E2E workflow test",
            },
            {
                "path": "tests/evee/integration/test_model_cleanup.py",
                "category": "C",
                "relevance": "Model cleanup test",
            },
            {
                "path": "CONTRIBUTING.md",
                "category": "S",
                "relevance": "References to `example/` directory",
            },
            {
                "path": "docs/development/contributing.md",
                "category": "S",
                "relevance": "Project structure showing `example/`",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "201",
        "title": "Move example project into end-to-end testing",
        "query_level": "Q3",
        "task": "The example project needs to move into the tests folder. It's currently used for both testing and as a sample, which is confusing. Where is the example project referenced throughout the codebase?",
        "gt_files": [
            "tests/evee/integration/helpers.py",
            "tests/mcp/conftest.py",
            ".github/workflows/integration-tests.yml",
            ".vscode/launch.json",
            "pyproject.toml",
            "cspell.config.yaml",
            "tests/evee/integration/test_example_evaluate_locally_core.py",
            "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
            "tests/evee/integration/test_example_evaluate_submission_remote_azureml.py",
            "tests/evee/integration/test_cli.py",
            "tests/evee/integration/test_e2e_new_project_workflow.py",
            "tests/evee/integration/test_model_cleanup.py",
            "CONTRIBUTING.md",
            "docs/development/contributing.md",
        ],
        "gt_categories": [
            {
                "path": "tests/evee/integration/helpers.py",
                "category": "E",
                "relevance": "`get_example_dir()` — path update from `example/` to `tests/fixtures/example/`",
            },
            {
                "path": "tests/mcp/conftest.py",
                "category": "E",
                "relevance": "`example_project` fixture — path update",
            },
            {
                "path": ".github/workflows/integration-tests.yml",
                "category": "E",
                "relevance": "Integration test workflow — path references",
            },
            {
                "path": ".vscode/launch.json",
                "category": "E",
                "relevance": "Debug configs referencing `example/`",
            },
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Coverage omit, testpaths — path references",
            },
            {
                "path": "cspell.config.yaml",
                "category": "E",
                "relevance": "Spell check paths referencing `example/`",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_core.py",
                "category": "C",
                "relevance": "Core integration test — import path may change",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_locally_mlflow.py",
                "category": "C",
                "relevance": "MLflow integration test",
            },
            {
                "path": "tests/evee/integration/test_example_evaluate_submission_remote_azureml.py",
                "category": "C",
                "relevance": "AzureML integration test",
            },
            {
                "path": "tests/evee/integration/test_cli.py",
                "category": "C",
                "relevance": "CLI integration test",
            },
            {
                "path": "tests/evee/integration/test_e2e_new_project_workflow.py",
                "category": "C",
                "relevance": "E2E workflow test",
            },
            {
                "path": "tests/evee/integration/test_model_cleanup.py",
                "category": "C",
                "relevance": "Model cleanup test",
            },
            {
                "path": "CONTRIBUTING.md",
                "category": "S",
                "relevance": "References to `example/` directory",
            },
            {
                "path": "docs/development/contributing.md",
                "category": "S",
                "relevance": "Project structure showing `example/`",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "210",
        "title": "Evaluate OSS Release Strategy: Options, Requirements & Decision Framework",
        "query_level": "Q1",
        "task": "I need to evaluate Evee's OSS release strategy. I need to understand the current distribution model: build infrastructure (`tools/build/build_wheels.sh`), release workflow (`.github/workflows/release.yml`), version calculation, CI/CD pipelines including CodeQL scanning, and the docs publishing workflow. I also need the package metadata in all `pyproject.toml` files (root, MLflow, AzureML), the hardcoded GitHub URLs in CLI commands, installation documentation, security/compliance files (LICENSE, SECURITY.md, CONTRIBUTING.md), and the docs site configuration. This is an investigation task to produce a decision matrix for Microsoft vs external hosting and PyPI vs GitHub Releases.",
        "gt_files": [
            "LICENSE",
            "SECURITY.md",
            "README.md",
            "CONTRIBUTING.md",
            "pyproject.toml",
            "packages/evee-mlflow/pyproject.toml",
            "packages/evee-azureml/pyproject.toml",
            "Makefile",
            "tools/build/build_wheels.sh",
            ".github/workflows/release.yml",
            ".github/tools/calculate_version.py",
            ".github/workflows/ci.yml",
            ".github/workflows/codeql.yml",
            ".github/workflows/docs.yml",
            "mkdocs.yml",
            "docs/index.md",
            "docs/getting-started/installation.md",
            "docs/user-guide/cli.md",
            "docs/development/contributing.md",
            "example/README.md",
            "src/evee/cli/commands/new.py",
            "src/evee/cli/main.py",
        ],
        "gt_categories": [
            {"path": "LICENSE", "category": "S", "relevance": "MIT license — compliance"},
            {"path": "SECURITY.md", "category": "S", "relevance": "Security/compliance checklist"},
            {
                "path": "README.md",
                "category": "S",
                "relevance": "Public-facing identity, install instructions",
            },
            {
                "path": "CONTRIBUTING.md",
                "category": "S",
                "relevance": "Contributor model, package descriptions",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "Package metadata, build system, entry points",
            },
            {
                "path": "packages/evee-mlflow/pyproject.toml",
                "category": "S",
                "relevance": "Package metadata, repository URL",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "S",
                "relevance": "Package metadata, repository URL",
            },
            {"path": "Makefile", "category": "S", "relevance": "Build targets"},
            {
                "path": "tools/build/build_wheels.sh",
                "category": "C",
                "relevance": "Wheel build script",
            },
            {
                "path": ".github/workflows/release.yml",
                "category": "C",
                "relevance": "Release workflow",
            },
            {
                "path": ".github/tools/calculate_version.py",
                "category": "C",
                "relevance": "Version calculation",
            },
            {"path": ".github/workflows/ci.yml", "category": "C", "relevance": "CI pipeline"},
            {
                "path": ".github/workflows/codeql.yml",
                "category": "C",
                "relevance": "Security scanning",
            },
            {"path": ".github/workflows/docs.yml", "category": "C", "relevance": "Docs publishing"},
            {"path": "mkdocs.yml", "category": "S", "relevance": "Documentation site config"},
            {"path": "docs/index.md", "category": "S", "relevance": "Documentation landing page"},
            {
                "path": "docs/getting-started/installation.md",
                "category": "S",
                "relevance": "Install instructions (GitHub-only distribution)",
            },
            {"path": "docs/user-guide/cli.md", "category": "S", "relevance": "CLI reference"},
            {
                "path": "docs/development/contributing.md",
                "category": "S",
                "relevance": "Contributor guide",
            },
            {
                "path": "example/README.md",
                "category": "S",
                "relevance": "References wheels, GitHub Releases",
            },
            {
                "path": "src/evee/cli/commands/new.py",
                "category": "C",
                "relevance": "`_GIT_BASE` URL hardcoded to github.com/microsoft/evee",
            },
            {
                "path": "src/evee/cli/main.py",
                "category": "C",
                "relevance": "Package name for version lookup",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "210",
        "title": "Evaluate OSS Release Strategy: Options, Requirements & Decision Framework",
        "query_level": "Q2",
        "task": "I'm evaluating how to release Evee publicly. I need to review the current build and release infrastructure, package metadata, distribution model, CI/CD pipelines, security scanning, and documentation. Where are the release workflows, build scripts, and compliance-related files?",
        "gt_files": [
            "LICENSE",
            "SECURITY.md",
            "README.md",
            "CONTRIBUTING.md",
            "pyproject.toml",
            "packages/evee-mlflow/pyproject.toml",
            "packages/evee-azureml/pyproject.toml",
            "Makefile",
            "tools/build/build_wheels.sh",
            ".github/workflows/release.yml",
            ".github/tools/calculate_version.py",
            ".github/workflows/ci.yml",
            ".github/workflows/codeql.yml",
            ".github/workflows/docs.yml",
            "mkdocs.yml",
            "docs/index.md",
            "docs/getting-started/installation.md",
            "docs/user-guide/cli.md",
            "docs/development/contributing.md",
            "example/README.md",
            "src/evee/cli/commands/new.py",
            "src/evee/cli/main.py",
        ],
        "gt_categories": [
            {"path": "LICENSE", "category": "S", "relevance": "MIT license — compliance"},
            {"path": "SECURITY.md", "category": "S", "relevance": "Security/compliance checklist"},
            {
                "path": "README.md",
                "category": "S",
                "relevance": "Public-facing identity, install instructions",
            },
            {
                "path": "CONTRIBUTING.md",
                "category": "S",
                "relevance": "Contributor model, package descriptions",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "Package metadata, build system, entry points",
            },
            {
                "path": "packages/evee-mlflow/pyproject.toml",
                "category": "S",
                "relevance": "Package metadata, repository URL",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "S",
                "relevance": "Package metadata, repository URL",
            },
            {"path": "Makefile", "category": "S", "relevance": "Build targets"},
            {
                "path": "tools/build/build_wheels.sh",
                "category": "C",
                "relevance": "Wheel build script",
            },
            {
                "path": ".github/workflows/release.yml",
                "category": "C",
                "relevance": "Release workflow",
            },
            {
                "path": ".github/tools/calculate_version.py",
                "category": "C",
                "relevance": "Version calculation",
            },
            {"path": ".github/workflows/ci.yml", "category": "C", "relevance": "CI pipeline"},
            {
                "path": ".github/workflows/codeql.yml",
                "category": "C",
                "relevance": "Security scanning",
            },
            {"path": ".github/workflows/docs.yml", "category": "C", "relevance": "Docs publishing"},
            {"path": "mkdocs.yml", "category": "S", "relevance": "Documentation site config"},
            {"path": "docs/index.md", "category": "S", "relevance": "Documentation landing page"},
            {
                "path": "docs/getting-started/installation.md",
                "category": "S",
                "relevance": "Install instructions (GitHub-only distribution)",
            },
            {"path": "docs/user-guide/cli.md", "category": "S", "relevance": "CLI reference"},
            {
                "path": "docs/development/contributing.md",
                "category": "S",
                "relevance": "Contributor guide",
            },
            {
                "path": "example/README.md",
                "category": "S",
                "relevance": "References wheels, GitHub Releases",
            },
            {
                "path": "src/evee/cli/commands/new.py",
                "category": "C",
                "relevance": "`_GIT_BASE` URL hardcoded to github.com/microsoft/evee",
            },
            {
                "path": "src/evee/cli/main.py",
                "category": "C",
                "relevance": "Package name for version lookup",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "210",
        "title": "Evaluate OSS Release Strategy: Options, Requirements & Decision Framework",
        "query_level": "Q3",
        "task": "What does Evee's release and distribution setup look like? I need to evaluate whether to publish on PyPI or stick with GitHub Releases, and whether to stay in the Microsoft org. Where are the relevant build, release, and compliance files?",
        "gt_files": [
            "LICENSE",
            "SECURITY.md",
            "README.md",
            "CONTRIBUTING.md",
            "pyproject.toml",
            "packages/evee-mlflow/pyproject.toml",
            "packages/evee-azureml/pyproject.toml",
            "Makefile",
            "tools/build/build_wheels.sh",
            ".github/workflows/release.yml",
            ".github/tools/calculate_version.py",
            ".github/workflows/ci.yml",
            ".github/workflows/codeql.yml",
            ".github/workflows/docs.yml",
            "mkdocs.yml",
            "docs/index.md",
            "docs/getting-started/installation.md",
            "docs/user-guide/cli.md",
            "docs/development/contributing.md",
            "example/README.md",
            "src/evee/cli/commands/new.py",
            "src/evee/cli/main.py",
        ],
        "gt_categories": [
            {"path": "LICENSE", "category": "S", "relevance": "MIT license — compliance"},
            {"path": "SECURITY.md", "category": "S", "relevance": "Security/compliance checklist"},
            {
                "path": "README.md",
                "category": "S",
                "relevance": "Public-facing identity, install instructions",
            },
            {
                "path": "CONTRIBUTING.md",
                "category": "S",
                "relevance": "Contributor model, package descriptions",
            },
            {
                "path": "pyproject.toml",
                "category": "S",
                "relevance": "Package metadata, build system, entry points",
            },
            {
                "path": "packages/evee-mlflow/pyproject.toml",
                "category": "S",
                "relevance": "Package metadata, repository URL",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "S",
                "relevance": "Package metadata, repository URL",
            },
            {"path": "Makefile", "category": "S", "relevance": "Build targets"},
            {
                "path": "tools/build/build_wheels.sh",
                "category": "C",
                "relevance": "Wheel build script",
            },
            {
                "path": ".github/workflows/release.yml",
                "category": "C",
                "relevance": "Release workflow",
            },
            {
                "path": ".github/tools/calculate_version.py",
                "category": "C",
                "relevance": "Version calculation",
            },
            {"path": ".github/workflows/ci.yml", "category": "C", "relevance": "CI pipeline"},
            {
                "path": ".github/workflows/codeql.yml",
                "category": "C",
                "relevance": "Security scanning",
            },
            {"path": ".github/workflows/docs.yml", "category": "C", "relevance": "Docs publishing"},
            {"path": "mkdocs.yml", "category": "S", "relevance": "Documentation site config"},
            {"path": "docs/index.md", "category": "S", "relevance": "Documentation landing page"},
            {
                "path": "docs/getting-started/installation.md",
                "category": "S",
                "relevance": "Install instructions (GitHub-only distribution)",
            },
            {"path": "docs/user-guide/cli.md", "category": "S", "relevance": "CLI reference"},
            {
                "path": "docs/development/contributing.md",
                "category": "S",
                "relevance": "Contributor guide",
            },
            {
                "path": "example/README.md",
                "category": "S",
                "relevance": "References wheels, GitHub Releases",
            },
            {
                "path": "src/evee/cli/commands/new.py",
                "category": "C",
                "relevance": "`_GIT_BASE` URL hardcoded to github.com/microsoft/evee",
            },
            {
                "path": "src/evee/cli/main.py",
                "category": "C",
                "relevance": "Package name for version lookup",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "226",
        "title": "Set default MLflow tracking URI to sqlite:///mlflow.db",
        "query_level": "Q1",
        "task": "I need to change the default MLflow tracking URI from the filesystem backend (`./mlruns`) to `sqlite:///mlflow.db` in the evee-mlflow package. The `MLflowTrackingConfig` in `packages/evee-mlflow/src/evee_mlflow/config.py` has `tracking_uri` defaulting to None with artifact_location defaulting to `./mlruns`. The `MLflowBackend.on_startup()` in `tracking.py` logs the `./mlruns directory` message when no URI is set. I need to update the config default, the startup logic, all tests that assert on `./mlruns`, example/sample configs, the MLflow project template, documentation references, and the core `TrackingBackendConfig` default.",
        "gt_files": [
            "packages/evee-mlflow/src/evee_mlflow/config.py",
            "packages/evee-mlflow/src/evee_mlflow/tracking.py",
            "src/evee/mcp/resources/config.py",
            "packages/evee-mlflow/pyproject.toml",
            "packages/evee-mlflow/tests/test_mlflow_backend.py",
            "packages/evee-mlflow/tests/test_integration.py",
            "packages/evee-mlflow/tests/test_mlflow_autolog.py",
            "src/evee/config/models.py",
            "tests/evee/config/test_models.py",
            "docs/backends/mlflow.md",
            "docs/user-guide/configuration.md",
            "docs/getting-started/quickstart.md",
            "example/experiment/config.mlflow.yaml",
            "example/Makefile",
            "example/.amlignore",
            "src/evee/cli/templates/overlays/mlflow/experiment/config.yaml",
        ],
        "gt_categories": [
            {
                "path": "packages/evee-mlflow/src/evee_mlflow/config.py",
                "category": "E",
                "relevance": "`MLflowTrackingConfig`: tracking_uri default, artifact_location",
            },
            {
                "path": "packages/evee-mlflow/src/evee_mlflow/tracking.py",
                "category": "E",
                "relevance": "`MLflowBackend.on_startup()` with `./mlruns` reference",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Config schema showing tracking_uri example — update to sqlite",
            },
            {
                "path": "packages/evee-mlflow/pyproject.toml",
                "category": "C",
                "relevance": "MLflow backend entry point",
            },
            {
                "path": "packages/evee-mlflow/tests/test_mlflow_backend.py",
                "category": "C",
                "relevance": "Tests for defaults, on_startup",
            },
            {
                "path": "packages/evee-mlflow/tests/test_integration.py",
                "category": "C",
                "relevance": "Integration tests using temp mlruns",
            },
            {
                "path": "packages/evee-mlflow/tests/test_mlflow_autolog.py",
                "category": "C",
                "relevance": "Autolog tests constructing config",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "`TrackingBackendConfig` — default is in MLflow package, core unchanged",
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "C",
                "relevance": "Tests for TrackingBackendConfig defaults",
            },
            {
                "path": "docs/backends/mlflow.md",
                "category": "S",
                "relevance": "MLflow backend documentation",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Configuration reference",
            },
            {
                "path": "docs/getting-started/quickstart.md",
                "category": "S",
                "relevance": "Quickstart tracking_uri example",
            },
            {
                "path": "example/experiment/config.mlflow.yaml",
                "category": "S",
                "relevance": "Example MLflow config",
            },
            {
                "path": "example/Makefile",
                "category": "S",
                "relevance": "MLflow UI command with backend-store-uri",
            },
            {
                "path": "example/.amlignore",
                "category": "S",
                "relevance": "Lists mlflow.db in ignore",
            },
            {
                "path": "src/evee/cli/templates/overlays/mlflow/experiment/config.yaml",
                "category": "S",
                "relevance": "MLflow template — shows default value",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "226",
        "title": "Set default MLflow tracking URI to sqlite:///mlflow.db",
        "query_level": "Q2",
        "task": "Change the default MLflow tracking URI to use SQLite instead of the filesystem backend being deprecated. I need to find where the MLflow tracking URI default is set in the evee-mlflow package, all tests and configs that reference `./mlruns`, and documentation that describes the default tracking setup.",
        "gt_files": [
            "packages/evee-mlflow/src/evee_mlflow/config.py",
            "packages/evee-mlflow/src/evee_mlflow/tracking.py",
            "src/evee/mcp/resources/config.py",
            "packages/evee-mlflow/pyproject.toml",
            "packages/evee-mlflow/tests/test_mlflow_backend.py",
            "packages/evee-mlflow/tests/test_integration.py",
            "packages/evee-mlflow/tests/test_mlflow_autolog.py",
            "src/evee/config/models.py",
            "tests/evee/config/test_models.py",
            "docs/backends/mlflow.md",
            "docs/user-guide/configuration.md",
            "docs/getting-started/quickstart.md",
            "example/experiment/config.mlflow.yaml",
            "example/Makefile",
            "example/.amlignore",
            "src/evee/cli/templates/overlays/mlflow/experiment/config.yaml",
        ],
        "gt_categories": [
            {
                "path": "packages/evee-mlflow/src/evee_mlflow/config.py",
                "category": "E",
                "relevance": "`MLflowTrackingConfig`: tracking_uri default, artifact_location",
            },
            {
                "path": "packages/evee-mlflow/src/evee_mlflow/tracking.py",
                "category": "E",
                "relevance": "`MLflowBackend.on_startup()` with `./mlruns` reference",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Config schema showing tracking_uri example — update to sqlite",
            },
            {
                "path": "packages/evee-mlflow/pyproject.toml",
                "category": "C",
                "relevance": "MLflow backend entry point",
            },
            {
                "path": "packages/evee-mlflow/tests/test_mlflow_backend.py",
                "category": "C",
                "relevance": "Tests for defaults, on_startup",
            },
            {
                "path": "packages/evee-mlflow/tests/test_integration.py",
                "category": "C",
                "relevance": "Integration tests using temp mlruns",
            },
            {
                "path": "packages/evee-mlflow/tests/test_mlflow_autolog.py",
                "category": "C",
                "relevance": "Autolog tests constructing config",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "`TrackingBackendConfig` — default is in MLflow package, core unchanged",
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "C",
                "relevance": "Tests for TrackingBackendConfig defaults",
            },
            {
                "path": "docs/backends/mlflow.md",
                "category": "S",
                "relevance": "MLflow backend documentation",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Configuration reference",
            },
            {
                "path": "docs/getting-started/quickstart.md",
                "category": "S",
                "relevance": "Quickstart tracking_uri example",
            },
            {
                "path": "example/experiment/config.mlflow.yaml",
                "category": "S",
                "relevance": "Example MLflow config",
            },
            {
                "path": "example/Makefile",
                "category": "S",
                "relevance": "MLflow UI command with backend-store-uri",
            },
            {
                "path": "example/.amlignore",
                "category": "S",
                "relevance": "Lists mlflow.db in ignore",
            },
            {
                "path": "src/evee/cli/templates/overlays/mlflow/experiment/config.yaml",
                "category": "S",
                "relevance": "MLflow template — shows default value",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "226",
        "title": "Set default MLflow tracking URI to sqlite:///mlflow.db",
        "query_level": "Q3",
        "task": "MLflow's filesystem backend is being deprecated. I need to change Evee's default tracking to use SQLite instead. Where is the MLflow backend configured and what references `./mlruns` or the tracking URI?",
        "gt_files": [
            "packages/evee-mlflow/src/evee_mlflow/config.py",
            "packages/evee-mlflow/src/evee_mlflow/tracking.py",
            "src/evee/mcp/resources/config.py",
            "packages/evee-mlflow/pyproject.toml",
            "packages/evee-mlflow/tests/test_mlflow_backend.py",
            "packages/evee-mlflow/tests/test_integration.py",
            "packages/evee-mlflow/tests/test_mlflow_autolog.py",
            "src/evee/config/models.py",
            "tests/evee/config/test_models.py",
            "docs/backends/mlflow.md",
            "docs/user-guide/configuration.md",
            "docs/getting-started/quickstart.md",
            "example/experiment/config.mlflow.yaml",
            "example/Makefile",
            "example/.amlignore",
            "src/evee/cli/templates/overlays/mlflow/experiment/config.yaml",
        ],
        "gt_categories": [
            {
                "path": "packages/evee-mlflow/src/evee_mlflow/config.py",
                "category": "E",
                "relevance": "`MLflowTrackingConfig`: tracking_uri default, artifact_location",
            },
            {
                "path": "packages/evee-mlflow/src/evee_mlflow/tracking.py",
                "category": "E",
                "relevance": "`MLflowBackend.on_startup()` with `./mlruns` reference",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Config schema showing tracking_uri example — update to sqlite",
            },
            {
                "path": "packages/evee-mlflow/pyproject.toml",
                "category": "C",
                "relevance": "MLflow backend entry point",
            },
            {
                "path": "packages/evee-mlflow/tests/test_mlflow_backend.py",
                "category": "C",
                "relevance": "Tests for defaults, on_startup",
            },
            {
                "path": "packages/evee-mlflow/tests/test_integration.py",
                "category": "C",
                "relevance": "Integration tests using temp mlruns",
            },
            {
                "path": "packages/evee-mlflow/tests/test_mlflow_autolog.py",
                "category": "C",
                "relevance": "Autolog tests constructing config",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "`TrackingBackendConfig` — default is in MLflow package, core unchanged",
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "C",
                "relevance": "Tests for TrackingBackendConfig defaults",
            },
            {
                "path": "docs/backends/mlflow.md",
                "category": "S",
                "relevance": "MLflow backend documentation",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Configuration reference",
            },
            {
                "path": "docs/getting-started/quickstart.md",
                "category": "S",
                "relevance": "Quickstart tracking_uri example",
            },
            {
                "path": "example/experiment/config.mlflow.yaml",
                "category": "S",
                "relevance": "Example MLflow config",
            },
            {
                "path": "example/Makefile",
                "category": "S",
                "relevance": "MLflow UI command with backend-store-uri",
            },
            {
                "path": "example/.amlignore",
                "category": "S",
                "relevance": "Lists mlflow.db in ignore",
            },
            {
                "path": "src/evee/cli/templates/overlays/mlflow/experiment/config.yaml",
                "category": "S",
                "relevance": "MLflow template — shows default value",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "233",
        "title": "Early stop for evaluation on certain threshold of errors",
        "query_level": "Q1",
        "task": "I need to implement early stopping in Evee's evaluation pipeline. Part 1: In the inference phase (`_run_evaluation_loop` in `ModelEvaluator`), count consecutive or total errors and abort early if a threshold is exceeded, alerting the user. Part 2: In the evaluation phase, mark metrics as optimization targets in `MetricConfig` and stop when no significant improvement is observed across hyperparameter variations. I need the evaluation loop code, config models for new threshold fields, progress tracking, error handling patterns, and tests for the evaluation pipeline.",
        "gt_files": [
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/config/models.py",
            "src/evee/evaluation/progress_tracker.py",
            "src/evee/tracking/events.py",
            "tests/evee/evaluation/test_model_evaluator_evaluation.py",
            "tests/evee/config/test_models.py",
            "docs/user-guide/configuration.md",
            "src/evee/evaluation/evaluate.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/core/base_model.py",
            "src/evee/core/base_metric.py",
            "src/evee/tracking/backend.py",
            "tests/evee/evaluation/test_model_evaluator_init.py",
            "tests/evee/evaluation/test_model_evaluator_metrics.py",
            "tests/evee/evaluation/test_progress_tracker.py",
            "docs/troubleshooting.md",
            "docs/design/architecture.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "Core evaluator with `_run_evaluation_loop` — add error counting and early abort logic",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "Config models — add `early_stop_threshold`, `early_stop_window` fields",
            },
            {
                "path": "src/evee/evaluation/progress_tracker.py",
                "category": "E",
                "relevance": "Progress tracking — display error count and early stop status",
            },
            {
                "path": "src/evee/tracking/events.py",
                "category": "E",
                "relevance": "Add `EarlyStopEvent` for tracking backends",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_evaluation.py",
                "category": "E",
                "relevance": "Evaluation tests — add early stop scenarios",
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "E",
                "relevance": "Config model tests — early stop fields",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config docs — new early stop fields",
            },
            {
                "path": "src/evee/evaluation/evaluate.py",
                "category": "C",
                "relevance": "`evaluate_main` entry point — passes through, no changes",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Metrics aggregation — no changes needed",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "ExperimentRunner — passes config through, no changes",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "BaseModel inference interface — unchanged",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "BaseMetric compute interface — unchanged",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend — unchanged; receives events",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_init.py",
                "category": "C",
                "relevance": "Evaluator init tests",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_metrics.py",
                "category": "C",
                "relevance": "Metric computation tests",
            },
            {
                "path": "tests/evee/evaluation/test_progress_tracker.py",
                "category": "C",
                "relevance": "Progress tracker tests",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "S",
                "relevance": "Error threshold guidance",
            },
            {
                "path": "docs/design/architecture.md",
                "category": "S",
                "relevance": "Architecture doc",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "233",
        "title": "Early stop for evaluation on certain threshold of errors",
        "query_level": "Q2",
        "task": "Add early stopping to Evee's evaluation. If too many inference errors occur, stop the evaluation early instead of running the full dataset. Also explore metric-based optimization stopping. I need to understand the evaluation loop, error handling, config schema, and progress tracking.",
        "gt_files": [
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/config/models.py",
            "src/evee/evaluation/progress_tracker.py",
            "src/evee/tracking/events.py",
            "tests/evee/evaluation/test_model_evaluator_evaluation.py",
            "tests/evee/config/test_models.py",
            "docs/user-guide/configuration.md",
            "src/evee/evaluation/evaluate.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/core/base_model.py",
            "src/evee/core/base_metric.py",
            "src/evee/tracking/backend.py",
            "tests/evee/evaluation/test_model_evaluator_init.py",
            "tests/evee/evaluation/test_model_evaluator_metrics.py",
            "tests/evee/evaluation/test_progress_tracker.py",
            "docs/troubleshooting.md",
            "docs/design/architecture.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "Core evaluator with `_run_evaluation_loop` — add error counting and early abort logic",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "Config models — add `early_stop_threshold`, `early_stop_window` fields",
            },
            {
                "path": "src/evee/evaluation/progress_tracker.py",
                "category": "E",
                "relevance": "Progress tracking — display error count and early stop status",
            },
            {
                "path": "src/evee/tracking/events.py",
                "category": "E",
                "relevance": "Add `EarlyStopEvent` for tracking backends",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_evaluation.py",
                "category": "E",
                "relevance": "Evaluation tests — add early stop scenarios",
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "E",
                "relevance": "Config model tests — early stop fields",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config docs — new early stop fields",
            },
            {
                "path": "src/evee/evaluation/evaluate.py",
                "category": "C",
                "relevance": "`evaluate_main` entry point — passes through, no changes",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Metrics aggregation — no changes needed",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "ExperimentRunner — passes config through, no changes",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "BaseModel inference interface — unchanged",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "BaseMetric compute interface — unchanged",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend — unchanged; receives events",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_init.py",
                "category": "C",
                "relevance": "Evaluator init tests",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_metrics.py",
                "category": "C",
                "relevance": "Metric computation tests",
            },
            {
                "path": "tests/evee/evaluation/test_progress_tracker.py",
                "category": "C",
                "relevance": "Progress tracker tests",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "S",
                "relevance": "Error threshold guidance",
            },
            {
                "path": "docs/design/architecture.md",
                "category": "S",
                "relevance": "Architecture doc",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "233",
        "title": "Early stop for evaluation on certain threshold of errors",
        "query_level": "Q3",
        "task": "Evee should stop evaluation early when there are too many errors. Where is the evaluation loop and how does error handling work? Can we also optimize the evaluation process to stop when metrics aren't improving?",
        "gt_files": [
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/config/models.py",
            "src/evee/evaluation/progress_tracker.py",
            "src/evee/tracking/events.py",
            "tests/evee/evaluation/test_model_evaluator_evaluation.py",
            "tests/evee/config/test_models.py",
            "docs/user-guide/configuration.md",
            "src/evee/evaluation/evaluate.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/core/base_model.py",
            "src/evee/core/base_metric.py",
            "src/evee/tracking/backend.py",
            "tests/evee/evaluation/test_model_evaluator_init.py",
            "tests/evee/evaluation/test_model_evaluator_metrics.py",
            "tests/evee/evaluation/test_progress_tracker.py",
            "docs/troubleshooting.md",
            "docs/design/architecture.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "Core evaluator with `_run_evaluation_loop` — add error counting and early abort logic",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "Config models — add `early_stop_threshold`, `early_stop_window` fields",
            },
            {
                "path": "src/evee/evaluation/progress_tracker.py",
                "category": "E",
                "relevance": "Progress tracking — display error count and early stop status",
            },
            {
                "path": "src/evee/tracking/events.py",
                "category": "E",
                "relevance": "Add `EarlyStopEvent` for tracking backends",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_evaluation.py",
                "category": "E",
                "relevance": "Evaluation tests — add early stop scenarios",
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "E",
                "relevance": "Config model tests — early stop fields",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config docs — new early stop fields",
            },
            {
                "path": "src/evee/evaluation/evaluate.py",
                "category": "C",
                "relevance": "`evaluate_main` entry point — passes through, no changes",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Metrics aggregation — no changes needed",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "ExperimentRunner — passes config through, no changes",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "BaseModel inference interface — unchanged",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "BaseMetric compute interface — unchanged",
            },
            {
                "path": "src/evee/tracking/backend.py",
                "category": "C",
                "relevance": "TrackingBackend — unchanged; receives events",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_init.py",
                "category": "C",
                "relevance": "Evaluator init tests",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_metrics.py",
                "category": "C",
                "relevance": "Metric computation tests",
            },
            {
                "path": "tests/evee/evaluation/test_progress_tracker.py",
                "category": "C",
                "relevance": "Progress tracker tests",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "S",
                "relevance": "Error threshold guidance",
            },
            {
                "path": "docs/design/architecture.md",
                "category": "S",
                "relevance": "Architecture doc",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "234",
        "title": "DEMO | use mcp server to create an agent evaluation",
        "query_level": "Q1",
        "task": "I need to demonstrate creating an agent evaluation end-to-end using Evee's MCP server. The demo should show an AI assistant using MCP tools (list_components, validate_config, run_experiment, view_results) to scaffold and run an agent evaluation. I need all MCP tools and resources, the agent sample project in `samples/agent-sample/` as reference, the MCP VS Code config template, the execution infrastructure, and any documentation or test gaps to fix before the demo.",
        "gt_files": [
            "src/evee/mcp/README.md",
            "src/evee/mcp/server.py",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/validation.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/resources/config.py",
            "tests/mcp/test_e2e.py",
            "tests/mcp/test_tools.py",
            "samples/agent-sample/README.md",
            "samples/agent-sample/experiment/config.yaml",
            "samples/agent-sample/models/baseline/baseline.py",
            "samples/agent-sample/models/foundry_agent/agent.py",
            "samples/agent-sample/metrics/agent_tool_call_f1_metric.py",
            "samples/agent-sample/pyproject.toml",
            "docs/user-guide/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP documentation — most likely fix target (agent workflow docs)",
            },
            {
                "path": "src/evee/mcp/server.py",
                "category": "C",
                "relevance": "MCP server — conditional fix if demo reveals bugs",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "run_experiment tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "C",
                "relevance": "validate_config tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "C",
                "relevance": "list_components tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "view_results tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "C",
                "relevance": "Config schema — agent eval YAML example",
            },
            {
                "path": "tests/mcp/test_e2e.py",
                "category": "C",
                "relevance": "E2E MCP tests — verify before demo",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "C",
                "relevance": "Tool tests — verify before demo",
            },
            {
                "path": "samples/agent-sample/README.md",
                "category": "S",
                "relevance": "Agent sample README — reference for demo",
            },
            {
                "path": "samples/agent-sample/experiment/config.yaml",
                "category": "S",
                "relevance": "Agent sample config",
            },
            {
                "path": "samples/agent-sample/models/baseline/baseline.py",
                "category": "S",
                "relevance": "Agent baseline model",
            },
            {
                "path": "samples/agent-sample/models/foundry_agent/agent.py",
                "category": "S",
                "relevance": "Foundry agent model",
            },
            {
                "path": "samples/agent-sample/metrics/agent_tool_call_f1_metric.py",
                "category": "S",
                "relevance": "Agent F1 metric",
            },
            {
                "path": "samples/agent-sample/pyproject.toml",
                "category": "S",
                "relevance": "Agent sample dependencies",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP user docs",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "234",
        "title": "DEMO | use mcp server to create an agent evaluation",
        "query_level": "Q2",
        "task": "I want to demo Evee's MCP server creating an agent evaluation project. I need to understand the MCP tools, available resources/patterns, the agent sample project structure, and any issues that might need fixing for a smooth demo flow.",
        "gt_files": [
            "src/evee/mcp/README.md",
            "src/evee/mcp/server.py",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/validation.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/resources/config.py",
            "tests/mcp/test_e2e.py",
            "tests/mcp/test_tools.py",
            "samples/agent-sample/README.md",
            "samples/agent-sample/experiment/config.yaml",
            "samples/agent-sample/models/baseline/baseline.py",
            "samples/agent-sample/models/foundry_agent/agent.py",
            "samples/agent-sample/metrics/agent_tool_call_f1_metric.py",
            "samples/agent-sample/pyproject.toml",
            "docs/user-guide/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP documentation — most likely fix target (agent workflow docs)",
            },
            {
                "path": "src/evee/mcp/server.py",
                "category": "C",
                "relevance": "MCP server — conditional fix if demo reveals bugs",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "run_experiment tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "C",
                "relevance": "validate_config tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "C",
                "relevance": "list_components tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "view_results tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "C",
                "relevance": "Config schema — agent eval YAML example",
            },
            {
                "path": "tests/mcp/test_e2e.py",
                "category": "C",
                "relevance": "E2E MCP tests — verify before demo",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "C",
                "relevance": "Tool tests — verify before demo",
            },
            {
                "path": "samples/agent-sample/README.md",
                "category": "S",
                "relevance": "Agent sample README — reference for demo",
            },
            {
                "path": "samples/agent-sample/experiment/config.yaml",
                "category": "S",
                "relevance": "Agent sample config",
            },
            {
                "path": "samples/agent-sample/models/baseline/baseline.py",
                "category": "S",
                "relevance": "Agent baseline model",
            },
            {
                "path": "samples/agent-sample/models/foundry_agent/agent.py",
                "category": "S",
                "relevance": "Foundry agent model",
            },
            {
                "path": "samples/agent-sample/metrics/agent_tool_call_f1_metric.py",
                "category": "S",
                "relevance": "Agent F1 metric",
            },
            {
                "path": "samples/agent-sample/pyproject.toml",
                "category": "S",
                "relevance": "Agent sample dependencies",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP user docs",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "234",
        "title": "DEMO | use mcp server to create an agent evaluation",
        "query_level": "Q3",
        "task": "How can I use Evee's MCP server to set up an agent evaluation from scratch? I need to create a demo. Where is the MCP server code and is there an example agent evaluation project?",
        "gt_files": [
            "src/evee/mcp/README.md",
            "src/evee/mcp/server.py",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/validation.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/resources/config.py",
            "tests/mcp/test_e2e.py",
            "tests/mcp/test_tools.py",
            "samples/agent-sample/README.md",
            "samples/agent-sample/experiment/config.yaml",
            "samples/agent-sample/models/baseline/baseline.py",
            "samples/agent-sample/models/foundry_agent/agent.py",
            "samples/agent-sample/metrics/agent_tool_call_f1_metric.py",
            "samples/agent-sample/pyproject.toml",
            "docs/user-guide/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP documentation — most likely fix target (agent workflow docs)",
            },
            {
                "path": "src/evee/mcp/server.py",
                "category": "C",
                "relevance": "MCP server — conditional fix if demo reveals bugs",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "run_experiment tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "C",
                "relevance": "validate_config tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "C",
                "relevance": "list_components tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "view_results tool — exercised during demo",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "C",
                "relevance": "Config schema — agent eval YAML example",
            },
            {
                "path": "tests/mcp/test_e2e.py",
                "category": "C",
                "relevance": "E2E MCP tests — verify before demo",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "C",
                "relevance": "Tool tests — verify before demo",
            },
            {
                "path": "samples/agent-sample/README.md",
                "category": "S",
                "relevance": "Agent sample README — reference for demo",
            },
            {
                "path": "samples/agent-sample/experiment/config.yaml",
                "category": "S",
                "relevance": "Agent sample config",
            },
            {
                "path": "samples/agent-sample/models/baseline/baseline.py",
                "category": "S",
                "relevance": "Agent baseline model",
            },
            {
                "path": "samples/agent-sample/models/foundry_agent/agent.py",
                "category": "S",
                "relevance": "Foundry agent model",
            },
            {
                "path": "samples/agent-sample/metrics/agent_tool_call_f1_metric.py",
                "category": "S",
                "relevance": "Agent F1 metric",
            },
            {
                "path": "samples/agent-sample/pyproject.toml",
                "category": "S",
                "relevance": "Agent sample dependencies",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP user docs",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "236",
        "title": "Add MCP analysis tool for LLM-based model suggestion and markdown reporting",
        "query_level": "Q1",
        "task": "I need to add a new MCP tool called `analyze_results` that analyzes experiment results and suggests the best model using LLM sampling. The tool should follow the existing tool pattern: extend `BaseTool` from `src/evee/mcp/tools/base.py`, register in `tools/__init__.py`, add a handler in `server.py`, and add the tool name to `constants.py`. It should reuse the results-loading logic from `ViewResultsTool` and leverage MCP's sampling capabilities to produce a markdown report. I also need to add tests in `tests/mcp/test_tools.py` and update MCP documentation.",
        "gt_files": [
            "src/evee/mcp/server.py",
            "src/evee/mcp/constants.py",
            "src/evee/mcp/tools/__init__.py",
            "tests/mcp/test_tools.py",
            "src/evee/mcp/README.md",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/tools/experiment.py",
            "tests/mcp/test_e2e.py",
            "docs/user-guide/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/server.py",
                "category": "E",
                "relevance": "MCP server — register `analyze_results` tool handler",
            },
            {
                "path": "src/evee/mcp/constants.py",
                "category": "E",
                "relevance": "`ToolNames` — add `ANALYZE_RESULTS`",
            },
            {
                "path": "src/evee/mcp/tools/__init__.py",
                "category": "E",
                "relevance": "Tool registry — register new tool",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "E",
                "relevance": "Tool tests — add analysis tool tests",
            },
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP documentation — add analysis tool section",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "C",
                "relevance": "BaseTool, ToolResult — base class reference",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "ViewResultsTool — reuse results loading pattern",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "RunExperimentTool — reference implementation",
            },
            {
                "path": "tests/mcp/test_e2e.py",
                "category": "C",
                "relevance": "E2E tests — add analysis tool e2e",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP user documentation",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "236",
        "title": "Add MCP analysis tool for LLM-based model suggestion and markdown reporting",
        "query_level": "Q2",
        "task": "Add an MCP tool that analyzes experiment results and recommends the best model. I need to understand the existing MCP tool architecture (BaseTool, tool registry, server handler), how results are loaded, and how MCP sampling works for LLM-powered analysis.",
        "gt_files": [
            "src/evee/mcp/server.py",
            "src/evee/mcp/constants.py",
            "src/evee/mcp/tools/__init__.py",
            "tests/mcp/test_tools.py",
            "src/evee/mcp/README.md",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/tools/experiment.py",
            "tests/mcp/test_e2e.py",
            "docs/user-guide/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/server.py",
                "category": "E",
                "relevance": "MCP server — register `analyze_results` tool handler",
            },
            {
                "path": "src/evee/mcp/constants.py",
                "category": "E",
                "relevance": "`ToolNames` — add `ANALYZE_RESULTS`",
            },
            {
                "path": "src/evee/mcp/tools/__init__.py",
                "category": "E",
                "relevance": "Tool registry — register new tool",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "E",
                "relevance": "Tool tests — add analysis tool tests",
            },
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP documentation — add analysis tool section",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "C",
                "relevance": "BaseTool, ToolResult — base class reference",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "ViewResultsTool — reuse results loading pattern",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "RunExperimentTool — reference implementation",
            },
            {
                "path": "tests/mcp/test_e2e.py",
                "category": "C",
                "relevance": "E2E tests — add analysis tool e2e",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP user documentation",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "236",
        "title": "Add MCP analysis tool for LLM-based model suggestion and markdown reporting",
        "query_level": "Q3",
        "task": "How do I add a new tool to Evee's MCP server? I want one that analyzes results and suggests the best model using AI. Where are the existing tools and how do they work?",
        "gt_files": [
            "src/evee/mcp/server.py",
            "src/evee/mcp/constants.py",
            "src/evee/mcp/tools/__init__.py",
            "tests/mcp/test_tools.py",
            "src/evee/mcp/README.md",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/tools/experiment.py",
            "tests/mcp/test_e2e.py",
            "docs/user-guide/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/server.py",
                "category": "E",
                "relevance": "MCP server — register `analyze_results` tool handler",
            },
            {
                "path": "src/evee/mcp/constants.py",
                "category": "E",
                "relevance": "`ToolNames` — add `ANALYZE_RESULTS`",
            },
            {
                "path": "src/evee/mcp/tools/__init__.py",
                "category": "E",
                "relevance": "Tool registry — register new tool",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "E",
                "relevance": "Tool tests — add analysis tool tests",
            },
            {
                "path": "src/evee/mcp/README.md",
                "category": "E",
                "relevance": "MCP documentation — add analysis tool section",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "C",
                "relevance": "BaseTool, ToolResult — base class reference",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "ViewResultsTool — reuse results loading pattern",
            },
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "RunExperimentTool — reference implementation",
            },
            {
                "path": "tests/mcp/test_e2e.py",
                "category": "C",
                "relevance": "E2E tests — add analysis tool e2e",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP user documentation",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "240",
        "title": "Remote server evaluation",
        "query_level": "Q1",
        "task": "I need to create a new `packages/evee-server/` package implementing a Dedicated Server compute backend for Evee. It should follow the `packages/evee-azureml/` pattern: implement `ComputeBackend` ABC from `src/evee/compute/backend.py`, register via entry points in `pyproject.toml`, include a FastAPI server daemon, code sync module, and HTTP client. The backend's `submit()` method should sync code, provision the remote env (reuse `wheels_provisioner`), and submit via HTTP. I need CLI commands for `evee server status/start`, updates to `VALID_COMPUTE_BACKENDS` in `cli/commands/compute.py`, deployment scripts, Makefile targets, and documentation.",
        "gt_files": [
            "src/evee/cli/commands/compute.py",
            "src/evee/mcp/resources/config.py",
            "docs/backends/overview.md",
            "Makefile",
            "tests/evee/cli/test_compute_commands.py",
            "src/evee/compute/backend.py",
            "src/evee/compute/local_compute_backend.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/pyproject.toml",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/compute/utils/wheels_provisioner.py",
            "tests/evee/execution/test_experiment_runner.py",
            "tests/evee/compute/test_local_compute_backend.py",
            "pyproject.toml",
            "CONTRIBUTING.md",
            "docs/backends/custom-backends.md",
            "docs/user-guide/configuration.md",
            "docs/user-guide/cli.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/compute.py",
                "category": "E",
                "relevance": 'Add `"server"` to `VALID_COMPUTE_BACKENDS`, `BACKEND_PACKAGES`',
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Update `CONFIG_SCHEMA_CONTENT` for server backend",
            },
            {
                "path": "docs/backends/overview.md",
                "category": "E",
                "relevance": "Add server backend to overview",
            },
            {
                "path": "Makefile",
                "category": "E",
                "relevance": "`setup-server`, `test-server` targets",
            },
            {
                "path": "tests/evee/cli/test_compute_commands.py",
                "category": "E",
                "relevance": "Add test cases for `evee compute set server`",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — interface unchanged",
            },
            {
                "path": "src/evee/compute/local_compute_backend.py",
                "category": "C",
                "relevance": "LocalComputeBackend — reference impl, unchanged",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "AzureML compute — reference pattern",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "Config pattern",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": '`ComputeBackendConfig` — `extra="allow"`, no changes',
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Uses `entry_points()` for discovery — no changes",
            },
            {
                "path": "src/evee/compute/utils/wheels_provisioner.py",
                "category": "C",
                "relevance": "May be reused by server backend — no changes",
            },
            {
                "path": "tests/evee/execution/test_experiment_runner.py",
                "category": "C",
                "relevance": "Runner tests reference",
            },
            {
                "path": "tests/evee/compute/test_local_compute_backend.py",
                "category": "C",
                "relevance": "Compute backend test reference",
            },
            {"path": "pyproject.toml", "category": "S", "relevance": "Core entry points reference"},
            {
                "path": "CONTRIBUTING.md",
                "category": "S",
                "relevance": "Extension package structure",
            },
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Custom backend guide",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
            {"path": "docs/user-guide/cli.md", "category": "S", "relevance": "CLI reference"},
        ],
        "difficulty": "complex",
    },
    {
        "issue": "240",
        "title": "Remote server evaluation",
        "query_level": "Q2",
        "task": "Add a remote server compute backend to Evee that lets users push evaluations to any remote environment. I need to follow the AzureML backend package pattern, create a new package with compute backend, server daemon, and code sync. Where are the compute backend interfaces, the reference AzureML implementation, and the CLI compute commands?",
        "gt_files": [
            "src/evee/cli/commands/compute.py",
            "src/evee/mcp/resources/config.py",
            "docs/backends/overview.md",
            "Makefile",
            "tests/evee/cli/test_compute_commands.py",
            "src/evee/compute/backend.py",
            "src/evee/compute/local_compute_backend.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/pyproject.toml",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/compute/utils/wheels_provisioner.py",
            "tests/evee/execution/test_experiment_runner.py",
            "tests/evee/compute/test_local_compute_backend.py",
            "pyproject.toml",
            "CONTRIBUTING.md",
            "docs/backends/custom-backends.md",
            "docs/user-guide/configuration.md",
            "docs/user-guide/cli.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/compute.py",
                "category": "E",
                "relevance": 'Add `"server"` to `VALID_COMPUTE_BACKENDS`, `BACKEND_PACKAGES`',
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Update `CONFIG_SCHEMA_CONTENT` for server backend",
            },
            {
                "path": "docs/backends/overview.md",
                "category": "E",
                "relevance": "Add server backend to overview",
            },
            {
                "path": "Makefile",
                "category": "E",
                "relevance": "`setup-server`, `test-server` targets",
            },
            {
                "path": "tests/evee/cli/test_compute_commands.py",
                "category": "E",
                "relevance": "Add test cases for `evee compute set server`",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — interface unchanged",
            },
            {
                "path": "src/evee/compute/local_compute_backend.py",
                "category": "C",
                "relevance": "LocalComputeBackend — reference impl, unchanged",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "AzureML compute — reference pattern",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "Config pattern",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": '`ComputeBackendConfig` — `extra="allow"`, no changes',
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Uses `entry_points()` for discovery — no changes",
            },
            {
                "path": "src/evee/compute/utils/wheels_provisioner.py",
                "category": "C",
                "relevance": "May be reused by server backend — no changes",
            },
            {
                "path": "tests/evee/execution/test_experiment_runner.py",
                "category": "C",
                "relevance": "Runner tests reference",
            },
            {
                "path": "tests/evee/compute/test_local_compute_backend.py",
                "category": "C",
                "relevance": "Compute backend test reference",
            },
            {"path": "pyproject.toml", "category": "S", "relevance": "Core entry points reference"},
            {
                "path": "CONTRIBUTING.md",
                "category": "S",
                "relevance": "Extension package structure",
            },
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Custom backend guide",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
            {"path": "docs/user-guide/cli.md", "category": "S", "relevance": "CLI reference"},
        ],
        "difficulty": "complex",
    },
    {
        "issue": "240",
        "title": "Remote server evaluation",
        "query_level": "Q3",
        "task": "I want to add a way to run Evee evaluations on a remote server instead of locally or on Azure ML. There should be a server component and a client that pushes jobs to it. How do Evee's compute backends work and where is the AzureML one for reference?",
        "gt_files": [
            "src/evee/cli/commands/compute.py",
            "src/evee/mcp/resources/config.py",
            "docs/backends/overview.md",
            "Makefile",
            "tests/evee/cli/test_compute_commands.py",
            "src/evee/compute/backend.py",
            "src/evee/compute/local_compute_backend.py",
            "packages/evee-azureml/src/evee_azureml/compute.py",
            "packages/evee-azureml/pyproject.toml",
            "packages/evee-azureml/src/evee_azureml/config.py",
            "src/evee/config/models.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/compute/utils/wheels_provisioner.py",
            "tests/evee/execution/test_experiment_runner.py",
            "tests/evee/compute/test_local_compute_backend.py",
            "pyproject.toml",
            "CONTRIBUTING.md",
            "docs/backends/custom-backends.md",
            "docs/user-guide/configuration.md",
            "docs/user-guide/cli.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/compute.py",
                "category": "E",
                "relevance": 'Add `"server"` to `VALID_COMPUTE_BACKENDS`, `BACKEND_PACKAGES`',
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Update `CONFIG_SCHEMA_CONTENT` for server backend",
            },
            {
                "path": "docs/backends/overview.md",
                "category": "E",
                "relevance": "Add server backend to overview",
            },
            {
                "path": "Makefile",
                "category": "E",
                "relevance": "`setup-server`, `test-server` targets",
            },
            {
                "path": "tests/evee/cli/test_compute_commands.py",
                "category": "E",
                "relevance": "Add test cases for `evee compute set server`",
            },
            {
                "path": "src/evee/compute/backend.py",
                "category": "C",
                "relevance": "ComputeBackend ABC — interface unchanged",
            },
            {
                "path": "src/evee/compute/local_compute_backend.py",
                "category": "C",
                "relevance": "LocalComputeBackend — reference impl, unchanged",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/compute.py",
                "category": "C",
                "relevance": "AzureML compute — reference pattern",
            },
            {
                "path": "packages/evee-azureml/pyproject.toml",
                "category": "C",
                "relevance": "Entry points pattern",
            },
            {
                "path": "packages/evee-azureml/src/evee_azureml/config.py",
                "category": "C",
                "relevance": "Config pattern",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": '`ComputeBackendConfig` — `extra="allow"`, no changes',
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "C",
                "relevance": "Uses `entry_points()` for discovery — no changes",
            },
            {
                "path": "src/evee/compute/utils/wheels_provisioner.py",
                "category": "C",
                "relevance": "May be reused by server backend — no changes",
            },
            {
                "path": "tests/evee/execution/test_experiment_runner.py",
                "category": "C",
                "relevance": "Runner tests reference",
            },
            {
                "path": "tests/evee/compute/test_local_compute_backend.py",
                "category": "C",
                "relevance": "Compute backend test reference",
            },
            {"path": "pyproject.toml", "category": "S", "relevance": "Core entry points reference"},
            {
                "path": "CONTRIBUTING.md",
                "category": "S",
                "relevance": "Extension package structure",
            },
            {
                "path": "docs/backends/custom-backends.md",
                "category": "S",
                "relevance": "Custom backend guide",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
            {"path": "docs/user-guide/cli.md", "category": "S", "relevance": "CLI reference"},
        ],
        "difficulty": "complex",
    },
    {
        "issue": "259",
        "title": "Raise explicit error when .env file is missing",
        "query_level": "Q1",
        "task": "I need to add startup validation in Evee that raises an explicit error when the `.env` file is missing. Currently, `.env` loading is handled silently in multiple places: `cli/commands/run.py`, `cli/commands/validate.py`, `cli/main.py` (`_execute_in_project_env`), `evaluation/model_evaluator.py`, `evaluation/evaluate.py`, `execution/experiment_runner.py`, and `core/base_model.py`. The `DEFAULT_ENV_FILE` constant is in `cli/constants.py`. I should add the check in `execution/preflight.py` (which already does pre-flight validation) and ensure it surfaces a clear error message before any downstream failures.",
        "gt_files": [
            "src/evee/cli/commands/run.py",
            "src/evee/cli/commands/validate.py",
            "src/evee/cli/main.py",
            "src/evee/execution/preflight.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/evaluation/evaluate.py",
            "src/evee/evaluation/model_evaluator.py",
            "tests/evee/cli/test_validate_command.py",
            "tests/evee/execution/test_experiment_runner.py",
            "tests/evee/execution/test_preflight.py",
            "src/evee/cli/constants.py",
            "src/evee/core/base_model.py",
            "src/evee/execution/runner.py",
            "docs/troubleshooting.md",
            "docs/getting-started/quickstart.md",
            "docs/user-guide/configuration.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/run.py",
                "category": "E",
                "relevance": "`_validate_paths()` — add .env check: warn if default missing, error if custom `--env` path missing",
            },
            {
                "path": "src/evee/cli/commands/validate.py",
                "category": "E",
                "relevance": "Silently skips missing .env — add explicit warning/error",
            },
            {
                "path": "src/evee/cli/main.py",
                "category": "E",
                "relevance": "`_execute_in_project_env()` — add .env check before delegation",
            },
            {
                "path": "src/evee/execution/preflight.py",
                "category": "E",
                "relevance": "Add .env existence check to `run_preflight_checks()`",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "E",
                "relevance": "`load_dotenv(dotenv_path=env_path)` — add check before call",
            },
            {
                "path": "src/evee/evaluation/evaluate.py",
                "category": "E",
                "relevance": "`load_dotenv(dotenv_path=env_path)` — add check before call",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "`load_dotenv(dotenv_path=env_path)` — add check/warning",
            },
            {
                "path": "tests/evee/cli/test_validate_command.py",
                "category": "E",
                "relevance": "Add tests for missing .env behavior",
            },
            {
                "path": "tests/evee/execution/test_experiment_runner.py",
                "category": "E",
                "relevance": "Add tests for .env missing error",
            },
            {
                "path": "tests/evee/execution/test_preflight.py",
                "category": "E",
                "relevance": "Add test for .env preflight check",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "C",
                "relevance": '`DEFAULT_ENV_FILE = ".env"` — already defined, no changes',
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`load_dotenv(verbose=True)` uses default discovery — check belongs upstream",
            },
            {
                "path": "src/evee/execution/runner.py",
                "category": "C",
                "relevance": "Generic runner, just passes `env_path` through",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "S",
                "relevance": "Troubleshooting — add missing .env entry",
            },
            {
                "path": "docs/getting-started/quickstart.md",
                "category": "S",
                "relevance": "Quickstart .env setup",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": ".env in config hierarchy",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "259",
        "title": "Raise explicit error when .env file is missing",
        "query_level": "Q2",
        "task": "Add a check that raises an explicit error when the `.env` file is missing in Evee. Currently it fails silently and causes confusing downstream errors. I need to find all places where `.env` is loaded, the preflight validation system, and the constants defining the default `.env` path.",
        "gt_files": [
            "src/evee/cli/commands/run.py",
            "src/evee/cli/commands/validate.py",
            "src/evee/cli/main.py",
            "src/evee/execution/preflight.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/evaluation/evaluate.py",
            "src/evee/evaluation/model_evaluator.py",
            "tests/evee/cli/test_validate_command.py",
            "tests/evee/execution/test_experiment_runner.py",
            "tests/evee/execution/test_preflight.py",
            "src/evee/cli/constants.py",
            "src/evee/core/base_model.py",
            "src/evee/execution/runner.py",
            "docs/troubleshooting.md",
            "docs/getting-started/quickstart.md",
            "docs/user-guide/configuration.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/run.py",
                "category": "E",
                "relevance": "`_validate_paths()` — add .env check: warn if default missing, error if custom `--env` path missing",
            },
            {
                "path": "src/evee/cli/commands/validate.py",
                "category": "E",
                "relevance": "Silently skips missing .env — add explicit warning/error",
            },
            {
                "path": "src/evee/cli/main.py",
                "category": "E",
                "relevance": "`_execute_in_project_env()` — add .env check before delegation",
            },
            {
                "path": "src/evee/execution/preflight.py",
                "category": "E",
                "relevance": "Add .env existence check to `run_preflight_checks()`",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "E",
                "relevance": "`load_dotenv(dotenv_path=env_path)` — add check before call",
            },
            {
                "path": "src/evee/evaluation/evaluate.py",
                "category": "E",
                "relevance": "`load_dotenv(dotenv_path=env_path)` — add check before call",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "`load_dotenv(dotenv_path=env_path)` — add check/warning",
            },
            {
                "path": "tests/evee/cli/test_validate_command.py",
                "category": "E",
                "relevance": "Add tests for missing .env behavior",
            },
            {
                "path": "tests/evee/execution/test_experiment_runner.py",
                "category": "E",
                "relevance": "Add tests for .env missing error",
            },
            {
                "path": "tests/evee/execution/test_preflight.py",
                "category": "E",
                "relevance": "Add test for .env preflight check",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "C",
                "relevance": '`DEFAULT_ENV_FILE = ".env"` — already defined, no changes',
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`load_dotenv(verbose=True)` uses default discovery — check belongs upstream",
            },
            {
                "path": "src/evee/execution/runner.py",
                "category": "C",
                "relevance": "Generic runner, just passes `env_path` through",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "S",
                "relevance": "Troubleshooting — add missing .env entry",
            },
            {
                "path": "docs/getting-started/quickstart.md",
                "category": "S",
                "relevance": "Quickstart .env setup",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": ".env in config hierarchy",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "259",
        "title": "Raise explicit error when .env file is missing",
        "query_level": "Q3",
        "task": "Evee should tell users clearly when their `.env` file is missing instead of failing with confusing errors later. Where does Evee load the `.env` file and where should this validation go?",
        "gt_files": [
            "src/evee/cli/commands/run.py",
            "src/evee/cli/commands/validate.py",
            "src/evee/cli/main.py",
            "src/evee/execution/preflight.py",
            "src/evee/execution/experiment_runner.py",
            "src/evee/evaluation/evaluate.py",
            "src/evee/evaluation/model_evaluator.py",
            "tests/evee/cli/test_validate_command.py",
            "tests/evee/execution/test_experiment_runner.py",
            "tests/evee/execution/test_preflight.py",
            "src/evee/cli/constants.py",
            "src/evee/core/base_model.py",
            "src/evee/execution/runner.py",
            "docs/troubleshooting.md",
            "docs/getting-started/quickstart.md",
            "docs/user-guide/configuration.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/run.py",
                "category": "E",
                "relevance": "`_validate_paths()` — add .env check: warn if default missing, error if custom `--env` path missing",
            },
            {
                "path": "src/evee/cli/commands/validate.py",
                "category": "E",
                "relevance": "Silently skips missing .env — add explicit warning/error",
            },
            {
                "path": "src/evee/cli/main.py",
                "category": "E",
                "relevance": "`_execute_in_project_env()` — add .env check before delegation",
            },
            {
                "path": "src/evee/execution/preflight.py",
                "category": "E",
                "relevance": "Add .env existence check to `run_preflight_checks()`",
            },
            {
                "path": "src/evee/execution/experiment_runner.py",
                "category": "E",
                "relevance": "`load_dotenv(dotenv_path=env_path)` — add check before call",
            },
            {
                "path": "src/evee/evaluation/evaluate.py",
                "category": "E",
                "relevance": "`load_dotenv(dotenv_path=env_path)` — add check before call",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "`load_dotenv(dotenv_path=env_path)` — add check/warning",
            },
            {
                "path": "tests/evee/cli/test_validate_command.py",
                "category": "E",
                "relevance": "Add tests for missing .env behavior",
            },
            {
                "path": "tests/evee/execution/test_experiment_runner.py",
                "category": "E",
                "relevance": "Add tests for .env missing error",
            },
            {
                "path": "tests/evee/execution/test_preflight.py",
                "category": "E",
                "relevance": "Add test for .env preflight check",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "C",
                "relevance": '`DEFAULT_ENV_FILE = ".env"` — already defined, no changes',
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`load_dotenv(verbose=True)` uses default discovery — check belongs upstream",
            },
            {
                "path": "src/evee/execution/runner.py",
                "category": "C",
                "relevance": "Generic runner, just passes `env_path` through",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "S",
                "relevance": "Troubleshooting — add missing .env entry",
            },
            {
                "path": "docs/getting-started/quickstart.md",
                "category": "S",
                "relevance": "Quickstart .env setup",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": ".env in config hierarchy",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "260",
        "title": "Add Config Flag to Disable rich Progress Bars in CI",
        "query_level": "Q1",
        "task": "I need to add a configuration flag in `config.yaml` to disable Rich progress bars for CI environments. The `is_rich_compatible_environment()` function in `src/evee/utils/environment.py` already checks `EVEE_DISABLE_RICH_LOGGING` env var and MCP mode. The `ProgressTracker` in `evaluation/progress_tracker.py` and logger in `logging/logger.py` both use this function. I need to add a new field to `RuntimeConfig` or `ExperimentConfig` in `config/models.py`, update the environment check to also consult the config flag, and update tests and documentation.",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/utils/environment.py",
            "src/evee/evaluation/progress_tracker.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/mcp/resources/config.py",
            "tests/evee/evaluation/test_progress_tracker.py",
            "tests/evee/log/test_logger.py",
            "docs/user-guide/configuration.md",
            "docs/troubleshooting.md",
            "src/evee/logging/logger.py",
            "tests/evee/conftest.py",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "`RuntimeConfig` or `ExperimentConfig` — add `disable_rich_progress: bool = False`",
            },
            {
                "path": "src/evee/utils/environment.py",
                "category": "E",
                "relevance": "`is_rich_compatible_environment()` — add config flag check",
            },
            {
                "path": "src/evee/evaluation/progress_tracker.py",
                "category": "E",
                "relevance": "`ProgressTracker` — pass config override to Rich check",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "Pass `disable_rich_progress` from config when constructing `ProgressTracker`",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Config schema — document `disable_rich_progress` field",
            },
            {
                "path": "tests/evee/evaluation/test_progress_tracker.py",
                "category": "E",
                "relevance": "Add tests for config-driven Rich disabling",
            },
            {
                "path": "tests/evee/log/test_logger.py",
                "category": "E",
                "relevance": "Add/update tests verifying flag propagation",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference — new flag",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "S",
                "relevance": "Disable Rich Console section",
            },
            {
                "path": "src/evee/logging/logger.py",
                "category": "C",
                "relevance": "Already calls `is_rich_compatible_environment()` — inherits change, no direct edits",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Sets `EVEE_DISABLE_RICH_LOGGING=true` — reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "260",
        "title": "Add Config Flag to Disable rich Progress Bars in CI",
        "query_level": "Q2",
        "task": "Add a config.yaml flag to disable Rich progress bars. Evee already suppresses them for MCP and AzureML, but users need to disable them for CI too. I need to find where Rich environment detection happens, the config model, and the progress tracker/logger code.",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/utils/environment.py",
            "src/evee/evaluation/progress_tracker.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/mcp/resources/config.py",
            "tests/evee/evaluation/test_progress_tracker.py",
            "tests/evee/log/test_logger.py",
            "docs/user-guide/configuration.md",
            "docs/troubleshooting.md",
            "src/evee/logging/logger.py",
            "tests/evee/conftest.py",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "`RuntimeConfig` or `ExperimentConfig` — add `disable_rich_progress: bool = False`",
            },
            {
                "path": "src/evee/utils/environment.py",
                "category": "E",
                "relevance": "`is_rich_compatible_environment()` — add config flag check",
            },
            {
                "path": "src/evee/evaluation/progress_tracker.py",
                "category": "E",
                "relevance": "`ProgressTracker` — pass config override to Rich check",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "Pass `disable_rich_progress` from config when constructing `ProgressTracker`",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Config schema — document `disable_rich_progress` field",
            },
            {
                "path": "tests/evee/evaluation/test_progress_tracker.py",
                "category": "E",
                "relevance": "Add tests for config-driven Rich disabling",
            },
            {
                "path": "tests/evee/log/test_logger.py",
                "category": "E",
                "relevance": "Add/update tests verifying flag propagation",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference — new flag",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "S",
                "relevance": "Disable Rich Console section",
            },
            {
                "path": "src/evee/logging/logger.py",
                "category": "C",
                "relevance": "Already calls `is_rich_compatible_environment()` — inherits change, no direct edits",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Sets `EVEE_DISABLE_RICH_LOGGING=true` — reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "260",
        "title": "Add Config Flag to Disable rich Progress Bars in CI",
        "query_level": "Q3",
        "task": "Rich progress bars clutter CI logs. I need to add a way to disable them via config. Where does Evee decide whether to show Rich output and how do I add a config flag?",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/utils/environment.py",
            "src/evee/evaluation/progress_tracker.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/mcp/resources/config.py",
            "tests/evee/evaluation/test_progress_tracker.py",
            "tests/evee/log/test_logger.py",
            "docs/user-guide/configuration.md",
            "docs/troubleshooting.md",
            "src/evee/logging/logger.py",
            "tests/evee/conftest.py",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "`RuntimeConfig` or `ExperimentConfig` — add `disable_rich_progress: bool = False`",
            },
            {
                "path": "src/evee/utils/environment.py",
                "category": "E",
                "relevance": "`is_rich_compatible_environment()` — add config flag check",
            },
            {
                "path": "src/evee/evaluation/progress_tracker.py",
                "category": "E",
                "relevance": "`ProgressTracker` — pass config override to Rich check",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "Pass `disable_rich_progress` from config when constructing `ProgressTracker`",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Config schema — document `disable_rich_progress` field",
            },
            {
                "path": "tests/evee/evaluation/test_progress_tracker.py",
                "category": "E",
                "relevance": "Add tests for config-driven Rich disabling",
            },
            {
                "path": "tests/evee/log/test_logger.py",
                "category": "E",
                "relevance": "Add/update tests verifying flag propagation",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference — new flag",
            },
            {
                "path": "docs/troubleshooting.md",
                "category": "S",
                "relevance": "Disable Rich Console section",
            },
            {
                "path": "src/evee/logging/logger.py",
                "category": "C",
                "relevance": "Already calls `is_rich_compatible_environment()` — inherits change, no direct edits",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Sets `EVEE_DISABLE_RICH_LOGGING=true` — reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "261",
        "title": "Add pytest-aitest MCP interface test suite",
        "query_level": "Q1",
        "task": "I need to add AI interface tests using `pytest-aitest` for Evee's MCP server. The tests (in `tests/mcp_aitest/`) should validate that an LLM can correctly invoke all 5 MCP tools (`validate_config`, `list_components`, `run_experiment`, `view_results`, `fetch_documentation`) from natural language prompts. I also need to reword the `validate_config` tool description in `tools/validation.py` so LLMs treat validation as advisory, remove empty `warnings: []` from error responses for token reduction, add `pytest-aitest>=0.5.6` to `pyproject.toml` dev dependencies, and add a `test-mcp-aitest` Makefile target.",
        "gt_files": [
            "src/evee/mcp/tools/validation.py",
            "pyproject.toml",
            "Makefile",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/server.py",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/constants.py",
            "tests/mcp/conftest.py",
            "tests/mcp/test_tools.py",
            "docs/user-guide/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "E",
                "relevance": "Reword tool description for clearer AI discoverability",
            },
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Add `pytest-aitest` dev dependency and `aitest` marker",
            },
            {"path": "Makefile", "category": "E", "relevance": "Add `test-mcp-aitest` target"},
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "Tool description may need minor rewording",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "C",
                "relevance": "Tool description may need minor rewording",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "Tool description may need minor rewording",
            },
            {
                "path": "src/evee/mcp/server.py",
                "category": "C",
                "relevance": "MCP server — read tool registration, no changes",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "C",
                "relevance": "`ToolSchema`, `ToolMetadata` — referenced, not modified",
            },
            {
                "path": "src/evee/mcp/constants.py",
                "category": "C",
                "relevance": "`ToolNames` — used in assertions, not modified",
            },
            {
                "path": "tests/mcp/conftest.py",
                "category": "C",
                "relevance": "Reference fixture patterns for new tests",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "C",
                "relevance": "Reference existing test patterns",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP user docs",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "261",
        "title": "Add pytest-aitest MCP interface test suite",
        "query_level": "Q2",
        "task": "Add AI-driven integration tests for Evee's MCP server using pytest-aitest. The tests should verify that an LLM can invoke all MCP tools from natural language. I need to understand the existing MCP tool implementations, their descriptions, and the existing test structure. Also need to adjust some tool descriptions.",
        "gt_files": [
            "src/evee/mcp/tools/validation.py",
            "pyproject.toml",
            "Makefile",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/server.py",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/constants.py",
            "tests/mcp/conftest.py",
            "tests/mcp/test_tools.py",
            "docs/user-guide/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "E",
                "relevance": "Reword tool description for clearer AI discoverability",
            },
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Add `pytest-aitest` dev dependency and `aitest` marker",
            },
            {"path": "Makefile", "category": "E", "relevance": "Add `test-mcp-aitest` target"},
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "Tool description may need minor rewording",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "C",
                "relevance": "Tool description may need minor rewording",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "Tool description may need minor rewording",
            },
            {
                "path": "src/evee/mcp/server.py",
                "category": "C",
                "relevance": "MCP server — read tool registration, no changes",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "C",
                "relevance": "`ToolSchema`, `ToolMetadata` — referenced, not modified",
            },
            {
                "path": "src/evee/mcp/constants.py",
                "category": "C",
                "relevance": "`ToolNames` — used in assertions, not modified",
            },
            {
                "path": "tests/mcp/conftest.py",
                "category": "C",
                "relevance": "Reference fixture patterns for new tests",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "C",
                "relevance": "Reference existing test patterns",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP user docs",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "261",
        "title": "Add pytest-aitest MCP interface test suite",
        "query_level": "Q3",
        "task": "I want to add tests that verify an LLM can use Evee's MCP tools correctly from natural language. There's a library called pytest-aitest for this. Where are the MCP tools and how do I set up these tests?",
        "gt_files": [
            "src/evee/mcp/tools/validation.py",
            "pyproject.toml",
            "Makefile",
            "src/evee/mcp/tools/experiment.py",
            "src/evee/mcp/tools/discovery.py",
            "src/evee/mcp/tools/view_results.py",
            "src/evee/mcp/server.py",
            "src/evee/mcp/tools/base.py",
            "src/evee/mcp/constants.py",
            "tests/mcp/conftest.py",
            "tests/mcp/test_tools.py",
            "docs/user-guide/mcp-server.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/mcp/tools/validation.py",
                "category": "E",
                "relevance": "Reword tool description for clearer AI discoverability",
            },
            {
                "path": "pyproject.toml",
                "category": "E",
                "relevance": "Add `pytest-aitest` dev dependency and `aitest` marker",
            },
            {"path": "Makefile", "category": "E", "relevance": "Add `test-mcp-aitest` target"},
            {
                "path": "src/evee/mcp/tools/experiment.py",
                "category": "C",
                "relevance": "Tool description may need minor rewording",
            },
            {
                "path": "src/evee/mcp/tools/discovery.py",
                "category": "C",
                "relevance": "Tool description may need minor rewording",
            },
            {
                "path": "src/evee/mcp/tools/view_results.py",
                "category": "C",
                "relevance": "Tool description may need minor rewording",
            },
            {
                "path": "src/evee/mcp/server.py",
                "category": "C",
                "relevance": "MCP server — read tool registration, no changes",
            },
            {
                "path": "src/evee/mcp/tools/base.py",
                "category": "C",
                "relevance": "`ToolSchema`, `ToolMetadata` — referenced, not modified",
            },
            {
                "path": "src/evee/mcp/constants.py",
                "category": "C",
                "relevance": "`ToolNames` — used in assertions, not modified",
            },
            {
                "path": "tests/mcp/conftest.py",
                "category": "C",
                "relevance": "Reference fixture patterns for new tests",
            },
            {
                "path": "tests/mcp/test_tools.py",
                "category": "C",
                "relevance": "Reference existing test patterns",
            },
            {
                "path": "docs/user-guide/mcp-server.md",
                "category": "S",
                "relevance": "MCP user docs",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "262",
        "title": "Support for Configurable REST-Based Models",
        "query_level": "Q1",
        "task": "I need to add support for configurable REST-based models in Evee. Instead of writing custom `@model` decorated classes for each REST endpoint, users should define REST model configuration in `config.yaml` (endpoint URL, request/response format, headers, auth). This requires changes to `ModelVariantConfig` in `config/models.py`, a new `RestModel` class that doesn't use `@model` decorator but still works with `ModelEvaluator._register_model`, connection handling via `ExecutionContext.connections_registry`, and a REST model template for CLI scaffolding. The model should bypass `decorator_discovery` and instead be resolved from config.",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/core/__init__.py",
            "src/evee/mcp/resources/model_patterns.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/cli/commands/model.py",
            "src/evee/cli/utils/model_operations.py",
            "src/evee/cli/commands/validate.py",
            "tests/evee/evaluation/test_model_evaluator_init.py",
            "src/evee/core/base_model.py",
            "src/evee/core/execution_context.py",
            "src/evee/core/decorator_discovery.py",
            "src/evee/cli/templates/model/empty_model.py",
            "src/evee/cli/templates/base/models/baseline.py",
            "tests/evee/cli/test_model_commands.py",
            "docs/user-guide/models.md",
            "docs/user-guide/configuration.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": '`ModelVariantConfig` — add REST-specific fields (`endpoint`, `method`, `headers`, `request_mapping`, `response_mapping`, `type: "rest"`)',
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": '`_register_model` — detect `type: "rest"` and instantiate `RestModel`',
            },
            {
                "path": "src/evee/core/__init__.py",
                "category": "E",
                "relevance": "Export `RestModel`",
            },
            {
                "path": "src/evee/mcp/resources/model_patterns.py",
                "category": "E",
                "relevance": "Add REST model pattern section",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Add REST model config schema",
            },
            {
                "path": "src/evee/cli/commands/model.py",
                "category": "E",
                "relevance": "Add `--type rest` option to scaffold REST models",
            },
            {
                "path": "src/evee/cli/utils/model_operations.py",
                "category": "E",
                "relevance": "REST model file creation logic",
            },
            {
                "path": "src/evee/cli/commands/validate.py",
                "category": "E",
                "relevance": "Validate REST model config fields",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_init.py",
                "category": "E",
                "relevance": "Add tests for REST model registration path",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`BaseModel`, `MODEL_REGISTRY` — interface unchanged",
            },
            {
                "path": "src/evee/core/execution_context.py",
                "category": "C",
                "relevance": "`connections_registry` — REST models use this, no changes",
            },
            {
                "path": "src/evee/core/decorator_discovery.py",
                "category": "C",
                "relevance": "REST models bypass discovery (config-driven) — no changes",
            },
            {
                "path": "src/evee/cli/templates/model/empty_model.py",
                "category": "C",
                "relevance": "Template reference",
            },
            {
                "path": "src/evee/cli/templates/base/models/baseline.py",
                "category": "C",
                "relevance": "Template reference",
            },
            {
                "path": "tests/evee/cli/test_model_commands.py",
                "category": "E",
                "relevance": "Model CLI tests — add REST scaffolding tests",
            },
            {
                "path": "docs/user-guide/models.md",
                "category": "S",
                "relevance": "Model documentation",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "262",
        "title": "Support for Configurable REST-Based Models",
        "query_level": "Q2",
        "task": "Add configuration-driven REST models to Evee so users don't need to write model classes for simple REST endpoints. I need to understand the model registration system (`@model` decorator, MODEL_REGISTRY, `_register_model`), the config schema, and how the evaluation pipeline works with models.",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/core/__init__.py",
            "src/evee/mcp/resources/model_patterns.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/cli/commands/model.py",
            "src/evee/cli/utils/model_operations.py",
            "src/evee/cli/commands/validate.py",
            "tests/evee/evaluation/test_model_evaluator_init.py",
            "src/evee/core/base_model.py",
            "src/evee/core/execution_context.py",
            "src/evee/core/decorator_discovery.py",
            "src/evee/cli/templates/model/empty_model.py",
            "src/evee/cli/templates/base/models/baseline.py",
            "tests/evee/cli/test_model_commands.py",
            "docs/user-guide/models.md",
            "docs/user-guide/configuration.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": '`ModelVariantConfig` — add REST-specific fields (`endpoint`, `method`, `headers`, `request_mapping`, `response_mapping`, `type: "rest"`)',
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": '`_register_model` — detect `type: "rest"` and instantiate `RestModel`',
            },
            {
                "path": "src/evee/core/__init__.py",
                "category": "E",
                "relevance": "Export `RestModel`",
            },
            {
                "path": "src/evee/mcp/resources/model_patterns.py",
                "category": "E",
                "relevance": "Add REST model pattern section",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Add REST model config schema",
            },
            {
                "path": "src/evee/cli/commands/model.py",
                "category": "E",
                "relevance": "Add `--type rest` option to scaffold REST models",
            },
            {
                "path": "src/evee/cli/utils/model_operations.py",
                "category": "E",
                "relevance": "REST model file creation logic",
            },
            {
                "path": "src/evee/cli/commands/validate.py",
                "category": "E",
                "relevance": "Validate REST model config fields",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_init.py",
                "category": "E",
                "relevance": "Add tests for REST model registration path",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`BaseModel`, `MODEL_REGISTRY` — interface unchanged",
            },
            {
                "path": "src/evee/core/execution_context.py",
                "category": "C",
                "relevance": "`connections_registry` — REST models use this, no changes",
            },
            {
                "path": "src/evee/core/decorator_discovery.py",
                "category": "C",
                "relevance": "REST models bypass discovery (config-driven) — no changes",
            },
            {
                "path": "src/evee/cli/templates/model/empty_model.py",
                "category": "C",
                "relevance": "Template reference",
            },
            {
                "path": "src/evee/cli/templates/base/models/baseline.py",
                "category": "C",
                "relevance": "Template reference",
            },
            {
                "path": "tests/evee/cli/test_model_commands.py",
                "category": "E",
                "relevance": "Model CLI tests — add REST scaffolding tests",
            },
            {
                "path": "docs/user-guide/models.md",
                "category": "S",
                "relevance": "Model documentation",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "262",
        "title": "Support for Configurable REST-Based Models",
        "query_level": "Q3",
        "task": "Users keep writing model classes that just wrap REST calls. I want to make REST models configurable instead. Where is the model system in Evee and how does model registration work?",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/core/__init__.py",
            "src/evee/mcp/resources/model_patterns.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/cli/commands/model.py",
            "src/evee/cli/utils/model_operations.py",
            "src/evee/cli/commands/validate.py",
            "tests/evee/evaluation/test_model_evaluator_init.py",
            "src/evee/core/base_model.py",
            "src/evee/core/execution_context.py",
            "src/evee/core/decorator_discovery.py",
            "src/evee/cli/templates/model/empty_model.py",
            "src/evee/cli/templates/base/models/baseline.py",
            "tests/evee/cli/test_model_commands.py",
            "docs/user-guide/models.md",
            "docs/user-guide/configuration.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": '`ModelVariantConfig` — add REST-specific fields (`endpoint`, `method`, `headers`, `request_mapping`, `response_mapping`, `type: "rest"`)',
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": '`_register_model` — detect `type: "rest"` and instantiate `RestModel`',
            },
            {
                "path": "src/evee/core/__init__.py",
                "category": "E",
                "relevance": "Export `RestModel`",
            },
            {
                "path": "src/evee/mcp/resources/model_patterns.py",
                "category": "E",
                "relevance": "Add REST model pattern section",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Add REST model config schema",
            },
            {
                "path": "src/evee/cli/commands/model.py",
                "category": "E",
                "relevance": "Add `--type rest` option to scaffold REST models",
            },
            {
                "path": "src/evee/cli/utils/model_operations.py",
                "category": "E",
                "relevance": "REST model file creation logic",
            },
            {
                "path": "src/evee/cli/commands/validate.py",
                "category": "E",
                "relevance": "Validate REST model config fields",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_init.py",
                "category": "E",
                "relevance": "Add tests for REST model registration path",
            },
            {
                "path": "src/evee/core/base_model.py",
                "category": "C",
                "relevance": "`BaseModel`, `MODEL_REGISTRY` — interface unchanged",
            },
            {
                "path": "src/evee/core/execution_context.py",
                "category": "C",
                "relevance": "`connections_registry` — REST models use this, no changes",
            },
            {
                "path": "src/evee/core/decorator_discovery.py",
                "category": "C",
                "relevance": "REST models bypass discovery (config-driven) — no changes",
            },
            {
                "path": "src/evee/cli/templates/model/empty_model.py",
                "category": "C",
                "relevance": "Template reference",
            },
            {
                "path": "src/evee/cli/templates/base/models/baseline.py",
                "category": "C",
                "relevance": "Template reference",
            },
            {
                "path": "tests/evee/cli/test_model_commands.py",
                "category": "E",
                "relevance": "Model CLI tests — add REST scaffolding tests",
            },
            {
                "path": "docs/user-guide/models.md",
                "category": "S",
                "relevance": "Model documentation",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Config reference",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "263",
        "title": "Implement Foundry metric automatically and fix metric scaffolding path",
        "query_level": "Q1",
        "task": "I need to make Foundry metric scaffolding fully implement the metric class instead of generating a stub with `NotImplementedError`. The `_create_azure_metric()` in `cli/commands/metric.py` uses the template at `cli/templates/metrics/azure_evaluator_metric.py` which produces a non-functional stub. I need to use the evaluator metadata from `cli/azure_evaluators.json` (parameters, callable_params) to generate a complete `compute()` and `aggregate()` implementation. Also fix `get_metric_file_path()` in `cli/utils/metric_operations.py` which places files in the wrong directory. Reference working implementations like `example/metrics/f1score_metric.py`.",
        "gt_files": [
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/utils/metric_operations.py",
            "src/evee/cli/templates/metrics/azure_evaluator_metric.py",
            "src/evee/cli/azure_evaluators.json",
            "scripts/generate_azure_evaluators.py",
            "tests/evee/cli/test_metric_commands.py",
            "tests/evee/cli/utils/test_metric_operations.py",
            "src/evee/cli/constants.py",
            "src/evee/cli/utils/validators.py",
            "src/evee/cli/utils/init_file_manager.py",
            "src/evee/cli/utils/config_manager.py",
            "src/evee/core/base_metric.py",
            "src/evee/config/models.py",
            "src/evee/evaluation/model_evaluator.py",
            "tests/evee/test_azure_evaluators_metadata.py",
            "tests/scripts/test_generate_azure_evaluators.py",
            "example/metrics/f1score_metric.py",
            "docs/user-guide/metrics.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "`_create_azure_metric()`, `_generate_metric_content()` — fix to produce working implementations",
            },
            {
                "path": "src/evee/cli/utils/metric_operations.py",
                "category": "E",
                "relevance": "`get_metric_file_path()` — fix path construction bug",
            },
            {
                "path": "src/evee/cli/templates/metrics/azure_evaluator_metric.py",
                "category": "E",
                "relevance": "Replace `NotImplementedError` stubs with working evaluator call and aggregation",
            },
            {
                "path": "src/evee/cli/azure_evaluators.json",
                "category": "E",
                "relevance": "Enhance metadata if parameter mappings insufficient for auto-generation",
            },
            {
                "path": "scripts/generate_azure_evaluators.py",
                "category": "E",
                "relevance": "Update if JSON metadata format changes",
            },
            {
                "path": "tests/evee/cli/test_metric_commands.py",
                "category": "E",
                "relevance": "Update/add tests for fixed metric scaffolding output",
            },
            {
                "path": "tests/evee/cli/utils/test_metric_operations.py",
                "category": "E",
                "relevance": "Test fixed `get_metric_file_path` behavior",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "C",
                "relevance": "`DEFAULT_METRICS_DIR`, `METRIC_FILE_SUFFIX` — referenced, not modified",
            },
            {
                "path": "src/evee/cli/utils/validators.py",
                "category": "C",
                "relevance": "`validate_metric_name` — used by command, not modified",
            },
            {
                "path": "src/evee/cli/utils/init_file_manager.py",
                "category": "C",
                "relevance": "`add_import_to_init` — used downstream, not modified",
            },
            {
                "path": "src/evee/cli/utils/config_manager.py",
                "category": "C",
                "relevance": "`add_metric` — used downstream, not modified",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`@metric`, `BaseMetric` interface — unchanged",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "`MetricConfig` interface — unchanged",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "C",
                "relevance": "`_register_metric()` — unchanged",
            },
            {
                "path": "tests/evee/test_azure_evaluators_metadata.py",
                "category": "C",
                "relevance": "Evaluator metadata tests",
            },
            {
                "path": "tests/scripts/test_generate_azure_evaluators.py",
                "category": "C",
                "relevance": "Metadata generation tests",
            },
            {
                "path": "example/metrics/f1score_metric.py",
                "category": "S",
                "relevance": "Reference: working metric",
            },
            {
                "path": "docs/user-guide/metrics.md",
                "category": "S",
                "relevance": "Metric implementation guide",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "263",
        "title": "Implement Foundry metric automatically and fix metric scaffolding path",
        "query_level": "Q2",
        "task": "Fix Foundry metric scaffolding to generate fully functional metric implementations instead of stubs. Also fix the file path where scaffolded metrics are created. I need the metric CLI commands, template system, evaluator metadata, and reference metric implementations.",
        "gt_files": [
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/utils/metric_operations.py",
            "src/evee/cli/templates/metrics/azure_evaluator_metric.py",
            "src/evee/cli/azure_evaluators.json",
            "scripts/generate_azure_evaluators.py",
            "tests/evee/cli/test_metric_commands.py",
            "tests/evee/cli/utils/test_metric_operations.py",
            "src/evee/cli/constants.py",
            "src/evee/cli/utils/validators.py",
            "src/evee/cli/utils/init_file_manager.py",
            "src/evee/cli/utils/config_manager.py",
            "src/evee/core/base_metric.py",
            "src/evee/config/models.py",
            "src/evee/evaluation/model_evaluator.py",
            "tests/evee/test_azure_evaluators_metadata.py",
            "tests/scripts/test_generate_azure_evaluators.py",
            "example/metrics/f1score_metric.py",
            "docs/user-guide/metrics.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "`_create_azure_metric()`, `_generate_metric_content()` — fix to produce working implementations",
            },
            {
                "path": "src/evee/cli/utils/metric_operations.py",
                "category": "E",
                "relevance": "`get_metric_file_path()` — fix path construction bug",
            },
            {
                "path": "src/evee/cli/templates/metrics/azure_evaluator_metric.py",
                "category": "E",
                "relevance": "Replace `NotImplementedError` stubs with working evaluator call and aggregation",
            },
            {
                "path": "src/evee/cli/azure_evaluators.json",
                "category": "E",
                "relevance": "Enhance metadata if parameter mappings insufficient for auto-generation",
            },
            {
                "path": "scripts/generate_azure_evaluators.py",
                "category": "E",
                "relevance": "Update if JSON metadata format changes",
            },
            {
                "path": "tests/evee/cli/test_metric_commands.py",
                "category": "E",
                "relevance": "Update/add tests for fixed metric scaffolding output",
            },
            {
                "path": "tests/evee/cli/utils/test_metric_operations.py",
                "category": "E",
                "relevance": "Test fixed `get_metric_file_path` behavior",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "C",
                "relevance": "`DEFAULT_METRICS_DIR`, `METRIC_FILE_SUFFIX` — referenced, not modified",
            },
            {
                "path": "src/evee/cli/utils/validators.py",
                "category": "C",
                "relevance": "`validate_metric_name` — used by command, not modified",
            },
            {
                "path": "src/evee/cli/utils/init_file_manager.py",
                "category": "C",
                "relevance": "`add_import_to_init` — used downstream, not modified",
            },
            {
                "path": "src/evee/cli/utils/config_manager.py",
                "category": "C",
                "relevance": "`add_metric` — used downstream, not modified",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`@metric`, `BaseMetric` interface — unchanged",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "`MetricConfig` interface — unchanged",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "C",
                "relevance": "`_register_metric()` — unchanged",
            },
            {
                "path": "tests/evee/test_azure_evaluators_metadata.py",
                "category": "C",
                "relevance": "Evaluator metadata tests",
            },
            {
                "path": "tests/scripts/test_generate_azure_evaluators.py",
                "category": "C",
                "relevance": "Metadata generation tests",
            },
            {
                "path": "example/metrics/f1score_metric.py",
                "category": "S",
                "relevance": "Reference: working metric",
            },
            {
                "path": "docs/user-guide/metrics.md",
                "category": "S",
                "relevance": "Metric implementation guide",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "263",
        "title": "Implement Foundry metric automatically and fix metric scaffolding path",
        "query_level": "Q3",
        "task": "When I add a Foundry metric via the CLI, it creates a stub that doesn't work. I need it to generate a working implementation. Also, files end up in the wrong directory. Where is the metric scaffolding code?",
        "gt_files": [
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/utils/metric_operations.py",
            "src/evee/cli/templates/metrics/azure_evaluator_metric.py",
            "src/evee/cli/azure_evaluators.json",
            "scripts/generate_azure_evaluators.py",
            "tests/evee/cli/test_metric_commands.py",
            "tests/evee/cli/utils/test_metric_operations.py",
            "src/evee/cli/constants.py",
            "src/evee/cli/utils/validators.py",
            "src/evee/cli/utils/init_file_manager.py",
            "src/evee/cli/utils/config_manager.py",
            "src/evee/core/base_metric.py",
            "src/evee/config/models.py",
            "src/evee/evaluation/model_evaluator.py",
            "tests/evee/test_azure_evaluators_metadata.py",
            "tests/scripts/test_generate_azure_evaluators.py",
            "example/metrics/f1score_metric.py",
            "docs/user-guide/metrics.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "`_create_azure_metric()`, `_generate_metric_content()` — fix to produce working implementations",
            },
            {
                "path": "src/evee/cli/utils/metric_operations.py",
                "category": "E",
                "relevance": "`get_metric_file_path()` — fix path construction bug",
            },
            {
                "path": "src/evee/cli/templates/metrics/azure_evaluator_metric.py",
                "category": "E",
                "relevance": "Replace `NotImplementedError` stubs with working evaluator call and aggregation",
            },
            {
                "path": "src/evee/cli/azure_evaluators.json",
                "category": "E",
                "relevance": "Enhance metadata if parameter mappings insufficient for auto-generation",
            },
            {
                "path": "scripts/generate_azure_evaluators.py",
                "category": "E",
                "relevance": "Update if JSON metadata format changes",
            },
            {
                "path": "tests/evee/cli/test_metric_commands.py",
                "category": "E",
                "relevance": "Update/add tests for fixed metric scaffolding output",
            },
            {
                "path": "tests/evee/cli/utils/test_metric_operations.py",
                "category": "E",
                "relevance": "Test fixed `get_metric_file_path` behavior",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "C",
                "relevance": "`DEFAULT_METRICS_DIR`, `METRIC_FILE_SUFFIX` — referenced, not modified",
            },
            {
                "path": "src/evee/cli/utils/validators.py",
                "category": "C",
                "relevance": "`validate_metric_name` — used by command, not modified",
            },
            {
                "path": "src/evee/cli/utils/init_file_manager.py",
                "category": "C",
                "relevance": "`add_import_to_init` — used downstream, not modified",
            },
            {
                "path": "src/evee/cli/utils/config_manager.py",
                "category": "C",
                "relevance": "`add_metric` — used downstream, not modified",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`@metric`, `BaseMetric` interface — unchanged",
            },
            {
                "path": "src/evee/config/models.py",
                "category": "C",
                "relevance": "`MetricConfig` interface — unchanged",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "C",
                "relevance": "`_register_metric()` — unchanged",
            },
            {
                "path": "tests/evee/test_azure_evaluators_metadata.py",
                "category": "C",
                "relevance": "Evaluator metadata tests",
            },
            {
                "path": "tests/scripts/test_generate_azure_evaluators.py",
                "category": "C",
                "relevance": "Metadata generation tests",
            },
            {
                "path": "example/metrics/f1score_metric.py",
                "category": "S",
                "relevance": "Reference: working metric",
            },
            {
                "path": "docs/user-guide/metrics.md",
                "category": "S",
                "relevance": "Metric implementation guide",
            },
        ],
        "difficulty": "medium",
    },
    {
        "issue": "268",
        "title": "Add support for configuring metric sets for different use cases (1P and 3P RAI)",
        "query_level": "Q1",
        "task": 'I need to add configurable metric sets (presets) to Evee for Responsible AI compliance. Users should be able to specify a metric set name (e.g., "rai_1p", "rai_3p") in their config and have it resolve to a predefined list of metrics with specific parameters. This requires a new config model for metric sets in `config/models.py`, resolution logic in `ModelEvaluator._get_metrics_for_model()`, CLI commands for managing sets in `cli/commands/metric.py`, categorization metadata in `cli/azure_evaluators.json`, and documentation covering 1P and 3P RAI use cases.',
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/cli/constants.py",
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/utils/metric_operations.py",
            "src/evee/cli/utils/config_manager.py",
            "src/evee/cli/azure_evaluators.json",
            "scripts/generate_azure_evaluators.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/evaluators.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "tests/evee/config/test_models.py",
            "tests/evee/cli/test_metric_commands.py",
            "tests/evee/cli/utils/test_metric_operations.py",
            "tests/evee/cli/utils/test_config_manager.py",
            "tests/evee/evaluation/test_model_evaluator_metrics.py",
            "tests/mcp/test_resources.py",
            "src/evee/core/base_metric.py",
            "docs/user-guide/configuration.md",
            "docs/user-guide/cli.md",
            "docs/getting-started/glossary.md",
            "docs/user-guide/metrics.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "Add `MetricSetConfig` model; add `metric_sets` to `ExperimentConfig`",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "E",
                "relevance": "Add metric set type constants (`METRIC_SET_1P`, `METRIC_SET_3P_RAI`)",
            },
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "Add `evee metric set` subcommands (`add-set`, `list-sets`, `apply-set`)",
            },
            {
                "path": "src/evee/cli/utils/metric_operations.py",
                "category": "E",
                "relevance": "Add batch-add helpers, set resolution logic",
            },
            {
                "path": "src/evee/cli/utils/config_manager.py",
                "category": "E",
                "relevance": "Add `add_metric_set()`, `get_metric_sets()` methods",
            },
            {
                "path": "src/evee/cli/azure_evaluators.json",
                "category": "E",
                "relevance": "Add `category`/`tags` fields per evaluator for set grouping",
            },
            {
                "path": "scripts/generate_azure_evaluators.py",
                "category": "E",
                "relevance": "Extract and emit `category`/`tags` from Azure AI SDK metadata",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "`_get_metrics_for_model` — resolve metric set references",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Update schema to include `metric_sets` section",
            },
            {
                "path": "src/evee/mcp/resources/evaluators.py",
                "category": "E",
                "relevance": "Surface category/tag info in evaluator resource",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "E",
                "relevance": "Add metric sets pattern/example",
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "E",
                "relevance": "Tests for `MetricSetConfig`, `ExperimentConfig.metric_sets`",
            },
            {
                "path": "tests/evee/cli/test_metric_commands.py",
                "category": "E",
                "relevance": "Tests for set management CLI commands",
            },
            {
                "path": "tests/evee/cli/utils/test_metric_operations.py",
                "category": "E",
                "relevance": "Test set resolution, batch operations",
            },
            {
                "path": "tests/evee/cli/utils/test_config_manager.py",
                "category": "E",
                "relevance": "Test `add_metric_set`, `get_metric_sets`",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_metrics.py",
                "category": "E",
                "relevance": "Test `_get_metrics_for_model` with set references",
            },
            {
                "path": "tests/mcp/test_resources.py",
                "category": "E",
                "relevance": "Test updated evaluators/config resource",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`METRIC_REGISTRY` — unchanged (sets are config-level)",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Document `metric_sets` config section",
            },
            {
                "path": "docs/user-guide/cli.md",
                "category": "S",
                "relevance": "Document `evee metric set` commands",
            },
            {
                "path": "docs/getting-started/glossary.md",
                "category": "S",
                "relevance": 'Add "metric set" term',
            },
            {
                "path": "docs/user-guide/metrics.md",
                "category": "S",
                "relevance": "Add metric sets usage section",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "268",
        "title": "Add support for configuring metric sets for different use cases (1P and 3P RAI)",
        "query_level": "Q2",
        "task": "Add metric set presets to Evee so users can select pre-configured groups of metrics for RAI (Responsible AI) evaluation. I need to understand the metric configuration schema, how metrics are registered and resolved, the evaluator metadata, and the CLI commands for metric management.",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/cli/constants.py",
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/utils/metric_operations.py",
            "src/evee/cli/utils/config_manager.py",
            "src/evee/cli/azure_evaluators.json",
            "scripts/generate_azure_evaluators.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/evaluators.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "tests/evee/config/test_models.py",
            "tests/evee/cli/test_metric_commands.py",
            "tests/evee/cli/utils/test_metric_operations.py",
            "tests/evee/cli/utils/test_config_manager.py",
            "tests/evee/evaluation/test_model_evaluator_metrics.py",
            "tests/mcp/test_resources.py",
            "src/evee/core/base_metric.py",
            "docs/user-guide/configuration.md",
            "docs/user-guide/cli.md",
            "docs/getting-started/glossary.md",
            "docs/user-guide/metrics.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "Add `MetricSetConfig` model; add `metric_sets` to `ExperimentConfig`",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "E",
                "relevance": "Add metric set type constants (`METRIC_SET_1P`, `METRIC_SET_3P_RAI`)",
            },
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "Add `evee metric set` subcommands (`add-set`, `list-sets`, `apply-set`)",
            },
            {
                "path": "src/evee/cli/utils/metric_operations.py",
                "category": "E",
                "relevance": "Add batch-add helpers, set resolution logic",
            },
            {
                "path": "src/evee/cli/utils/config_manager.py",
                "category": "E",
                "relevance": "Add `add_metric_set()`, `get_metric_sets()` methods",
            },
            {
                "path": "src/evee/cli/azure_evaluators.json",
                "category": "E",
                "relevance": "Add `category`/`tags` fields per evaluator for set grouping",
            },
            {
                "path": "scripts/generate_azure_evaluators.py",
                "category": "E",
                "relevance": "Extract and emit `category`/`tags` from Azure AI SDK metadata",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "`_get_metrics_for_model` — resolve metric set references",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Update schema to include `metric_sets` section",
            },
            {
                "path": "src/evee/mcp/resources/evaluators.py",
                "category": "E",
                "relevance": "Surface category/tag info in evaluator resource",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "E",
                "relevance": "Add metric sets pattern/example",
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "E",
                "relevance": "Tests for `MetricSetConfig`, `ExperimentConfig.metric_sets`",
            },
            {
                "path": "tests/evee/cli/test_metric_commands.py",
                "category": "E",
                "relevance": "Tests for set management CLI commands",
            },
            {
                "path": "tests/evee/cli/utils/test_metric_operations.py",
                "category": "E",
                "relevance": "Test set resolution, batch operations",
            },
            {
                "path": "tests/evee/cli/utils/test_config_manager.py",
                "category": "E",
                "relevance": "Test `add_metric_set`, `get_metric_sets`",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_metrics.py",
                "category": "E",
                "relevance": "Test `_get_metrics_for_model` with set references",
            },
            {
                "path": "tests/mcp/test_resources.py",
                "category": "E",
                "relevance": "Test updated evaluators/config resource",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`METRIC_REGISTRY` — unchanged (sets are config-level)",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Document `metric_sets` config section",
            },
            {
                "path": "docs/user-guide/cli.md",
                "category": "S",
                "relevance": "Document `evee metric set` commands",
            },
            {
                "path": "docs/getting-started/glossary.md",
                "category": "S",
                "relevance": 'Add "metric set" term',
            },
            {
                "path": "docs/user-guide/metrics.md",
                "category": "S",
                "relevance": "Add metric sets usage section",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "268",
        "title": "Add support for configuring metric sets for different use cases (1P and 3P RAI)",
        "query_level": "Q3",
        "task": "Different users need different sets of metrics for compliance (1P vs 3P RAI). I want to add preset metric groups that users can select by name. How does Evee handle metric configuration and where would I add metric set support?",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/cli/constants.py",
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/utils/metric_operations.py",
            "src/evee/cli/utils/config_manager.py",
            "src/evee/cli/azure_evaluators.json",
            "scripts/generate_azure_evaluators.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/evaluators.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "tests/evee/config/test_models.py",
            "tests/evee/cli/test_metric_commands.py",
            "tests/evee/cli/utils/test_metric_operations.py",
            "tests/evee/cli/utils/test_config_manager.py",
            "tests/evee/evaluation/test_model_evaluator_metrics.py",
            "tests/mcp/test_resources.py",
            "src/evee/core/base_metric.py",
            "docs/user-guide/configuration.md",
            "docs/user-guide/cli.md",
            "docs/getting-started/glossary.md",
            "docs/user-guide/metrics.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "Add `MetricSetConfig` model; add `metric_sets` to `ExperimentConfig`",
            },
            {
                "path": "src/evee/cli/constants.py",
                "category": "E",
                "relevance": "Add metric set type constants (`METRIC_SET_1P`, `METRIC_SET_3P_RAI`)",
            },
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "Add `evee metric set` subcommands (`add-set`, `list-sets`, `apply-set`)",
            },
            {
                "path": "src/evee/cli/utils/metric_operations.py",
                "category": "E",
                "relevance": "Add batch-add helpers, set resolution logic",
            },
            {
                "path": "src/evee/cli/utils/config_manager.py",
                "category": "E",
                "relevance": "Add `add_metric_set()`, `get_metric_sets()` methods",
            },
            {
                "path": "src/evee/cli/azure_evaluators.json",
                "category": "E",
                "relevance": "Add `category`/`tags` fields per evaluator for set grouping",
            },
            {
                "path": "scripts/generate_azure_evaluators.py",
                "category": "E",
                "relevance": "Extract and emit `category`/`tags` from Azure AI SDK metadata",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "`_get_metrics_for_model` — resolve metric set references",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Update schema to include `metric_sets` section",
            },
            {
                "path": "src/evee/mcp/resources/evaluators.py",
                "category": "E",
                "relevance": "Surface category/tag info in evaluator resource",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "E",
                "relevance": "Add metric sets pattern/example",
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "E",
                "relevance": "Tests for `MetricSetConfig`, `ExperimentConfig.metric_sets`",
            },
            {
                "path": "tests/evee/cli/test_metric_commands.py",
                "category": "E",
                "relevance": "Tests for set management CLI commands",
            },
            {
                "path": "tests/evee/cli/utils/test_metric_operations.py",
                "category": "E",
                "relevance": "Test set resolution, batch operations",
            },
            {
                "path": "tests/evee/cli/utils/test_config_manager.py",
                "category": "E",
                "relevance": "Test `add_metric_set`, `get_metric_sets`",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_metrics.py",
                "category": "E",
                "relevance": "Test `_get_metrics_for_model` with set references",
            },
            {
                "path": "tests/mcp/test_resources.py",
                "category": "E",
                "relevance": "Test updated evaluators/config resource",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`METRIC_REGISTRY` — unchanged (sets are config-level)",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Document `metric_sets` config section",
            },
            {
                "path": "docs/user-guide/cli.md",
                "category": "S",
                "relevance": "Document `evee metric set` commands",
            },
            {
                "path": "docs/getting-started/glossary.md",
                "category": "S",
                "relevance": 'Add "metric set" term',
            },
            {
                "path": "docs/user-guide/metrics.md",
                "category": "S",
                "relevance": "Add metric sets usage section",
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "275",
        "title": "Support Reusing Metric Implementations with Custom Instance Names",
        "query_level": "Q1",
        "task": "I need to decouple metric implementation lookup from display names in Evee. Currently `MetricConfig.name` in `config/models.py` serves as both the `METRIC_REGISTRY` lookup key (in `base_metric.py`) and the reporting label (in `metrics_aggregator.py`, `model_evaluator.py`). To reuse the same metric class with different parameters (e.g., LLM judge with different prompts), I need either an `entry_point` field for implementation lookup while `name` becomes display-only, or a `display_name` field for reporting while `name` stays as lookup key. This affects `_register_metric()`, metric templates, CLI commands, validation, list command, aggregation, and all related tests.",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/cli/commands/validate.py",
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/utils/metric_operations.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "tests/evee/config/test_models.py",
            "tests/evee/cli/test_validate_command.py",
            "tests/evee/cli/utils/test_metric_operations.py",
            "tests/evee/cli/test_metric_commands.py",
            "tests/evee/evaluation/test_metrics_aggregator.py",
            "tests/evee/evaluation/test_model_evaluator_metrics.py",
            "src/evee/core/base_metric.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "src/evee/cli/utils/config_manager.py",
            "tests/evee/conftest.py",
            "tests/evee/core/test_base_metric.py",
            "docs/user-guide/configuration.md",
            "docs/user-guide/metrics.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "`MetricConfig` — add `entry_point: str | None = None` field",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "`_register_metric` — use `entry_point or name` for `METRIC_REGISTRY` lookup",
            },
            {
                "path": "src/evee/cli/commands/validate.py",
                "category": "E",
                "relevance": "`_deep_validate` — use `entry_point or name` for registry lookup",
            },
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "Scaffolding — emit `entry_point` in generated config for foundry metrics",
            },
            {
                "path": "src/evee/cli/utils/metric_operations.py",
                "category": "E",
                "relevance": "`add_metric_to_config` — support `entry_point` kwarg",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Update schema to show `entry_point` field on `metrics[]`",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "E",
                "relevance": 'Add "metric reuse" pattern: same `entry_point`, different `name` + params',
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "E",
                "relevance": "Test `MetricConfig.entry_point` field, default behavior",
            },
            {
                "path": "tests/evee/cli/test_validate_command.py",
                "category": "E",
                "relevance": "Test validation with entry_point resolution",
            },
            {
                "path": "tests/evee/cli/utils/test_metric_operations.py",
                "category": "E",
                "relevance": "Test `entry_point` in config dict construction",
            },
            {
                "path": "tests/evee/cli/test_metric_commands.py",
                "category": "E",
                "relevance": "Test foundry scaffolding emits `entry_point`",
            },
            {
                "path": "tests/evee/evaluation/test_metrics_aggregator.py",
                "category": "E",
                "relevance": "Verify aggregation uses display name",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_metrics.py",
                "category": "E",
                "relevance": "Test entry_point-based registry lookup",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`METRIC_REGISTRY` keyed by decorator name — unchanged",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Uses `metric_name` from `metrics_registry` keys — inherits display name, no direct changes",
            },
            {
                "path": "src/evee/cli/utils/config_manager.py",
                "category": "C",
                "relevance": "`add_metric` accepts arbitrary config dicts — no structural change",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Mock fixtures already have `entry_point` field — confirms pattern anticipated",
            },
            {
                "path": "tests/evee/core/test_base_metric.py",
                "category": "C",
                "relevance": "METRIC_REGISTRY tests — unchanged",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Document `entry_point` field, reuse pattern",
            },
            {
                "path": "docs/user-guide/metrics.md",
                "category": "S",
                "relevance": 'Add "reusing metrics with custom names" section',
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "275",
        "title": "Support Reusing Metric Implementations with Custom Instance Names",
        "query_level": "Q2",
        "task": 'Evee can\'t reuse the same metric with different configurations because `name` is used for both lookup and display. I need to separate these concerns so I can have "Coherence" and "Violence" both using the `llm_judge` metric with different prompts. Where is the metric registry, config model, evaluation pipeline, and CLI metric management?',
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/cli/commands/validate.py",
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/utils/metric_operations.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "tests/evee/config/test_models.py",
            "tests/evee/cli/test_validate_command.py",
            "tests/evee/cli/utils/test_metric_operations.py",
            "tests/evee/cli/test_metric_commands.py",
            "tests/evee/evaluation/test_metrics_aggregator.py",
            "tests/evee/evaluation/test_model_evaluator_metrics.py",
            "src/evee/core/base_metric.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "src/evee/cli/utils/config_manager.py",
            "tests/evee/conftest.py",
            "tests/evee/core/test_base_metric.py",
            "docs/user-guide/configuration.md",
            "docs/user-guide/metrics.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "`MetricConfig` — add `entry_point: str | None = None` field",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "`_register_metric` — use `entry_point or name` for `METRIC_REGISTRY` lookup",
            },
            {
                "path": "src/evee/cli/commands/validate.py",
                "category": "E",
                "relevance": "`_deep_validate` — use `entry_point or name` for registry lookup",
            },
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "Scaffolding — emit `entry_point` in generated config for foundry metrics",
            },
            {
                "path": "src/evee/cli/utils/metric_operations.py",
                "category": "E",
                "relevance": "`add_metric_to_config` — support `entry_point` kwarg",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Update schema to show `entry_point` field on `metrics[]`",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "E",
                "relevance": 'Add "metric reuse" pattern: same `entry_point`, different `name` + params',
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "E",
                "relevance": "Test `MetricConfig.entry_point` field, default behavior",
            },
            {
                "path": "tests/evee/cli/test_validate_command.py",
                "category": "E",
                "relevance": "Test validation with entry_point resolution",
            },
            {
                "path": "tests/evee/cli/utils/test_metric_operations.py",
                "category": "E",
                "relevance": "Test `entry_point` in config dict construction",
            },
            {
                "path": "tests/evee/cli/test_metric_commands.py",
                "category": "E",
                "relevance": "Test foundry scaffolding emits `entry_point`",
            },
            {
                "path": "tests/evee/evaluation/test_metrics_aggregator.py",
                "category": "E",
                "relevance": "Verify aggregation uses display name",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_metrics.py",
                "category": "E",
                "relevance": "Test entry_point-based registry lookup",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`METRIC_REGISTRY` keyed by decorator name — unchanged",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Uses `metric_name` from `metrics_registry` keys — inherits display name, no direct changes",
            },
            {
                "path": "src/evee/cli/utils/config_manager.py",
                "category": "C",
                "relevance": "`add_metric` accepts arbitrary config dicts — no structural change",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Mock fixtures already have `entry_point` field — confirms pattern anticipated",
            },
            {
                "path": "tests/evee/core/test_base_metric.py",
                "category": "C",
                "relevance": "METRIC_REGISTRY tests — unchanged",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Document `entry_point` field, reuse pattern",
            },
            {
                "path": "docs/user-guide/metrics.md",
                "category": "S",
                "relevance": 'Add "reusing metrics with custom names" section',
            },
        ],
        "difficulty": "complex",
    },
    {
        "issue": "275",
        "title": "Support Reusing Metric Implementations with Custom Instance Names",
        "query_level": "Q3",
        "task": "I want to use the same metric class multiple times with different parameters, but Evee's metric naming system doesn't allow it. How does metric registration and lookup work, and where would I change it?",
        "gt_files": [
            "src/evee/config/models.py",
            "src/evee/evaluation/model_evaluator.py",
            "src/evee/cli/commands/validate.py",
            "src/evee/cli/commands/metric.py",
            "src/evee/cli/utils/metric_operations.py",
            "src/evee/mcp/resources/config.py",
            "src/evee/mcp/resources/metric_patterns.py",
            "tests/evee/config/test_models.py",
            "tests/evee/cli/test_validate_command.py",
            "tests/evee/cli/utils/test_metric_operations.py",
            "tests/evee/cli/test_metric_commands.py",
            "tests/evee/evaluation/test_metrics_aggregator.py",
            "tests/evee/evaluation/test_model_evaluator_metrics.py",
            "src/evee/core/base_metric.py",
            "src/evee/evaluation/metrics_aggregator.py",
            "src/evee/cli/utils/config_manager.py",
            "tests/evee/conftest.py",
            "tests/evee/core/test_base_metric.py",
            "docs/user-guide/configuration.md",
            "docs/user-guide/metrics.md",
        ],
        "gt_categories": [
            {
                "path": "src/evee/config/models.py",
                "category": "E",
                "relevance": "`MetricConfig` — add `entry_point: str | None = None` field",
            },
            {
                "path": "src/evee/evaluation/model_evaluator.py",
                "category": "E",
                "relevance": "`_register_metric` — use `entry_point or name` for `METRIC_REGISTRY` lookup",
            },
            {
                "path": "src/evee/cli/commands/validate.py",
                "category": "E",
                "relevance": "`_deep_validate` — use `entry_point or name` for registry lookup",
            },
            {
                "path": "src/evee/cli/commands/metric.py",
                "category": "E",
                "relevance": "Scaffolding — emit `entry_point` in generated config for foundry metrics",
            },
            {
                "path": "src/evee/cli/utils/metric_operations.py",
                "category": "E",
                "relevance": "`add_metric_to_config` — support `entry_point` kwarg",
            },
            {
                "path": "src/evee/mcp/resources/config.py",
                "category": "E",
                "relevance": "Update schema to show `entry_point` field on `metrics[]`",
            },
            {
                "path": "src/evee/mcp/resources/metric_patterns.py",
                "category": "E",
                "relevance": 'Add "metric reuse" pattern: same `entry_point`, different `name` + params',
            },
            {
                "path": "tests/evee/config/test_models.py",
                "category": "E",
                "relevance": "Test `MetricConfig.entry_point` field, default behavior",
            },
            {
                "path": "tests/evee/cli/test_validate_command.py",
                "category": "E",
                "relevance": "Test validation with entry_point resolution",
            },
            {
                "path": "tests/evee/cli/utils/test_metric_operations.py",
                "category": "E",
                "relevance": "Test `entry_point` in config dict construction",
            },
            {
                "path": "tests/evee/cli/test_metric_commands.py",
                "category": "E",
                "relevance": "Test foundry scaffolding emits `entry_point`",
            },
            {
                "path": "tests/evee/evaluation/test_metrics_aggregator.py",
                "category": "E",
                "relevance": "Verify aggregation uses display name",
            },
            {
                "path": "tests/evee/evaluation/test_model_evaluator_metrics.py",
                "category": "E",
                "relevance": "Test entry_point-based registry lookup",
            },
            {
                "path": "src/evee/core/base_metric.py",
                "category": "C",
                "relevance": "`METRIC_REGISTRY` keyed by decorator name — unchanged",
            },
            {
                "path": "src/evee/evaluation/metrics_aggregator.py",
                "category": "C",
                "relevance": "Uses `metric_name` from `metrics_registry` keys — inherits display name, no direct changes",
            },
            {
                "path": "src/evee/cli/utils/config_manager.py",
                "category": "C",
                "relevance": "`add_metric` accepts arbitrary config dicts — no structural change",
            },
            {
                "path": "tests/evee/conftest.py",
                "category": "C",
                "relevance": "Mock fixtures already have `entry_point` field — confirms pattern anticipated",
            },
            {
                "path": "tests/evee/core/test_base_metric.py",
                "category": "C",
                "relevance": "METRIC_REGISTRY tests — unchanged",
            },
            {
                "path": "docs/user-guide/configuration.md",
                "category": "S",
                "relevance": "Document `entry_point` field, reuse pattern",
            },
            {
                "path": "docs/user-guide/metrics.md",
                "category": "S",
                "relevance": 'Add "reusing metrics with custom names" section',
            },
        ],
        "difficulty": "complex",
    },
]
